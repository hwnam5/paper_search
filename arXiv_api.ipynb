{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8342222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\hwnam\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d374eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import arxiv\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np # for array manipulation\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "%matplotlib inline \n",
    "import datetime\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import feedparser\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de511a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 2252\n"
     ]
    }
   ],
   "source": [
    "# read filtered_papers.json\n",
    "with open('filtered_papers.json', 'r', encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "print(f\"Number of papers: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d310ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Residual Attention Network for Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\남현원\\AppData\\Local\\Temp\\ipykernel_4200\\3935089147.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "Paper found in arXiv\n",
      "Paper found source file\n"
     ]
    }
   ],
   "source": [
    "paper = papers[0]\n",
    "query = paper[\"title\"]\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    ")\n",
    "\n",
    "for result in search.results():\n",
    "    title = result.title\n",
    "    paper_id = result.entry_id.split(\"/\")[-1]\n",
    "    print(f\"Found paper: {title}\")\n",
    "    print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "    if query == title:\n",
    "        max_try = 0\n",
    "        print(\"Paper found in arXiv\")\n",
    "        \n",
    "        url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "        response = requests.get(url)\n",
    "        while max_try < 5:\n",
    "            if response.status_code == 200:\n",
    "                with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(\"Paper found source file\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Paper not found source file\")\n",
    "                max_try += 1\n",
    "                response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7189e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention-net_camera_ready_merge_final.tex\n"
     ]
    }
   ],
   "source": [
    "tar_path = \"1704.06904v1.tar.gz\"\n",
    "\n",
    "#with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    #tar.extractall(\"Example\")\n",
    "\n",
    "\n",
    "for fname in os.listdir(\"Example\"):\n",
    "    if fname.endswith(\".tex\"):\n",
    "        print(fname)\n",
    "        tex_file = fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff48c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_section(tex_file):\n",
    "    sections = []\n",
    "    match = re.search(r\"\\\\begin{abstract}(?:\\[[^\\]]*\\])?(.*?)\\\\end{abstract}\", tex_file, re.DOTALL)\n",
    "    section_name = \"abstract\"\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        sections.append((section_name, content))\n",
    "    else:\n",
    "        print(\"No abstract found\")\n",
    "\n",
    "    pattern = r\"\\\\section\\*?{([^}]+)}\"\n",
    "    matches = list(re.finditer(pattern, tex_file))\n",
    "    print(f\"Number of sections: {len(matches)}\")\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        section_name = match.group(1).strip()\n",
    "        \n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(tex_file)\n",
    "        \n",
    "        content = tex_file[start:end].strip()\n",
    "        sections.append((section_name, content))\n",
    "        \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a188e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abstract',\n",
       "  '%Mixed nature of human attention has been proposed in the literature of biology and been applied to sequential learning task using RNN and LSTM.\\nIn this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\n%\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\n\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'),\n",
       " ('Introduction',\n",
       "  '\\\\begin{figure*}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{.9\\\\linewidth}{0pt}}\\n\\\\includegraphics[width=1\\\\linewidth]{motivation.pdf}\\n\\\\end{center}\\n   \\\\caption{\\\\textbf{Left:} an example shows the interaction between features and attention masks. \\\\textbf{Right:} example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes low-level background blue color features. The balloon instance mask highlights high-level balloon bottom part features.}\\n\\\\label{fig:motivation}\\n\\\\end{figure*}\\n\\nNot only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\n\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\n\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\n\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n%\\n\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'),\n",
       " ('Related Work',\n",
       "  'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\n\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\n\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\n\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\n\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\n\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\n\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\n\\n\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.\\n\\n%-------------------------------------------------------------------------'),\n",
       " ('Residual Attention Network',\n",
       "  \"\\\\begin{figure*}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n  \\\\includegraphics[width=1.0\\\\linewidth]{whole_net.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of Attention Module: $p,t$ and $r$. The hyper-parameter $p$ denotes the number of pre-processing Residual Units before splitting into trunk branch and mask branch. $t$ denotes the number of Residual Units in trunk branch. $r$ denotes the number of Residual Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting: $\\\\{p=1$, $t=2$, $r=1\\\\}$. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the same.}\\n\\n\\\\label{fig:Attention}\\n\\\\end{figure*}\\n\\nOur Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\n%\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\n\\\\begin{equation}\\nH_{i,c}(x)=M_{i,c}(x)*T_{i,c}(x)\\n\\\\end{equation}\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\n\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\begin{equation}\\n\\\\frac{\\\\partial M(x, \\\\theta)T(x,\\\\phi)}{\\\\partial \\\\phi} = M(x, \\\\theta)\\\\frac{\\\\partial T(x,\\\\phi)}{\\\\partial \\\\phi}\\n\\\\end{equation}\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\n\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\n\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\n\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\n\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n\\\\begin{equation}\\nH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)\\n\\\\end{equation}\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\n\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n   \\\\includegraphics[width=1\\\\linewidth]{attention.pdf}\\n\\\\end{center}\\n   \\\\caption{The receptive field comparison between mask branch and trunk branch.}\\n\\\\label{fig:attentionunit}\\n\\\\end{figure}\\n\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\n\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n%Using additional classification supervision on mask branch directly leads to 0.5\\\\% performance drop on CIFAR-10.\\n\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\n\\\\begin{eqnarray}\\n&&f_{1}(x_{i,c}) = \\\\frac{1}{1+ exp(-x_{i,c})}\\\\\\\\\\n&&f_{2}(x_{i,c}) = \\\\frac{x_{i,c}}{\\\\|x_{i}\\\\|}\\\\\\\\\\n&&f_{3}(x_{i,c}) = \\\\frac{1}{1+ exp(-(x_{i,c} - \\\\text{mean}_c) / \\\\text{std}_c)}\\n\\\\end{eqnarray}\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nActivation Function & Attention Type & Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n$f_{1}(x)$ & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n$f_{2}(x)$  & Channel Attention &6.24\\\\\\\\\\n\\\\hline\\n$f_{3}(x)$ & Spatial Attention &6.33\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{The test error (\\\\%) on CIFAR-10 of Attention-56 network with different activation functions.}\\n\\\\label{tab:activation_exp}\\n\\\\end{table}\\n\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\footnotesize\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c|c} \\\\hline\\n\\nLayer &Output Size &Attention-56&Attention-92 \\\\\\\\\\n\\\\hline\\nConv1 & 112$\\\\times$112 & \\\\multicolumn{2}{|c}{$7\\\\times 7$, 64, stride 2}  \\\\\\\\\\n\\\\hline\\nMax pooling & 56$\\\\times$56& \\\\multicolumn{2}{|c}{$3\\\\times 3$ stride 2}  \\\\\\\\\\n\\\\hline\\nResidual Unit& 56$\\\\times$56 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 64 \\\\\\\\\\n\\t3\\\\times 3, 64 \\\\\\\\\\n\\t1\\\\times\\t1, 256\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 56$\\\\times$56 & Attention $\\\\times$1 & Attention $\\\\times$1  \\\\\\\\\\n\\\\hline\\nResidual Unit& 28$\\\\times$28 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 128 \\\\\\\\\\n\\t3\\\\times 3, 128 \\\\\\\\\\n\\t1\\\\times\\t1, 512\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 28$\\\\times$28 & Attention $\\\\times$1 & Attention $\\\\times$2  \\\\\\\\\\n\\\\hline\\nResidual Unit& 14$\\\\times$14 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 256 \\\\\\\\\\n\\t3\\\\times 3, 256 \\\\\\\\\\n\\t1\\\\times\\t1, 1024\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 14$\\\\times$14 & Attention $\\\\times$1 & Attention $\\\\times$3  \\\\\\\\\\n\\\\hline\\nResidual Unit& 7$\\\\times$7 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 512 \\\\\\\\\\n\\t3\\\\times 3, 512 \\\\\\\\\\n\\t1\\\\times\\t1, 2048\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 3$\\n}  \\\\\\\\\\n\\\\hline\\nAverage pooling & 1$\\\\times$1& \\\\multicolumn{2}{|c}{$7\\\\times 7$ stride 1}  \\\\\\\\\\n\\\\hline\\nFC,Softmax & \\\\multicolumn{3}{|c}{1000}  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{params$\\\\times 10^6$} & $31.9$ & $51.3$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{FLOPs$\\\\times 10^9$} & $6.2$ &$10.4$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{Trunk depth} & $56 $ & $92$  \\\\\\\\\\n\\\\hline\\n\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Residual Attention Network architecture details for ImageNet. Attention structure is described in Fig.~\\\\ref{fig:Attention}.  We make the size of the smallest output map in each mask branch 7$\\\\times$7 to be consistent with the smallest trunk output map size. Thus 3,2,1 max-pooling layers are used in mask branch with input size 56$\\\\times$56, 28$\\\\times$28, 14$\\\\times$14 respectively.\\n%\\nThe Attention Module is built by pre-activation Residual Unit~\\\\cite{he2016identity} with the number of channels in each stage is the same as ResNet~\\\\cite{resnet2016}.\\n%\\n}\\n\\\\label{tab:attention_structure}\\n\\\\end{table}\\n\\n\\n%-------------------------------------------------------------------------\"),\n",
       " ('Experiments',\n",
       "  \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\n%\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\n%\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\n%\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\n%\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n%\\n\\n\\n\\\\subsection{CIFAR and Analysis}\\n\\n\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\n%\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\n%\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\n%\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\n%\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\n%\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\n\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\n%\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\n%\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\n%\\nWe use original $32\\\\times32$ image for testing.\\n\\n\\n\\\\paragraph{Attention Residual Learning.}\\n\\n\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\n%\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\n%\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\n%\\n% Add a small figure here.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\n%\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\n Network & ARL (Top-1 err. \\\\%) & NAL (Top-1 err.\\\\%)\\\\\\\\\\n\\\\hline\\nAttention-56 &\\\\textbf{5.52} & 5.89\\\\\\\\\\n\\\\hline\\nAttention-92 &\\\\textbf{4.99} & 5.35\\\\\\\\\\n\\\\hline\\nAttention-128 &\\\\textbf{4.44} & 5.57\\\\\\\\\\n\\\\hline\\nAttention-164 &\\\\textbf{4.31} & 7.18\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{\\nClassification error (\\\\%) on CIAFR-10.}\\n\\\\label{tab:learning}\\n\\\\end{table}\\n\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\n%\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\n%\\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\n\\n%\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-15pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n  \\\\includegraphics[width=1\\\\linewidth]{mean_value.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{The mean absolute response of output features in each stage. }\\n\\\\label{fig:mean_response}\\n\\\\end{figure}\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\n%\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\n%\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\n%\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\n%\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\n%\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\n%----------------------------------------------------------------------------------\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\n%\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\n%\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\n%\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\n\\\\begin{table}[h]\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nMask Type  & Attention Type &Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nLocal Convolutions & Local Attention &6.48 \\\\\\\\\\n\\\\hline\\nEncoder and Decoder  & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 using different mask structures.}\\n\\\\label{tab:local_global_attention}\\n\\\\end{table}\\n\\n\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nNoise Level &ResNet-164 err. (\\\\%) & Attention-92 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n10\\\\% &5.93 &5.15\\\\\\\\\\n\\\\hline\\n30\\\\% &6.61 &5.79\\\\\\\\\\n\\\\hline\\n50\\\\% &8.35 &7.27\\\\\\\\\\n\\\\hline\\n70\\\\% &17.21 &15.75\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 with label noises.}\\n\\\\label{tab:noise_label}\\n\\\\end{table}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\n%\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\begin{equation}\\nQ =\\n\\\\left(\\n\\\\begin{matrix}\\nr & \\\\frac{1-r}{9} &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\frac{1-r}{9} &r  &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n\\\\frac{1-r}{9} & \\\\frac{1-r}{9} &\\\\cdots &r \\\\\\\\\\n\\\\end{matrix}\\n\\\\right)_{10\\\\times 10}\\n\\\\end{equation}\\n\\n%\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\n\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\n%\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\n%\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\n%\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\n%\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\n%\\n%The encode-decode structure can fast feedforward the whole image and obtain the global and local information of image.\\n%\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\n%\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\n\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c}\\n\\t\\\\hline\\n   \\tNetwork& params$\\\\times 10^6$ & CIFAR-10  &  CIFAR-100 \\\\\\\\\\n\\t\\\\hline\\n\\tResNet-164~\\\\cite{he2016identity}  & 1.7   & 5.46  & 24.33 \\\\\\\\\\n\\tResNet-1001~\\\\cite{he2016identity} & 10.3   & 4.64  & 22.71 \\\\\\\\\\n\\t\\\\hline\\n\\tWRN-16-8~\\\\cite{zagoruyko2016wide} & 11.0   & 4.81  & 22.07 \\\\\\\\\\n\\tWRN-28-10~\\\\cite{zagoruyko2016wide} & 36.5   & 4.17  & 20.50 \\\\\\\\\\n\\t\\\\hline\\n\\tAttention-92 & 1.9 & 4.99 & 21.71 \\\\\\\\\\n\\tAttention-236 & 5.1 & 4.14 & 21.16 \\\\\\\\\\n\\tAttention-452$\\\\dag$ & 8.6 & \\\\textbf{3.90}  & \\\\textbf{20.45}\\\\\\\\\\n\\t\\\\hline\\n\\\\end{tabular}\\n}\\n\\\\end{center}\\n\\\\caption{Comparisons with state-of-the-art methods on CIFAR-10/100. $\\\\dag$: the Attention-452 consists of Attention Module with hyper-parameters setting: $\\\\{p=2$, $t=4$, $r=3\\\\}$ and 6 Attention Modules per stage. }\\n\\\\label{tab:cifar_results}\\n\\\\end{table}\\n\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\n%\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\n%\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\n%\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\n%\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n%\\n%It worth to mention that, our method is complementary with other state-of-the-art methods which focus on regularization and can achieve better results by applying these advanced techniques.\\n\\n\\n\\\\subsection{ImageNet Classification}\\n\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\n%\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\n%\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n%\\n\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\n%\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\n%\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\n%\\nThe network is trained using SGD with a momentum of $0.9$.\\n%\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\n\\n\\\\paragraph{Mask Influence.}\\n\\n\\\\begin{table*}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c|c|c} \\\\hline\\nNetwork & params$\\\\times 10^6$ &FLOPs$\\\\times 10^9$ & Test Size &Top-1 err. (\\\\%) &Top-5 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nResNet-152~\\\\cite{resnet2016}  &60.2 &11.3 &$224\\\\times224$&22.16 &6.16\\\\\\\\\\n\\\\hline\\nAttention-56 &31.9 &6.3 &$224\\\\times224$&\\\\textbf{21.76} &\\\\textbf{5.9} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNeXt-101 ~\\\\cite{resnext}&44.5 & 7.8&$224\\\\times224$    &21.2 &5.6 \\\\\\\\\\n\\\\hline\\nAttentionNeXt-56 &31.9 & 6.3&$224\\\\times224$  &\\\\textbf{21.2} &\\\\textbf{5.6} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nInception-ResNet-v1~\\\\cite{inception} &- &-&$299\\\\times299$&21.3 &5.5 \\\\\\\\\\n\\\\hline\\nAttentionInception-56 &31.9 & 6.3 &$299\\\\times299$ &\\\\textbf{20.36} &\\\\textbf{5.29} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNet-200~\\\\cite{he2016identity} &64.7 &15.0 &$320\\\\times320$ &20.1  &4.8 \\\\\\\\\\n\\\\hline\\n{Inception-ResNet-v2} &- &- &$299\\\\times299$ &19.9  &4.9 \\\\\\\\\\n\\\\hline\\nAttention-92 &51.3  & 10.4&$320\\\\times320$ &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n%}\\n\\\\end{center}\\n\\t\\\\caption{Single crop validation error on ImageNet.\\n}\\n\\\\label{tab:single_crop_validation_error}\\n\\\\end{table*}\\n\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\n%\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\n%\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\n%\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\n%\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\n%\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\n\\n\\\\paragraph{Different Basic Units.}\\n%\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\n\\n%\\n%\\\\begin{figure}[t]\\n%\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n%  \\\\includegraphics[width=1\\\\linewidth]{images/inception.png}\\n  %\\\\includegraphics{images/whole_net.eps}\\n%\\\\end{center}\\n%   \\\\caption{The simple inception module stucture. The hyper-parameter $c$ denotes the number of channel in one stage. In this experiment, we choose $\\\\{256, 512, 1024, 2048\\\\}$ at feature map $\\\\{56\\\\times56, 28\\\\times28, 14\\\\times14, 7\\\\times7\\\\}$.}\\n%\\\\label{fig:inception}\\n%\\\\end{figure}\\n%\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\n%\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\n%\\nThe results show that our method can be applied on different network structures.\\n\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\n\\n%\\\\begin{table}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n%\\\\begin{tabular}{c|c|c|c|c} \\\\hline\\n%Network &param/M & FLOPs$\\\\times 10^9$ &top-1 err. &top-5 err.\\\\\\\\\\n%\\\\hline\\n%{ResNet-200}~\\\\cite{he2016identity} &64.7 &15.0  &20.1  &4.8 \\\\\\\\\\n%\\\\hline\\n%{Inception-ResNet-v2} &- &-  &19.9  &4.9 \\\\\\\\\\n%\\\\hline\\n%Attention-92 &51.3  & 10.4 &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n%\\\\hline\\n%\\\\end{tabular}\\n%}\\n%\\\\end{center}\\n %\\\\caption{\\n%Comparisons of single crop error on the ILSVRC 2012 validation set. In order to compare fairly, we also test our Attention Network on a %single 320$\\\\times$320 crop.}\\n%\\\\label{tab:imagenet_result}\\n%\\\\end{table}\\n\\n\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\n%\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\n%\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\n%\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\n%\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\n%\\nAbove results suggest that our model enjoys high efficiency and good performance.\\n\\n%Our architecture is parallel to major structure of original network, which is friendly to parallel computation. (3) Stacked Attention Module on $14\\\\times14$ feature map gains $1.3\\\\%$ improvement, contrast to the one of single unit, benefits from more Attention Module.\\n\\n%Note that we test a single 320$\\\\times$320 crop from short side of 320, which is consistent with ResNet-200[].\\n%Although our Attention-80 has significantly computation complexity than pre-activation ResNet-200 [](15.0$\\\\times 10^9$), Our Attention-80 has achieved top-1 error rate of 20.3\\\\%, which is 0.4\\\\% lower than the baseline ResNet-200.\"),\n",
       " ('Discussion',\n",
       "  'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n\\n\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\n\\\\end{document}')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Example/\" + tex_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    latex_text = f.read()\n",
    "\n",
    "splitBy_section = split_section(latex_text)\n",
    "\n",
    "splitBy_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c290e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for section_name, content in splitBy_section:\n",
    "    cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\"\n",
    ", \"\", content, flags=re.DOTALL)\n",
    "    cleaned_content = cleaned_content.strip()\n",
    "    \n",
    "    lines = cleaned_content.splitlines()\n",
    "    cleaned_content = [line for line in lines if line.strip() and not line.startswith(\"%\")]\n",
    "    cleaned_content = \"\\n\".join(cleaned_content)\n",
    "    \n",
    "    json_data.append({\n",
    "        \"section_name\": section_name,\n",
    "        \"content\": cleaned_content\n",
    "    })\n",
    "    \n",
    "with open(\"Example/cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08996fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'section_name': 'abstract', 'content': 'In this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'}, {'section_name': 'Introduction', 'content': 'Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'}, {'section_name': 'Related Work', 'content': 'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.'}, {'section_name': 'Residual Attention Network', 'content': \"Our Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\"}, {'section_name': 'Experiments', 'content': \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n\\\\subsection{CIFAR and Analysis}\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\nWe use original $32\\\\times32$ image for testing.\\n\\\\paragraph{Attention Residual Learning.}\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n\\\\subsection{ImageNet Classification}\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\nThe network is trained using SGD with a momentum of $0.9$.\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\\\paragraph{Mask Influence.}\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\\\paragraph{Different Basic Units.}\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\nThe results show that our method can be applied on different network structures.\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\nAbove results suggest that our model enjoys high efficiency and good performance.\"}, {'section_name': 'Discussion', 'content': 'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\\\end{document}'}]\n"
     ]
    }
   ],
   "source": [
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b567552",
   "metadata": {},
   "source": [
    "# Make all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deecc5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\남현원\\AppData\\Local\\Temp\\ipykernel_4200\\2420375828.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "Paper found in arXiv\n",
      "Paper found source file\n",
      "attention-net_camera_ready_merge_final.tex\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "Found paper: Privacy-Preserving Image Classification Using Vision Transformer\n",
      "Paper_id: 2205.12041v1\n",
      "Paper not found in arXiv\n",
      "query: Residual Attention Network for Image Classification\n",
      "title: Privacy-Preserving Image Classification Using Vision Transformer\n",
      "\n",
      "\n",
      "Found paper: Classification of optics-free images with deep neural networks\n",
      "Paper_id: 2011.05132v1\n",
      "Paper not found in arXiv\n",
      "query: Residual Attention Network for Image Classification\n",
      "title: Classification of optics-free images with deep neural networks\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m title \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(\n\u001b[0;32m      7\u001b[0m     query\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[0;32m      8\u001b[0m     max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortCriterion\u001b[38;5;241m.\u001b[39mRelevance,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:601\u001b[0m, in \u001b[0;36mClient._results\u001b[1;34m(self, search, offset)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, search: Search, offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Result, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    600\u001b[0m     page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_size)\n\u001b[1;32m--> 601\u001b[0m     feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mentries:\n\u001b[0;32m    603\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot empty first page; stopping generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:658\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_feed(url, first_page\u001b[38;5;241m=\u001b[39mfirst_page, _try_index\u001b[38;5;241m=\u001b[39m_try_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    678\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(to_sleep)\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mOK:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "for paper in papers:\n",
    "    title = paper[\"title\"]\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=1,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "\n",
    "    for result in search.results():\n",
    "        title = result.title\n",
    "        paper_id = result.entry_id.split(\"/\")[-1]\n",
    "        print(f\"Found paper: {title}\")\n",
    "        print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "        if query == title:\n",
    "            max_try = 0\n",
    "            print(\"Paper found in arXiv\")\n",
    "\n",
    "            url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "            response = requests.get(url)\n",
    "            while max_try < 5:\n",
    "                if response.status_code == 200:\n",
    "                    with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                        f.write(response.content)\n",
    "                    print(\"Paper found source file\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Paper not found source file\")\n",
    "                    max_try += 1\n",
    "                    response = requests.get(url)\n",
    "\n",
    "            tar_path = f\"{paper_id}.tar.gz\"\n",
    "            with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                tar.extractall(\"SourceFiles\")\n",
    "            os.remove(tar_path)\n",
    "\n",
    "            for fname in os.listdir(\"SourceFiles\"):\n",
    "                if fname.endswith(\".tex\"):\n",
    "                    print(fname)\n",
    "                    tex_file = fname\n",
    "\n",
    "            with open(\"SourceFiles/\" + tex_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                latex_text = f.read()\n",
    "            \n",
    "            splitBy_section = split_section(latex_text)\n",
    "            \n",
    "            json_data.append({\n",
    "                    \"title\": title,\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"sections\": []\n",
    "            })\n",
    "            for section_name, content in splitBy_section:\n",
    "                cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\", \"\", content, flags=re.DOTALL)\n",
    "                cleaned_content = cleaned_content.strip()\n",
    "\n",
    "                lines = cleaned_content.splitlines()\n",
    "                cleaned_content = [line for line in lines if line.strip() and not line.startswith(\"%\")]\n",
    "                cleaned_content = \"\\n\".join(cleaned_content)\n",
    "\n",
    "                json_data[-1][\"sections\"].append({\n",
    "                    section_name : content\n",
    "                })\n",
    "\n",
    "            shutil.rmtree(\"SourceFiles\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Paper not found in arXiv\")\n",
    "            print(f\"query: {query}\")\n",
    "            print(f\"title: {title}\")\n",
    "            continue\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-column deep neural networks for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Locality-constrained Linear Coding for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bag of Tricks for Image Classification with Convolutional Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Big Self-Supervised Models Advance Medical Image Classification\n",
      "Paper_id: 2101.05224v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Understanding Robustness of Transformers for Image Classification\n",
      "Paper_id: 2103.14586v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Linear spatial pyramid matching using sparse coding for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover’s Distance and Structured Classifiers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Improving the Fisher Kernel for Large-Scale Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Invariant Information Clustering for Unsupervised Image Classification and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CNN-RNN: A Unified Framework for Multi-label Image Classification\n",
      "\n",
      "\n",
      "🎯 Found paper: General Multi-label Image Classification with Transformers\n",
      "Paper_id: 2011.14027v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Benchmarking Adversarial Robustness on Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Power of Ensembles for Active Learning in Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning from massive noisy labeled data for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Object Detection with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FCOS: Fully Convolutional One-Stage Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EfficientDet: Scalable and Efficient Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Cascade R-CNN: Delving into High Quality Object Detection\n",
      "Paper_id: 1712.00726v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Feature Pyramid Networks for Object Detection\n",
      "Paper_id: 1612.03144v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: You Only Look Once: Unified, Real-Time Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Focal Loss for Dense Object Detection\n",
      "Paper_id: 1708.02002v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointPillars: Fast Encoders for Object Detection From Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CenterNet: Keypoint Triplets for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: DETRs Beat YOLOs on Real-time Object Detection\n",
      "Paper_id: 2304.08069v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cut and Learn for Unsupervised Object Detection and Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Frustum PointNets for 3D Object Detection from RGB-D Data\n",
      "Paper_id: 1711.08488v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Center-based 3D Object Detection and Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DOTA: A Large-Scale Dataset for Object Detection in Aerial Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-view 3D Object Detection Network for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Plain Vision Transformer Backbones for Object Detection\n",
      "Paper_id: 2203.16527v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PETR: Position Embedding Transformation for Multi-View 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffusionDet: Diffusion Model for Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Hough Voting for 3D Object Detection in Point Clouds\n",
      "Paper_id: 1904.09664v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Large Selective Kernel Network for Remote Sensing Object Detection\n",
      "Paper_id: 2303.09030v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Sparse R-CNN: End-to-End Object Detection with Learnable Proposals\n",
      "Paper_id: 2011.12450v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Oriented R-CNN for Object Detection\n",
      "Paper_id: 2108.05699v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Semi-Supervised Object Detection with Soft Teacher\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TOOD: Task-aligned One-stage Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Open World Object Detection\n",
      "Paper_id: 2103.02603v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 22\n",
      "\n",
      "\n",
      "🎯 Found paper: An End-to-End Transformer Model for 3D Object Detection\n",
      "Paper_id: 2109.08141v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReDet: A Rotation-equivariant Detector for Aerial Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Libra R-CNN: Towards Balanced Learning for Object Detection\n",
      "Paper_id: 1904.02701v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Categorical Depth Distribution Network for Monocular 3D Object Detection\n",
      "Paper_id: 2103.01100v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Group-Free 3D Object Detection via Transformers\n",
      "Paper_id: 2104.00678v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Voxel Transformer for 3D Object Detection\n",
      "Paper_id: 2109.02497v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Dynamic Head: Unifying Object Detection Heads with Attentions\n",
      "Paper_id: 2106.08322v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Soft-NMS — Improving Object Detection with One Line of Code\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Camouflaged Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain Adaptive Faster R-CNN for Object Detection in the Wild\n",
      "Paper_id: 1803.03243v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ByteTrack: Multi-Object Tracking by Associating Every Detection Box\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Scale Interactive Network for Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-Domain Adaptive Teacher for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BASNet: Boundary-Aware Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Bounding Box Estimation Using Deep Learning and Geometry\n",
      "Paper_id: 1612.00496v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open Vocabulary Object Detection with Pseudo Bounding-Box Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Bounding Box Regression with Uncertainty for Accurate Object Detection\n",
      "Paper_id: 1809.08545v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond Bounding-Box: Convex-hull Feature Adaptation for Oriented and Densely Packed Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bounding-Box Channels for Visual Relationship Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Task Self-Supervised Object Detection via Recycling of Bounding Box Annotations\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Intelligent Dialogs for Bounding Box Annotation\n",
      "Paper_id: 1712.08087v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploit Bounding Box Annotations for Multi-label Object Recognition\n",
      "Paper_id: 1504.05843v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Visual Tracking with Double Bounding Box Model\n",
      "\n",
      "\n",
      "🎯 Found paper: Siamese Box Adaptive Network for Visual Tracking\n",
      "Paper_id: 2003.06761v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLO-World: Real-Time Open-Vocabulary Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss\n",
      "\n",
      "\n",
      "🎯 Found paper: Is Faster R-CNN Doing Well for Pedestrian Detection?\n",
      "Paper_id: 1607.07032v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking the Faster R-CNN Architecture for Temporal Action Localization\n",
      "\n",
      "\n",
      "🎯 Found paper: Applying Faster R-CNN for Object Detection on Malaria Images\n",
      "Paper_id: 1804.09548v2\n",
      "📦 Source file downloaded\n",
      "❌ 1804.09548v2.tar.gz is not a valid gzip file. Skipping...\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Adaptive Faster R-CNN\n",
      "\n",
      "\n",
      "🎯 Found paper: Faster R-CNN Features for Instance Search\n",
      "Paper_id: 1604.08893v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Fully Convolutional Networks for Semantic Segmentation\n",
      "Paper_id: 1411.4038v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Deconvolution Network for Semantic Segmentation\n",
      "Paper_id: 1505.04366v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Segmenter: Transformer for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CCNet: Criss-Cross Attention for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Adapt Structured Output Space for Semantic Segmentation\n",
      "Paper_id: 1802.10349v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Side Adapter Network for Open-Vocabulary Semantic Segmentation\n",
      "Paper_id: 2302.12242v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision\n",
      "Paper_id: 2106.01226v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GroupViT: Semantic Segmentation Emerges from Text Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Object-Contextual Representations for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels\n",
      "Paper_id: 2203.03884v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FDA: Fourier Domain Adaptation for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-view Transformers for real-time Map-view Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-class Token Transformer for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Delivering Arbitrary-Modal Semantic Segmentation\n",
      "Paper_id: 2303.01480v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semi-Supervised Semantic Segmentation With Cross-Consistency Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking BiSeNet For Real-time Semantic Segmentation\n",
      "Paper_id: 2104.13188v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Cross-Image Pixel Contrast for Semantic Segmentation\n",
      "Paper_id: 2101.11939v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Language-Grounded Indoor 3D Semantic Segmentation in the Wild\n",
      "Paper_id: 2204.07761v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST++: Make Self-trainingWork Better for Semi-supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training\n",
      "\n",
      "\n",
      "🎯 Found paper: ICNet for Real-Time Semantic Segmentation on High-Resolution Images\n",
      "Paper_id: 1704.08545v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers\n",
      "\n",
      "\n",
      "🎯 Found paper: Context Encoding for Semantic Segmentation\n",
      "Paper_id: 1803.08904v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks\n",
      "Paper_id: 1711.10275v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseASPP for Semantic Segmentation in Street Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Semantic Segmentation\n",
      "Paper_id: 2303.11316v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Modeling the Background for Incremental Learning in Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Source-Free Domain Adaptation for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals\n",
      "Paper_id: 2102.06191v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PLOP: Learning without Forgetting for Continual Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking Semantic Segmentation: A Prototype View\n",
      "Paper_id: 2203.15102v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "🎯 Found paper: Prototype Mixture Models for Few-shot Semantic Segmentation\n",
      "Paper_id: 2008.03898v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Pixel-Level Semantic Affinity with Image-Level Supervision for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds\n",
      "\n",
      "\n",
      "🎯 Found paper: Representation Compensation Networks for Continual Semantic Segmentation\n",
      "Paper_id: 2203.05402v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 13\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Generate Text-Grounded Mask for Open-World Semantic Segmentation from Only Image-Text Pairs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs\n",
      "Paper_id: 1711.09869v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-supervised Augmentation Consistency for Adapting Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Path Aggregation Network for Instance Segmentation\n",
      "Paper_id: 1803.01534v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LVIS: A Dataset for Large Vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLACT: Real-Time Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Hybrid Task Cascade for Instance Segmentation\n",
      "Paper_id: 1901.07518v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaskClustering: View Consensus Based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-End Video Instance Segmentation with Transformers\n",
      "Paper_id: 2011.14503v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional Convolutions for Instance Segmentation\n",
      "Paper_id: 2003.05664v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DVIS: Decoupled Video Instance Segmentation Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: SoftGroup for 3D Instance Segmentation on Point Clouds\n",
      "Paper_id: 2203.01509v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CTVIS: Consistent Training for Online Video Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask-Attention-Free Transformer for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Open-Vocabulary Video Instance Segmentation\n",
      "Paper_id: 2304.01715v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Query Refinement Transformer for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Sparse Instance Activation for Real-Time Instance Segmentation\n",
      "Paper_id: 2203.12827v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Video Instance Segmentation\n",
      "Paper_id: 1905.04804v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask-Free Video Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Snake for Real-Time Instance Segmentation\n",
      "Paper_id: 2001.01629v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hi4D: 4D Instance Segmentation of Close Human Interaction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WaterMask: Instance Segmentation for Underwater Imagery\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Weakly Supervised Learning of Instance Segmentation With Inter-Pixel Relations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DoNet: Deep De-Overlapping Network for Cytology Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TopoSeg: Topology-Aware Nuclear Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Camouflaged Instance Segmentation via Explicit De-Camouflaging\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: In Defense of Online Models for Video Instance Segmentation\n",
      "Paper_id: 2207.10661v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxInst: High-Performance Instance Segmentation with Box Annotations\n",
      "\n",
      "\n",
      "🎯 Found paper: Hierarchical Aggregation for 3D Instance Segmentation\n",
      "Paper_id: 2108.02350v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Seesaw Loss for Long-Tailed Instance Segmentation\n",
      "Paper_id: 2008.10032v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxSnake: Polygonal Instance Segmentation with Box Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: SAM-guided Graph Cut for 3D Instance Segmentation\n",
      "Paper_id: 2312.08372v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarMask: Single Shot Instance Segmentation With Polar Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CenterMask: Real-Time Anchor-Free Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Box-supervised Instance Segmentation with Level Set Evolution\n",
      "Paper_id: 2207.09055v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Temporally Efficient Vision Transformer for Video Instance Segmentation\n",
      "Paper_id: 2204.08412v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers\n",
      "Paper_id: 2103.12340v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Generalized Framework for Video Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pointly-Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Panoptic Segmentation\n",
      "Paper_id: 1801.00868v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: You Only Segment Once: Towards Real-Time Panoptic Segmentation\n",
      "Paper_id: 2303.14651v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-vocabulary Panoptic Segmentation with Embedding Modulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UPSNet: A Unified Panoptic Segmentation Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: 4D Panoptic Segmentation as Invariant and Equivariant Field Prediction\n",
      "Paper_id: 2303.15651v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanopticVis: Integrated Panoptic Segmentation for Visibility Estimation at Twilight and Night\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TubeFormer-DeepLab: Video Mask Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Mask R-CNN\n",
      "Paper_id: 1703.06870v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Boundary-preserving Mask R-CNN\n",
      "Paper_id: 2007.08921v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Building Extraction from Satellite Images Using Mask R-CNN with Building Boundary Regularization\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep High-Resolution Representation Learning for Human Pose Estimation\n",
      "Paper_id: 1902.09212v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Stacked Hourglass Networks for Human Pose Estimation\n",
      "Paper_id: 1603.06937v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\n",
      "Paper_id: 1611.08050v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects\n",
      "\n",
      "\n",
      "🎯 Found paper: Simple Baselines for Human Pose Estimation and Tracking\n",
      "Paper_id: 1804.06208v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Effective Whole-body Pose Estimation with Two-stages Distillation\n",
      "Paper_id: 2307.15880v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation with Spatial and Temporal Transformers\n",
      "Paper_id: 2103.10455v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HuMoR: 3D Human Motion Model for Robust Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2D Human Pose Estimation: New Benchmark and State of the Art Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DensePose: Dense Human Pose Estimation in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CosyPose: Consistent multi-view multi-object 6D pose estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Cascaded Pyramid Network for Multi-Person Pose Estimation\n",
      "Paper_id: 1711.07319v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: A simple yet effective baseline for 3d human pose estimation\n",
      "Paper_id: 1705.03098v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation via Intuitive Physics\n",
      "Paper_id: 2303.18246v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face detection, pose estimation, and landmark localization in the wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TokenPose: Learning Keypoint Tokens for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RMPE: Regional Multi-person Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OnePose: One-Shot Object Pose Estimation without CAD Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Graph Stacked Hourglass Networks for 3D Human Pose Estimation\n",
      "Paper_id: 2103.16385v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepPose: Human Pose Estimation via Deep Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Multi-Person Pose Estimation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffPose: Toward More Reliable 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression\n",
      "Paper_id: 2104.02300v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gaussian Activated Neural Radiance Fields for High Fidelity Reconstruction and Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OSOP: A Multi-Stage One Shot Object Pose Estimation Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FS6D: Few-Shot 6D Pose Estimation of Novel Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GraFormer: Graph-oriented Transformer for 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation\n",
      "Paper_id: 2205.01271v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HybridPose: 6D Object Pose Estimation Under Hybrid Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-Shot Category-Level Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ego-Body Pose Estimation via Ego-Head Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Distribution-Aware Coordinate Representation for Human Pose Estimation\n",
      "Paper_id: 1910.06278v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Hand Shape and Pose Estimation from a Single RGB Image\n",
      "Paper_id: 1903.00812v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PifPaf: Composite Fields for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pose for Everything: Towards Category-Agnostic Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: A Unified Framework for Domain Adaptive Pose Estimation\n",
      "Paper_id: 2204.00172v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimPoE: Simulated Character Control for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimCC: A Simple Coordinate Classification Perspective for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Localizing Human Keypoints beyond the Bounding Box\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Click Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Virtual Try-On with Pose-Garment Keypoints Guided Inpainting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: All Keypoints You Need: Detecting Arbitrary Keypoints on the Body of Triple, High, and Long Jump Athletes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: X-Pose: Detecting Any Keypoints\n",
      "\n",
      "\n",
      "🎯 Found paper: Fine-Grained Head Pose Estimation Without Keypoints\n",
      "Paper_id: 1710.00925v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Recovery of Human Shape and Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Mesh Estimation from Virtual Markers\n",
      "Paper_id: 2303.11726v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReFit: Recurrent Fitting Network for 3D Human Recovery\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Estimate 3D Human Pose and Shape from a Single Color Image\n",
      "Paper_id: 1805.04092v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Poseur: Direct Human Pose Regression with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diff3DHPE: A Diffusion Model for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 15 Keypoints Is All You Need\n",
      "Paper_id: 1912.02323v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseScript: 3D Human Poses from Natural Language\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation = 2D Pose Estimation + Matching\n",
      "Paper_id: 1612.06524v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Online Multi-Person 2D Pose Tracking With Recurrent Spatio-Temporal Affinity Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Animatable Gaussians: Learning Pose-Dependent Gaussian Maps for High-Fidelity Human Avatar Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate 3D Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of Seen and Unseen Rigid Objects\n",
      "\n",
      "\n",
      "🎯 Found paper: Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene\n",
      "Paper_id: 1712.01812v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation\n",
      "Paper_id: 1611.05708v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FoundPose: Unseen Object Pose Estimation with Foundation Features\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop\n",
      "\n",
      "\n",
      "🎯 Found paper: On the Benefits of 3D Pose and Tracking for Human Action Recognition\n",
      "Paper_id: 2304.01199v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single-Stage is Enough: Multi-Person Absolute 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation\n",
      "Paper_id: 2108.07181v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TesseTrack: End-to-End Learnable Multi-Person Articulated 3D Pose Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views\n",
      "Paper_id: 1901.04111v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo\n",
      "Paper_id: 2104.02273v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Motion Guided 3D Pose Estimation from Videos\n",
      "Paper_id: 2004.13985v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS\n",
      "Paper_id: 2003.03972v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild\n",
      "Paper_id: 1803.11493v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Person 3D Pose Estimation and Tracking in Sports\n",
      "\n",
      "\n",
      "🎯 Found paper: Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation\n",
      "Paper_id: 2004.00329v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Propagating LSTM: 3D Pose Estimation Based on Joint Interdependency\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised 3D Pose Estimation with Geometric Self-Supervision\n",
      "Paper_id: 1904.04812v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Regression using Convolutional Neural Networks\n",
      "Paper_id: 1708.05628v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion\n",
      "\n",
      "\n",
      "🎯 Found paper: Distill Knowledge from NRSfM for Weakly Supervised 3D Pose Learning\n",
      "Paper_id: 1908.06377v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "🎯 Found paper: Recurrent 3D Pose Sequence Machines\n",
      "Paper_id: 1707.09695v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ChatPose: Chatting about 3D Human Pose\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Descriptors for Object Recognition and 3D Pose Estimation\n",
      "Paper_id: 1502.05908v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bayesian Image Based 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: A Dual-Source Approach for 3D Pose Estimation from a Single Image\n",
      "Paper_id: 1509.06720v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Regression using Convolutional Neural Networks\n",
      "Paper_id: 1708.05628v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A coarse-to-fine model for 3D pose estimation and sub-category recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tracking People by Predicting 3D Appearance, Location and Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GFPose: Learning 3D Human Pose Prior with Gradient Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Online Object Tracking: A Benchmark\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TrackFormer: Multi-Object Tracking with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeqTrack: Sequence to Sequence Learning for Visual Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTR: End-to-End Multiple-Object Tracking with TRansformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Referring Multi-Object Tracking\n",
      "Paper_id: 2303.03366v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast Online Object Tracking and Segmentation: A Unifying Approach\n",
      "Paper_id: 1812.05050v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Quasi-Dense Similarity Learning for Multiple Object Tracking\n",
      "Paper_id: 2006.06664v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Real-Time Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Distractor-aware Siamese Networks for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Know Your Surroundings: Exploiting Scene Information for Object Tracking\n",
      "Paper_id: 2003.11014v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Deformable Siamese Attention Networks for Visual Object Tracking\n",
      "Paper_id: 2004.06711v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEHAVE: Dataset and Method for Tracking Human Object Interactions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MeMOT: Multi-Object Tracking with Memory\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Grand Unification of Object Tracking\n",
      "Paper_id: 2207.07078v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spiking Transformers for Event-based Single Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Visual object tracking using adaptive correlation filters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Unified Transformer Tracker for Object Tracking\n",
      "Paper_id: 2203.15175v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Object Tracking Meets Moving UAV\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Seventh Visual Object Tracking VOT2019 Challenge Results\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTS: Multi-Object Tracking and Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning a Neural Solver for Multiple Object Tracking\n",
      "Paper_id: 1912.07515v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiple Object Tracking with Correlation Learning\n",
      "Paper_id: 2104.03541v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SiamMOT: Siamese Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PTTR: Relational 3D Point Cloud Object Tracking with Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking With 2D-3D Multi-Feature Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Object Tracking by Jointly Exploiting Frame and Event Domain\n",
      "Paper_id: 2109.09052v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Ninth Visual Object Tracking VOT2021 Challenge Results\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning a Proposal Classifier for Multiple Object Tracking\n",
      "Paper_id: 2103.07889v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Improving Multiple Object Tracking with Single Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: A Twofold Siamese Network for Real-Time Object Tracking\n",
      "Paper_id: 1802.08817v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Simple 3D Multi-Object Tracking for Autonomous Driving\n",
      "Paper_id: 2108.10312v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Saliency-Associated Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VirtualWorlds as Proxy for Multi-object Tracking Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Triplet Loss in Siamese Network for Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Unified Object Motion and Affinity Model for Online Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ocean: Object-aware Anchor-free Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Dynamic Siamese Network for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Efficient Adversarial Attacks for Visual Object Tracking\n",
      "Paper_id: 2008.00217v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Track: Online Multi-object Tracking by Decision Making\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Focus On Details: Online Multi-Object Tracking with Diverse Fine-Grained Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Multi-Modality Multi-Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Online Multi-Object Tracking with Dual Matching Attention Networks\n",
      "Paper_id: 1902.00749v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseTrack21: A Dataset for Person Search, Multi-Object Tracking and Multi-Person Pose Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Adiabatic Quantum Computing for Multi Object Tracking\n",
      "Paper_id: 2202.08837v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-object Tracking with Neural Gating Using Bilinear LSTM\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning of Global Objective for Network Flow in Multi-Object Tracking\n",
      "Paper_id: 2203.16210v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Multi-Object Tracking by Marginal Inference\n",
      "Paper_id: 2208.03727v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spatial-Temporal Relation Networks for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOT: Masked Optimal Transport for Partial Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Distractor-Aware Fast Tracking via Dynamic Convolutions and MOT Philosophy\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multiple Hypothesis Tracking Revisited\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tracking the Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Track to Detect and Segment: An Online Multi-Object Tracker\n",
      "\n",
      "\n",
      "🎯 Found paper: Rank & Sort Loss for Object Detection and Instance Segmentation\n",
      "Paper_id: 2107.11669v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SORT: Second-Order Response Transform for Visual Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Enhancing Retail Checkout through Video Inpainting, YOLOv8 Detection, and DeepSort Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Monocular Depth Estimation with Left-Right Consistency\n",
      "Paper_id: 1609.03677v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniDepth: Universal Monocular Metric Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Digging Into Self-Supervised Monocular Depth Estimation\n",
      "Paper_id: 1806.01260v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Ordinal Regression Network for Monocular Depth Estimation\n",
      "Paper_id: 1806.02446v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaBins: Depth Estimation Using Adaptive Bins\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: iDisc: Internal Discretization for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Zero-Shot Scale-Aware Monocular Depth Estimation\n",
      "Paper_id: 2306.17253v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Monocular Depth Estimation under Challenging Conditions\n",
      "Paper_id: 2308.09711v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Depth Estimation from Thermal Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NDDepth: Normal-Distance Assisted Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural Window Fully-connected CRFs for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Packing for Self-Supervised Monocular Depth Estimation\n",
      "Paper_id: 1905.02693v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LocalBins: Improving Depth Estimation by Learning Local Distributions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanoFormer: Panorama Transformer for Indoor 360° Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Occlusion-Aware Cost Constructor for Light Field Depth Estimation\n",
      "Paper_id: 2203.01576v1\n"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "for paper in papers:\n",
    "    query = paper[\"title\"].strip()\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "\n",
    "    # 정확히 제목 기반으로 arXiv 검색\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=ti:{encoded_query}&max_results=5\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    found_match = False\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        arxiv_title = entry.title.strip()\n",
    "\n",
    "        if query.lower() == arxiv_title.lower():\n",
    "            found_match = True\n",
    "            paper_id = entry.id.split(\"/\")[-1]\n",
    "            print(f\"🎯 Found paper: {arxiv_title}\")\n",
    "            print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "            # 논문 소스 다운로드\n",
    "            url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "            max_try = 0\n",
    "            while max_try < 5:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                        f.write(response.content)\n",
    "                    print(\"📦 Source file downloaded\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Retrying download...\")\n",
    "                    max_try += 1\n",
    "            else:\n",
    "                print(\"❌ Failed to download after 5 tries\")\n",
    "                continue\n",
    "\n",
    "            # 압축 해제\n",
    "            tar_path = f\"{paper_id}.tar.gz\"\n",
    "            try:\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(\"SourceFiles\")\n",
    "                os.remove(tar_path)\n",
    "            except tarfile.ReadError:\n",
    "                print(f\"❌ {tar_path} is not a valid gzip file. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # .tex 파일 추출\n",
    "            tex_file = None\n",
    "            for fname in os.listdir(\"SourceFiles\"):\n",
    "                if fname.endswith(\".tex\"):\n",
    "                    tex_file = fname\n",
    "                    break\n",
    "\n",
    "            if not tex_file:\n",
    "                print(\"❌ No .tex file found.\")\n",
    "                shutil.rmtree(\"SourceFiles\")\n",
    "                continue\n",
    "\n",
    "            with open(os.path.join(\"SourceFiles\", tex_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                latex_text = f.read()\n",
    "\n",
    "            splitBy_section = split_section(latex_text)\n",
    "\n",
    "            paper_entry = {\n",
    "                \"title\": arxiv_title,\n",
    "                \"paper_id\": paper_id,\n",
    "                \"sections\": []\n",
    "            }\n",
    "\n",
    "            for section_name, content in splitBy_section:\n",
    "                cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\", \"\", content, flags=re.DOTALL)\n",
    "                lines = cleaned_content.strip().splitlines()\n",
    "                cleaned_content = \"\\n\".join([line for line in lines if line.strip() and not line.strip().startswith(\"%\")])\n",
    "                paper_entry[\"sections\"].append({section_name: cleaned_content})\n",
    "\n",
    "            json_data.append(paper_entry)\n",
    "            shutil.rmtree(\"SourceFiles\")\n",
    "            break  # 일치하는 제목 찾으면 종료\n",
    "\n",
    "    if not found_match:\n",
    "        print(\"⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\")\n",
    "        print(f\"query: {query}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35d37d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Residual Attention Network for Image Classification',\n",
       "  'paper_id': '1704.06904v1',\n",
       "  'sections': [{'abstract': '%Mixed nature of human attention has been proposed in the literature of biology and been applied to sequential learning task using RNN and LSTM.\\nIn this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\n%\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\n\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'},\n",
       "   {'Introduction': '\\\\begin{figure*}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{.9\\\\linewidth}{0pt}}\\n\\\\includegraphics[width=1\\\\linewidth]{motivation.pdf}\\n\\\\end{center}\\n   \\\\caption{\\\\textbf{Left:} an example shows the interaction between features and attention masks. \\\\textbf{Right:} example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes low-level background blue color features. The balloon instance mask highlights high-level balloon bottom part features.}\\n\\\\label{fig:motivation}\\n\\\\end{figure*}\\n\\nNot only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\n\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\n\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\n\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n%\\n\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'},\n",
       "   {'Related Work': 'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\n\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\n\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\n\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\n\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\n\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\n\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\n\\n\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.\\n\\n%-------------------------------------------------------------------------'},\n",
       "   {'Residual Attention Network': \"\\\\begin{figure*}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n  \\\\includegraphics[width=1.0\\\\linewidth]{whole_net.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of Attention Module: $p,t$ and $r$. The hyper-parameter $p$ denotes the number of pre-processing Residual Units before splitting into trunk branch and mask branch. $t$ denotes the number of Residual Units in trunk branch. $r$ denotes the number of Residual Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting: $\\\\{p=1$, $t=2$, $r=1\\\\}$. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the same.}\\n\\n\\\\label{fig:Attention}\\n\\\\end{figure*}\\n\\nOur Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\n%\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\n\\\\begin{equation}\\nH_{i,c}(x)=M_{i,c}(x)*T_{i,c}(x)\\n\\\\end{equation}\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\n\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\begin{equation}\\n\\\\frac{\\\\partial M(x, \\\\theta)T(x,\\\\phi)}{\\\\partial \\\\phi} = M(x, \\\\theta)\\\\frac{\\\\partial T(x,\\\\phi)}{\\\\partial \\\\phi}\\n\\\\end{equation}\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\n\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\n\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\n\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\n\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n\\\\begin{equation}\\nH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)\\n\\\\end{equation}\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\n\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n   \\\\includegraphics[width=1\\\\linewidth]{attention.pdf}\\n\\\\end{center}\\n   \\\\caption{The receptive field comparison between mask branch and trunk branch.}\\n\\\\label{fig:attentionunit}\\n\\\\end{figure}\\n\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\n\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n%Using additional classification supervision on mask branch directly leads to 0.5\\\\% performance drop on CIFAR-10.\\n\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\n\\\\begin{eqnarray}\\n&&f_{1}(x_{i,c}) = \\\\frac{1}{1+ exp(-x_{i,c})}\\\\\\\\\\n&&f_{2}(x_{i,c}) = \\\\frac{x_{i,c}}{\\\\|x_{i}\\\\|}\\\\\\\\\\n&&f_{3}(x_{i,c}) = \\\\frac{1}{1+ exp(-(x_{i,c} - \\\\text{mean}_c) / \\\\text{std}_c)}\\n\\\\end{eqnarray}\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nActivation Function & Attention Type & Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n$f_{1}(x)$ & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n$f_{2}(x)$  & Channel Attention &6.24\\\\\\\\\\n\\\\hline\\n$f_{3}(x)$ & Spatial Attention &6.33\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{The test error (\\\\%) on CIFAR-10 of Attention-56 network with different activation functions.}\\n\\\\label{tab:activation_exp}\\n\\\\end{table}\\n\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\footnotesize\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c|c} \\\\hline\\n\\nLayer &Output Size &Attention-56&Attention-92 \\\\\\\\\\n\\\\hline\\nConv1 & 112$\\\\times$112 & \\\\multicolumn{2}{|c}{$7\\\\times 7$, 64, stride 2}  \\\\\\\\\\n\\\\hline\\nMax pooling & 56$\\\\times$56& \\\\multicolumn{2}{|c}{$3\\\\times 3$ stride 2}  \\\\\\\\\\n\\\\hline\\nResidual Unit& 56$\\\\times$56 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 64 \\\\\\\\\\n\\t3\\\\times 3, 64 \\\\\\\\\\n\\t1\\\\times\\t1, 256\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 56$\\\\times$56 & Attention $\\\\times$1 & Attention $\\\\times$1  \\\\\\\\\\n\\\\hline\\nResidual Unit& 28$\\\\times$28 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 128 \\\\\\\\\\n\\t3\\\\times 3, 128 \\\\\\\\\\n\\t1\\\\times\\t1, 512\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 28$\\\\times$28 & Attention $\\\\times$1 & Attention $\\\\times$2  \\\\\\\\\\n\\\\hline\\nResidual Unit& 14$\\\\times$14 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 256 \\\\\\\\\\n\\t3\\\\times 3, 256 \\\\\\\\\\n\\t1\\\\times\\t1, 1024\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 14$\\\\times$14 & Attention $\\\\times$1 & Attention $\\\\times$3  \\\\\\\\\\n\\\\hline\\nResidual Unit& 7$\\\\times$7 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 512 \\\\\\\\\\n\\t3\\\\times 3, 512 \\\\\\\\\\n\\t1\\\\times\\t1, 2048\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 3$\\n}  \\\\\\\\\\n\\\\hline\\nAverage pooling & 1$\\\\times$1& \\\\multicolumn{2}{|c}{$7\\\\times 7$ stride 1}  \\\\\\\\\\n\\\\hline\\nFC,Softmax & \\\\multicolumn{3}{|c}{1000}  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{params$\\\\times 10^6$} & $31.9$ & $51.3$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{FLOPs$\\\\times 10^9$} & $6.2$ &$10.4$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{Trunk depth} & $56 $ & $92$  \\\\\\\\\\n\\\\hline\\n\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Residual Attention Network architecture details for ImageNet. Attention structure is described in Fig.~\\\\ref{fig:Attention}.  We make the size of the smallest output map in each mask branch 7$\\\\times$7 to be consistent with the smallest trunk output map size. Thus 3,2,1 max-pooling layers are used in mask branch with input size 56$\\\\times$56, 28$\\\\times$28, 14$\\\\times$14 respectively.\\n%\\nThe Attention Module is built by pre-activation Residual Unit~\\\\cite{he2016identity} with the number of channels in each stage is the same as ResNet~\\\\cite{resnet2016}.\\n%\\n}\\n\\\\label{tab:attention_structure}\\n\\\\end{table}\\n\\n\\n%-------------------------------------------------------------------------\"},\n",
       "   {'Experiments': \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\n%\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\n%\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\n%\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\n%\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n%\\n\\n\\n\\\\subsection{CIFAR and Analysis}\\n\\n\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\n%\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\n%\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\n%\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\n%\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\n%\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\n\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\n%\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\n%\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\n%\\nWe use original $32\\\\times32$ image for testing.\\n\\n\\n\\\\paragraph{Attention Residual Learning.}\\n\\n\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\n%\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\n%\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\n%\\n% Add a small figure here.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\n%\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\n Network & ARL (Top-1 err. \\\\%) & NAL (Top-1 err.\\\\%)\\\\\\\\\\n\\\\hline\\nAttention-56 &\\\\textbf{5.52} & 5.89\\\\\\\\\\n\\\\hline\\nAttention-92 &\\\\textbf{4.99} & 5.35\\\\\\\\\\n\\\\hline\\nAttention-128 &\\\\textbf{4.44} & 5.57\\\\\\\\\\n\\\\hline\\nAttention-164 &\\\\textbf{4.31} & 7.18\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{\\nClassification error (\\\\%) on CIAFR-10.}\\n\\\\label{tab:learning}\\n\\\\end{table}\\n\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\n%\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\n%\\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\n\\n%\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-15pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n  \\\\includegraphics[width=1\\\\linewidth]{mean_value.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{The mean absolute response of output features in each stage. }\\n\\\\label{fig:mean_response}\\n\\\\end{figure}\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\n%\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\n%\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\n%\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\n%\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\n%\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\n%----------------------------------------------------------------------------------\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\n%\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\n%\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\n%\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\n\\\\begin{table}[h]\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nMask Type  & Attention Type &Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nLocal Convolutions & Local Attention &6.48 \\\\\\\\\\n\\\\hline\\nEncoder and Decoder  & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 using different mask structures.}\\n\\\\label{tab:local_global_attention}\\n\\\\end{table}\\n\\n\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nNoise Level &ResNet-164 err. (\\\\%) & Attention-92 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n10\\\\% &5.93 &5.15\\\\\\\\\\n\\\\hline\\n30\\\\% &6.61 &5.79\\\\\\\\\\n\\\\hline\\n50\\\\% &8.35 &7.27\\\\\\\\\\n\\\\hline\\n70\\\\% &17.21 &15.75\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 with label noises.}\\n\\\\label{tab:noise_label}\\n\\\\end{table}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\n%\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\begin{equation}\\nQ =\\n\\\\left(\\n\\\\begin{matrix}\\nr & \\\\frac{1-r}{9} &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\frac{1-r}{9} &r  &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n\\\\frac{1-r}{9} & \\\\frac{1-r}{9} &\\\\cdots &r \\\\\\\\\\n\\\\end{matrix}\\n\\\\right)_{10\\\\times 10}\\n\\\\end{equation}\\n\\n%\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\n\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\n%\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\n%\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\n%\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\n%\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\n%\\n%The encode-decode structure can fast feedforward the whole image and obtain the global and local information of image.\\n%\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\n%\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\n\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c}\\n\\t\\\\hline\\n   \\tNetwork& params$\\\\times 10^6$ & CIFAR-10  &  CIFAR-100 \\\\\\\\\\n\\t\\\\hline\\n\\tResNet-164~\\\\cite{he2016identity}  & 1.7   & 5.46  & 24.33 \\\\\\\\\\n\\tResNet-1001~\\\\cite{he2016identity} & 10.3   & 4.64  & 22.71 \\\\\\\\\\n\\t\\\\hline\\n\\tWRN-16-8~\\\\cite{zagoruyko2016wide} & 11.0   & 4.81  & 22.07 \\\\\\\\\\n\\tWRN-28-10~\\\\cite{zagoruyko2016wide} & 36.5   & 4.17  & 20.50 \\\\\\\\\\n\\t\\\\hline\\n\\tAttention-92 & 1.9 & 4.99 & 21.71 \\\\\\\\\\n\\tAttention-236 & 5.1 & 4.14 & 21.16 \\\\\\\\\\n\\tAttention-452$\\\\dag$ & 8.6 & \\\\textbf{3.90}  & \\\\textbf{20.45}\\\\\\\\\\n\\t\\\\hline\\n\\\\end{tabular}\\n}\\n\\\\end{center}\\n\\\\caption{Comparisons with state-of-the-art methods on CIFAR-10/100. $\\\\dag$: the Attention-452 consists of Attention Module with hyper-parameters setting: $\\\\{p=2$, $t=4$, $r=3\\\\}$ and 6 Attention Modules per stage. }\\n\\\\label{tab:cifar_results}\\n\\\\end{table}\\n\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\n%\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\n%\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\n%\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\n%\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n%\\n%It worth to mention that, our method is complementary with other state-of-the-art methods which focus on regularization and can achieve better results by applying these advanced techniques.\\n\\n\\n\\\\subsection{ImageNet Classification}\\n\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\n%\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\n%\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n%\\n\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\n%\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\n%\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\n%\\nThe network is trained using SGD with a momentum of $0.9$.\\n%\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\n\\n\\\\paragraph{Mask Influence.}\\n\\n\\\\begin{table*}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c|c|c} \\\\hline\\nNetwork & params$\\\\times 10^6$ &FLOPs$\\\\times 10^9$ & Test Size &Top-1 err. (\\\\%) &Top-5 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nResNet-152~\\\\cite{resnet2016}  &60.2 &11.3 &$224\\\\times224$&22.16 &6.16\\\\\\\\\\n\\\\hline\\nAttention-56 &31.9 &6.3 &$224\\\\times224$&\\\\textbf{21.76} &\\\\textbf{5.9} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNeXt-101 ~\\\\cite{resnext}&44.5 & 7.8&$224\\\\times224$    &21.2 &5.6 \\\\\\\\\\n\\\\hline\\nAttentionNeXt-56 &31.9 & 6.3&$224\\\\times224$  &\\\\textbf{21.2} &\\\\textbf{5.6} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nInception-ResNet-v1~\\\\cite{inception} &- &-&$299\\\\times299$&21.3 &5.5 \\\\\\\\\\n\\\\hline\\nAttentionInception-56 &31.9 & 6.3 &$299\\\\times299$ &\\\\textbf{20.36} &\\\\textbf{5.29} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNet-200~\\\\cite{he2016identity} &64.7 &15.0 &$320\\\\times320$ &20.1  &4.8 \\\\\\\\\\n\\\\hline\\n{Inception-ResNet-v2} &- &- &$299\\\\times299$ &19.9  &4.9 \\\\\\\\\\n\\\\hline\\nAttention-92 &51.3  & 10.4&$320\\\\times320$ &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n%}\\n\\\\end{center}\\n\\t\\\\caption{Single crop validation error on ImageNet.\\n}\\n\\\\label{tab:single_crop_validation_error}\\n\\\\end{table*}\\n\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\n%\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\n%\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\n%\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\n%\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\n%\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\n\\n\\\\paragraph{Different Basic Units.}\\n%\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\n\\n%\\n%\\\\begin{figure}[t]\\n%\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n%  \\\\includegraphics[width=1\\\\linewidth]{images/inception.png}\\n  %\\\\includegraphics{images/whole_net.eps}\\n%\\\\end{center}\\n%   \\\\caption{The simple inception module stucture. The hyper-parameter $c$ denotes the number of channel in one stage. In this experiment, we choose $\\\\{256, 512, 1024, 2048\\\\}$ at feature map $\\\\{56\\\\times56, 28\\\\times28, 14\\\\times14, 7\\\\times7\\\\}$.}\\n%\\\\label{fig:inception}\\n%\\\\end{figure}\\n%\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\n%\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\n%\\nThe results show that our method can be applied on different network structures.\\n\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\n\\n%\\\\begin{table}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n%\\\\begin{tabular}{c|c|c|c|c} \\\\hline\\n%Network &param/M & FLOPs$\\\\times 10^9$ &top-1 err. &top-5 err.\\\\\\\\\\n%\\\\hline\\n%{ResNet-200}~\\\\cite{he2016identity} &64.7 &15.0  &20.1  &4.8 \\\\\\\\\\n%\\\\hline\\n%{Inception-ResNet-v2} &- &-  &19.9  &4.9 \\\\\\\\\\n%\\\\hline\\n%Attention-92 &51.3  & 10.4 &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n%\\\\hline\\n%\\\\end{tabular}\\n%}\\n%\\\\end{center}\\n %\\\\caption{\\n%Comparisons of single crop error on the ILSVRC 2012 validation set. In order to compare fairly, we also test our Attention Network on a %single 320$\\\\times$320 crop.}\\n%\\\\label{tab:imagenet_result}\\n%\\\\end{table}\\n\\n\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\n%\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\n%\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\n%\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\n%\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\n%\\nAbove results suggest that our model enjoys high efficiency and good performance.\\n\\n%Our architecture is parallel to major structure of original network, which is friendly to parallel computation. (3) Stacked Attention Module on $14\\\\times14$ feature map gains $1.3\\\\%$ improvement, contrast to the one of single unit, benefits from more Attention Module.\\n\\n%Note that we test a single 320$\\\\times$320 crop from short side of 320, which is consistent with ResNet-200[].\\n%Although our Attention-80 has significantly computation complexity than pre-activation ResNet-200 [](15.0$\\\\times 10^9$), Our Attention-80 has achieved top-1 error rate of 20.3\\\\%, which is 0.4\\\\% lower than the baseline ResNet-200.\"},\n",
       "   {'Discussion': 'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n\\n\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\n\\\\end{document}'}]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
