{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8342222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hwnam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\hwnam\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d374eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import arxiv\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np # for array manipulation\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "%matplotlib inline \n",
    "import datetime\n",
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import feedparser\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de511a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 2252\n"
     ]
    }
   ],
   "source": [
    "# read filtered_papers.json\n",
    "with open('filtered_papers.json', 'r', encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "print(f\"Number of papers: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d310ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Residual Attention Network for Image Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\남현원\\AppData\\Local\\Temp\\ipykernel_4200\\3935089147.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "Paper found in arXiv\n",
      "Paper found source file\n"
     ]
    }
   ],
   "source": [
    "paper = papers[0]\n",
    "query = paper[\"title\"]\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=1,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    ")\n",
    "\n",
    "for result in search.results():\n",
    "    title = result.title\n",
    "    paper_id = result.entry_id.split(\"/\")[-1]\n",
    "    print(f\"Found paper: {title}\")\n",
    "    print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "    if query == title:\n",
    "        max_try = 0\n",
    "        print(\"Paper found in arXiv\")\n",
    "        \n",
    "        url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "        response = requests.get(url)\n",
    "        while max_try < 5:\n",
    "            if response.status_code == 200:\n",
    "                with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(\"Paper found source file\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Paper not found source file\")\n",
    "                max_try += 1\n",
    "                response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7189e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention-net_camera_ready_merge_final.tex\n"
     ]
    }
   ],
   "source": [
    "tar_path = \"1704.06904v1.tar.gz\"\n",
    "\n",
    "#with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    #tar.extractall(\"Example\")\n",
    "\n",
    "\n",
    "for fname in os.listdir(\"Example\"):\n",
    "    if fname.endswith(\".tex\"):\n",
    "        print(fname)\n",
    "        tex_file = fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff48c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_section(tex_file):\n",
    "    sections = []\n",
    "    match = re.search(r\"\\\\begin{abstract}(?:\\[[^\\]]*\\])?(.*?)\\\\end{abstract}\", tex_file, re.DOTALL)\n",
    "    section_name = \"abstract\"\n",
    "    if match:\n",
    "        content = match.group(1).strip()\n",
    "        sections.append((section_name, content))\n",
    "    else:\n",
    "        print(\"No abstract found\")\n",
    "\n",
    "    pattern = r\"\\\\section\\*?{([^}]+)}\"\n",
    "    matches = list(re.finditer(pattern, tex_file))\n",
    "    print(f\"Number of sections: {len(matches)}\")\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        section_name = match.group(1).strip()\n",
    "        \n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(tex_file)\n",
    "        \n",
    "        content = tex_file[start:end].strip()\n",
    "        sections.append((section_name, content))\n",
    "        \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a188e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sections: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abstract',\n",
       "  '%Mixed nature of human attention has been proposed in the literature of biology and been applied to sequential learning task using RNN and LSTM.\\nIn this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\n%\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\n\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'),\n",
       " ('Introduction',\n",
       "  '\\\\begin{figure*}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{.9\\\\linewidth}{0pt}}\\n\\\\includegraphics[width=1\\\\linewidth]{motivation.pdf}\\n\\\\end{center}\\n   \\\\caption{\\\\textbf{Left:} an example shows the interaction between features and attention masks. \\\\textbf{Right:} example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes low-level background blue color features. The balloon instance mask highlights high-level balloon bottom part features.}\\n\\\\label{fig:motivation}\\n\\\\end{figure*}\\n\\nNot only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\n\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\n\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\n\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n%\\n\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'),\n",
       " ('Related Work',\n",
       "  'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\n\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\n\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\n\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\n\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\n\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\n\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\n\\n\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.\\n\\n%-------------------------------------------------------------------------'),\n",
       " ('Residual Attention Network',\n",
       "  \"\\\\begin{figure*}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n  \\\\includegraphics[width=1.0\\\\linewidth]{whole_net.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of Attention Module: $p,t$ and $r$. The hyper-parameter $p$ denotes the number of pre-processing Residual Units before splitting into trunk branch and mask branch. $t$ denotes the number of Residual Units in trunk branch. $r$ denotes the number of Residual Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting: $\\\\{p=1$, $t=2$, $r=1\\\\}$. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the same.}\\n\\n\\\\label{fig:Attention}\\n\\\\end{figure*}\\n\\nOur Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\n%\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\n\\\\begin{equation}\\nH_{i,c}(x)=M_{i,c}(x)*T_{i,c}(x)\\n\\\\end{equation}\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\n\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\begin{equation}\\n\\\\frac{\\\\partial M(x, \\\\theta)T(x,\\\\phi)}{\\\\partial \\\\phi} = M(x, \\\\theta)\\\\frac{\\\\partial T(x,\\\\phi)}{\\\\partial \\\\phi}\\n\\\\end{equation}\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\n\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\n\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\n\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\n\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n\\\\begin{equation}\\nH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)\\n\\\\end{equation}\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\n\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n   \\\\includegraphics[width=1\\\\linewidth]{attention.pdf}\\n\\\\end{center}\\n   \\\\caption{The receptive field comparison between mask branch and trunk branch.}\\n\\\\label{fig:attentionunit}\\n\\\\end{figure}\\n\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\n\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n%Using additional classification supervision on mask branch directly leads to 0.5\\\\% performance drop on CIFAR-10.\\n\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\n\\\\begin{eqnarray}\\n&&f_{1}(x_{i,c}) = \\\\frac{1}{1+ exp(-x_{i,c})}\\\\\\\\\\n&&f_{2}(x_{i,c}) = \\\\frac{x_{i,c}}{\\\\|x_{i}\\\\|}\\\\\\\\\\n&&f_{3}(x_{i,c}) = \\\\frac{1}{1+ exp(-(x_{i,c} - \\\\text{mean}_c) / \\\\text{std}_c)}\\n\\\\end{eqnarray}\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nActivation Function & Attention Type & Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n$f_{1}(x)$ & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n$f_{2}(x)$  & Channel Attention &6.24\\\\\\\\\\n\\\\hline\\n$f_{3}(x)$ & Spatial Attention &6.33\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{The test error (\\\\%) on CIFAR-10 of Attention-56 network with different activation functions.}\\n\\\\label{tab:activation_exp}\\n\\\\end{table}\\n\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\footnotesize\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c|c} \\\\hline\\n\\nLayer &Output Size &Attention-56&Attention-92 \\\\\\\\\\n\\\\hline\\nConv1 & 112$\\\\times$112 & \\\\multicolumn{2}{|c}{$7\\\\times 7$, 64, stride 2}  \\\\\\\\\\n\\\\hline\\nMax pooling & 56$\\\\times$56& \\\\multicolumn{2}{|c}{$3\\\\times 3$ stride 2}  \\\\\\\\\\n\\\\hline\\nResidual Unit& 56$\\\\times$56 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 64 \\\\\\\\\\n\\t3\\\\times 3, 64 \\\\\\\\\\n\\t1\\\\times\\t1, 256\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 56$\\\\times$56 & Attention $\\\\times$1 & Attention $\\\\times$1  \\\\\\\\\\n\\\\hline\\nResidual Unit& 28$\\\\times$28 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 128 \\\\\\\\\\n\\t3\\\\times 3, 128 \\\\\\\\\\n\\t1\\\\times\\t1, 512\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 28$\\\\times$28 & Attention $\\\\times$1 & Attention $\\\\times$2  \\\\\\\\\\n\\\\hline\\nResidual Unit& 14$\\\\times$14 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 256 \\\\\\\\\\n\\t3\\\\times 3, 256 \\\\\\\\\\n\\t1\\\\times\\t1, 1024\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 14$\\\\times$14 & Attention $\\\\times$1 & Attention $\\\\times$3  \\\\\\\\\\n\\\\hline\\nResidual Unit& 7$\\\\times$7 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 512 \\\\\\\\\\n\\t3\\\\times 3, 512 \\\\\\\\\\n\\t1\\\\times\\t1, 2048\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 3$\\n}  \\\\\\\\\\n\\\\hline\\nAverage pooling & 1$\\\\times$1& \\\\multicolumn{2}{|c}{$7\\\\times 7$ stride 1}  \\\\\\\\\\n\\\\hline\\nFC,Softmax & \\\\multicolumn{3}{|c}{1000}  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{params$\\\\times 10^6$} & $31.9$ & $51.3$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{FLOPs$\\\\times 10^9$} & $6.2$ &$10.4$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{Trunk depth} & $56 $ & $92$  \\\\\\\\\\n\\\\hline\\n\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Residual Attention Network architecture details for ImageNet. Attention structure is described in Fig.~\\\\ref{fig:Attention}.  We make the size of the smallest output map in each mask branch 7$\\\\times$7 to be consistent with the smallest trunk output map size. Thus 3,2,1 max-pooling layers are used in mask branch with input size 56$\\\\times$56, 28$\\\\times$28, 14$\\\\times$14 respectively.\\n%\\nThe Attention Module is built by pre-activation Residual Unit~\\\\cite{he2016identity} with the number of channels in each stage is the same as ResNet~\\\\cite{resnet2016}.\\n%\\n}\\n\\\\label{tab:attention_structure}\\n\\\\end{table}\\n\\n\\n%-------------------------------------------------------------------------\"),\n",
       " ('Experiments',\n",
       "  \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\n%\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\n%\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\n%\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\n%\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n%\\n\\n\\n\\\\subsection{CIFAR and Analysis}\\n\\n\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\n%\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\n%\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\n%\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\n%\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\n%\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\n\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\n%\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\n%\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\n%\\nWe use original $32\\\\times32$ image for testing.\\n\\n\\n\\\\paragraph{Attention Residual Learning.}\\n\\n\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\n%\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\n%\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\n%\\n% Add a small figure here.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\n%\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\n Network & ARL (Top-1 err. \\\\%) & NAL (Top-1 err.\\\\%)\\\\\\\\\\n\\\\hline\\nAttention-56 &\\\\textbf{5.52} & 5.89\\\\\\\\\\n\\\\hline\\nAttention-92 &\\\\textbf{4.99} & 5.35\\\\\\\\\\n\\\\hline\\nAttention-128 &\\\\textbf{4.44} & 5.57\\\\\\\\\\n\\\\hline\\nAttention-164 &\\\\textbf{4.31} & 7.18\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{\\nClassification error (\\\\%) on CIAFR-10.}\\n\\\\label{tab:learning}\\n\\\\end{table}\\n\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\n%\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\n%\\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\n\\n%\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-15pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n  \\\\includegraphics[width=1\\\\linewidth]{mean_value.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{The mean absolute response of output features in each stage. }\\n\\\\label{fig:mean_response}\\n\\\\end{figure}\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\n%\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\n%\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\n%\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\n%\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\n%\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\n%----------------------------------------------------------------------------------\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\n%\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\n%\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\n%\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\n\\\\begin{table}[h]\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nMask Type  & Attention Type &Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nLocal Convolutions & Local Attention &6.48 \\\\\\\\\\n\\\\hline\\nEncoder and Decoder  & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 using different mask structures.}\\n\\\\label{tab:local_global_attention}\\n\\\\end{table}\\n\\n\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nNoise Level &ResNet-164 err. (\\\\%) & Attention-92 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n10\\\\% &5.93 &5.15\\\\\\\\\\n\\\\hline\\n30\\\\% &6.61 &5.79\\\\\\\\\\n\\\\hline\\n50\\\\% &8.35 &7.27\\\\\\\\\\n\\\\hline\\n70\\\\% &17.21 &15.75\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 with label noises.}\\n\\\\label{tab:noise_label}\\n\\\\end{table}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\n%\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\begin{equation}\\nQ =\\n\\\\left(\\n\\\\begin{matrix}\\nr & \\\\frac{1-r}{9} &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\frac{1-r}{9} &r  &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n\\\\frac{1-r}{9} & \\\\frac{1-r}{9} &\\\\cdots &r \\\\\\\\\\n\\\\end{matrix}\\n\\\\right)_{10\\\\times 10}\\n\\\\end{equation}\\n\\n%\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\n\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\n%\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\n%\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\n%\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\n%\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\n%\\n%The encode-decode structure can fast feedforward the whole image and obtain the global and local information of image.\\n%\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\n%\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\n\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c}\\n\\t\\\\hline\\n   \\tNetwork& params$\\\\times 10^6$ & CIFAR-10  &  CIFAR-100 \\\\\\\\\\n\\t\\\\hline\\n\\tResNet-164~\\\\cite{he2016identity}  & 1.7   & 5.46  & 24.33 \\\\\\\\\\n\\tResNet-1001~\\\\cite{he2016identity} & 10.3   & 4.64  & 22.71 \\\\\\\\\\n\\t\\\\hline\\n\\tWRN-16-8~\\\\cite{zagoruyko2016wide} & 11.0   & 4.81  & 22.07 \\\\\\\\\\n\\tWRN-28-10~\\\\cite{zagoruyko2016wide} & 36.5   & 4.17  & 20.50 \\\\\\\\\\n\\t\\\\hline\\n\\tAttention-92 & 1.9 & 4.99 & 21.71 \\\\\\\\\\n\\tAttention-236 & 5.1 & 4.14 & 21.16 \\\\\\\\\\n\\tAttention-452$\\\\dag$ & 8.6 & \\\\textbf{3.90}  & \\\\textbf{20.45}\\\\\\\\\\n\\t\\\\hline\\n\\\\end{tabular}\\n}\\n\\\\end{center}\\n\\\\caption{Comparisons with state-of-the-art methods on CIFAR-10/100. $\\\\dag$: the Attention-452 consists of Attention Module with hyper-parameters setting: $\\\\{p=2$, $t=4$, $r=3\\\\}$ and 6 Attention Modules per stage. }\\n\\\\label{tab:cifar_results}\\n\\\\end{table}\\n\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\n%\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\n%\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\n%\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\n%\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n%\\n%It worth to mention that, our method is complementary with other state-of-the-art methods which focus on regularization and can achieve better results by applying these advanced techniques.\\n\\n\\n\\\\subsection{ImageNet Classification}\\n\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\n%\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\n%\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n%\\n\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\n%\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\n%\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\n%\\nThe network is trained using SGD with a momentum of $0.9$.\\n%\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\n\\n\\\\paragraph{Mask Influence.}\\n\\n\\\\begin{table*}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c|c|c} \\\\hline\\nNetwork & params$\\\\times 10^6$ &FLOPs$\\\\times 10^9$ & Test Size &Top-1 err. (\\\\%) &Top-5 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nResNet-152~\\\\cite{resnet2016}  &60.2 &11.3 &$224\\\\times224$&22.16 &6.16\\\\\\\\\\n\\\\hline\\nAttention-56 &31.9 &6.3 &$224\\\\times224$&\\\\textbf{21.76} &\\\\textbf{5.9} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNeXt-101 ~\\\\cite{resnext}&44.5 & 7.8&$224\\\\times224$    &21.2 &5.6 \\\\\\\\\\n\\\\hline\\nAttentionNeXt-56 &31.9 & 6.3&$224\\\\times224$  &\\\\textbf{21.2} &\\\\textbf{5.6} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nInception-ResNet-v1~\\\\cite{inception} &- &-&$299\\\\times299$&21.3 &5.5 \\\\\\\\\\n\\\\hline\\nAttentionInception-56 &31.9 & 6.3 &$299\\\\times299$ &\\\\textbf{20.36} &\\\\textbf{5.29} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNet-200~\\\\cite{he2016identity} &64.7 &15.0 &$320\\\\times320$ &20.1  &4.8 \\\\\\\\\\n\\\\hline\\n{Inception-ResNet-v2} &- &- &$299\\\\times299$ &19.9  &4.9 \\\\\\\\\\n\\\\hline\\nAttention-92 &51.3  & 10.4&$320\\\\times320$ &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n%}\\n\\\\end{center}\\n\\t\\\\caption{Single crop validation error on ImageNet.\\n}\\n\\\\label{tab:single_crop_validation_error}\\n\\\\end{table*}\\n\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\n%\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\n%\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\n%\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\n%\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\n%\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\n\\n\\\\paragraph{Different Basic Units.}\\n%\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\n\\n%\\n%\\\\begin{figure}[t]\\n%\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n%  \\\\includegraphics[width=1\\\\linewidth]{images/inception.png}\\n  %\\\\includegraphics{images/whole_net.eps}\\n%\\\\end{center}\\n%   \\\\caption{The simple inception module stucture. The hyper-parameter $c$ denotes the number of channel in one stage. In this experiment, we choose $\\\\{256, 512, 1024, 2048\\\\}$ at feature map $\\\\{56\\\\times56, 28\\\\times28, 14\\\\times14, 7\\\\times7\\\\}$.}\\n%\\\\label{fig:inception}\\n%\\\\end{figure}\\n%\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\n%\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\n%\\nThe results show that our method can be applied on different network structures.\\n\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\n\\n%\\\\begin{table}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n%\\\\begin{tabular}{c|c|c|c|c} \\\\hline\\n%Network &param/M & FLOPs$\\\\times 10^9$ &top-1 err. &top-5 err.\\\\\\\\\\n%\\\\hline\\n%{ResNet-200}~\\\\cite{he2016identity} &64.7 &15.0  &20.1  &4.8 \\\\\\\\\\n%\\\\hline\\n%{Inception-ResNet-v2} &- &-  &19.9  &4.9 \\\\\\\\\\n%\\\\hline\\n%Attention-92 &51.3  & 10.4 &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n%\\\\hline\\n%\\\\end{tabular}\\n%}\\n%\\\\end{center}\\n %\\\\caption{\\n%Comparisons of single crop error on the ILSVRC 2012 validation set. In order to compare fairly, we also test our Attention Network on a %single 320$\\\\times$320 crop.}\\n%\\\\label{tab:imagenet_result}\\n%\\\\end{table}\\n\\n\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\n%\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\n%\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\n%\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\n%\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\n%\\nAbove results suggest that our model enjoys high efficiency and good performance.\\n\\n%Our architecture is parallel to major structure of original network, which is friendly to parallel computation. (3) Stacked Attention Module on $14\\\\times14$ feature map gains $1.3\\\\%$ improvement, contrast to the one of single unit, benefits from more Attention Module.\\n\\n%Note that we test a single 320$\\\\times$320 crop from short side of 320, which is consistent with ResNet-200[].\\n%Although our Attention-80 has significantly computation complexity than pre-activation ResNet-200 [](15.0$\\\\times 10^9$), Our Attention-80 has achieved top-1 error rate of 20.3\\\\%, which is 0.4\\\\% lower than the baseline ResNet-200.\"),\n",
       " ('Discussion',\n",
       "  'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n\\n\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\n\\\\end{document}')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Example/\" + tex_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    latex_text = f.read()\n",
    "\n",
    "splitBy_section = split_section(latex_text)\n",
    "\n",
    "splitBy_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c290e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "for section_name, content in splitBy_section:\n",
    "    cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\"\n",
    ", \"\", content, flags=re.DOTALL)\n",
    "    cleaned_content = cleaned_content.strip()\n",
    "    \n",
    "    lines = cleaned_content.splitlines()\n",
    "    cleaned_content = [line for line in lines if line.strip() and not line.startswith(\"%\")]\n",
    "    cleaned_content = \"\\n\".join(cleaned_content)\n",
    "    \n",
    "    json_data.append({\n",
    "        \"section_name\": section_name,\n",
    "        \"content\": cleaned_content\n",
    "    })\n",
    "    \n",
    "with open(\"Example/cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08996fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'section_name': 'abstract', 'content': 'In this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'}, {'section_name': 'Introduction', 'content': 'Not only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'}, {'section_name': 'Related Work', 'content': 'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.'}, {'section_name': 'Residual Attention Network', 'content': \"Our Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\"}, {'section_name': 'Experiments', 'content': \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n\\\\subsection{CIFAR and Analysis}\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\nWe use original $32\\\\times32$ image for testing.\\n\\\\paragraph{Attention Residual Learning.}\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n\\\\subsection{ImageNet Classification}\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\nThe network is trained using SGD with a momentum of $0.9$.\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\\\paragraph{Mask Influence.}\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\\\paragraph{Different Basic Units.}\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\nThe results show that our method can be applied on different network structures.\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\nAbove results suggest that our model enjoys high efficiency and good performance.\"}, {'section_name': 'Discussion', 'content': 'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\\\end{document}'}]\n"
     ]
    }
   ],
   "source": [
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b567552",
   "metadata": {},
   "source": [
    "# Make all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deecc5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\남현원\\AppData\\Local\\Temp\\ipykernel_4200\\2420375828.py:12: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "Paper found in arXiv\n",
      "Paper found source file\n",
      "attention-net_camera_ready_merge_final.tex\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "Found paper: Privacy-Preserving Image Classification Using Vision Transformer\n",
      "Paper_id: 2205.12041v1\n",
      "Paper not found in arXiv\n",
      "query: Residual Attention Network for Image Classification\n",
      "title: Privacy-Preserving Image Classification Using Vision Transformer\n",
      "\n",
      "\n",
      "Found paper: Classification of optics-free images with deep neural networks\n",
      "Paper_id: 2011.05132v1\n",
      "Paper not found in arXiv\n",
      "query: Residual Attention Network for Image Classification\n",
      "title: Classification of optics-free images with deep neural networks\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] 현재 연결은 원격 호스트에 의해 강제로 끊겼습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:488\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    487\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:741\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 741\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:920\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    918\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 920\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    458\u001b[0m context\u001b[38;5;241m.\u001b[39mset_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[1;32m--> 460\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m title \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(\n\u001b[0;32m      7\u001b[0m     query\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[0;32m      8\u001b[0m     max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      9\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortCriterion\u001b[38;5;241m.\u001b[39mRelevance,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:601\u001b[0m, in \u001b[0;36mClient._results\u001b[1;34m(self, search, offset)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, search: Search, offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[Result, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    600\u001b[0m     page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_size)\n\u001b[1;32m--> 601\u001b[0m     feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m feed\u001b[38;5;241m.\u001b[39mentries:\n\u001b[0;32m    603\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot empty first page; stopping generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:656\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n\u001b[0;32m    655\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot error (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_try_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:658\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_feed(url, first_page\u001b[38;5;241m=\u001b[39mfirst_page, _try_index\u001b[38;5;241m=\u001b[39m_try_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    657\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiving up (try \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _try_index, err)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:648\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[1;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    650\u001b[0m     HTTPError,\n\u001b[0;32m    651\u001b[0m     UnexpectedEmptyPageError,\n\u001b[0;32m    652\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[0;32m    653\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\arxiv\\__init__.py:682\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[1;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[0;32m    678\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(to_sleep)\n\u001b[0;32m    680\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m--> 682\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser-agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marxiv.py/2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_request_dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mOK:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(10054, '현재 연결은 원격 호스트에 의해 강제로 끊겼습니다', None, 10054, None))"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "for paper in papers:\n",
    "    title = paper[\"title\"]\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=1,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "    )\n",
    "\n",
    "    for result in search.results():\n",
    "        title = result.title\n",
    "        paper_id = result.entry_id.split(\"/\")[-1]\n",
    "        print(f\"Found paper: {title}\")\n",
    "        print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "        if query == title:\n",
    "            max_try = 0\n",
    "            print(\"Paper found in arXiv\")\n",
    "\n",
    "            url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "            response = requests.get(url)\n",
    "            while max_try < 5:\n",
    "                if response.status_code == 200:\n",
    "                    with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                        f.write(response.content)\n",
    "                    print(\"Paper found source file\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Paper not found source file\")\n",
    "                    max_try += 1\n",
    "                    response = requests.get(url)\n",
    "\n",
    "            tar_path = f\"{paper_id}.tar.gz\"\n",
    "            with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                tar.extractall(\"SourceFiles\")\n",
    "            os.remove(tar_path)\n",
    "\n",
    "            for fname in os.listdir(\"SourceFiles\"):\n",
    "                if fname.endswith(\".tex\"):\n",
    "                    print(fname)\n",
    "                    tex_file = fname\n",
    "\n",
    "            with open(\"SourceFiles/\" + tex_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                latex_text = f.read()\n",
    "            \n",
    "            splitBy_section = split_section(latex_text)\n",
    "            \n",
    "            json_data.append({\n",
    "                    \"title\": title,\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"sections\": []\n",
    "            })\n",
    "            for section_name, content in splitBy_section:\n",
    "                cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\", \"\", content, flags=re.DOTALL)\n",
    "                cleaned_content = cleaned_content.strip()\n",
    "\n",
    "                lines = cleaned_content.splitlines()\n",
    "                cleaned_content = [line for line in lines if line.strip() and not line.startswith(\"%\")]\n",
    "                cleaned_content = \"\\n\".join(cleaned_content)\n",
    "\n",
    "                json_data[-1][\"sections\"].append({\n",
    "                    section_name : content\n",
    "                })\n",
    "\n",
    "            shutil.rmtree(\"SourceFiles\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Paper not found in arXiv\")\n",
    "            print(f\"query: {query}\")\n",
    "            print(f\"title: {title}\")\n",
    "            continue\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eec2e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Found paper: Residual Attention Network for Image Classification\n",
      "Paper_id: 1704.06904v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-column deep neural networks for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Locality-constrained Linear Coding for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bag of Tricks for Image Classification with Convolutional Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Big Self-Supervised Models Advance Medical Image Classification\n",
      "Paper_id: 2101.05224v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Understanding Robustness of Transformers for Image Classification\n",
      "Paper_id: 2103.14586v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Linear spatial pyramid matching using sparse coding for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover’s Distance and Structured Classifiers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Improving the Fisher Kernel for Large-Scale Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Invariant Information Clustering for Unsupervised Image Classification and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CNN-RNN: A Unified Framework for Multi-label Image Classification\n",
      "\n",
      "\n",
      "🎯 Found paper: General Multi-label Image Classification with Transformers\n",
      "Paper_id: 2011.14027v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Benchmarking Adversarial Robustness on Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Power of Ensembles for Active Learning in Image Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning from massive noisy labeled data for image classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Object Detection with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FCOS: Fully Convolutional One-Stage Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EfficientDet: Scalable and Efficient Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Cascade R-CNN: Delving into High Quality Object Detection\n",
      "Paper_id: 1712.00726v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Feature Pyramid Networks for Object Detection\n",
      "Paper_id: 1612.03144v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: You Only Look Once: Unified, Real-Time Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Focal Loss for Dense Object Detection\n",
      "Paper_id: 1708.02002v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointPillars: Fast Encoders for Object Detection From Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CenterNet: Keypoint Triplets for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: DETRs Beat YOLOs on Real-time Object Detection\n",
      "Paper_id: 2304.08069v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cut and Learn for Unsupervised Object Detection and Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Frustum PointNets for 3D Object Detection from RGB-D Data\n",
      "Paper_id: 1711.08488v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Center-based 3D Object Detection and Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DOTA: A Large-Scale Dataset for Object Detection in Aerial Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-view 3D Object Detection Network for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Plain Vision Transformer Backbones for Object Detection\n",
      "Paper_id: 2203.16527v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PETR: Position Embedding Transformation for Multi-View 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffusionDet: Diffusion Model for Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Hough Voting for 3D Object Detection in Point Clouds\n",
      "Paper_id: 1904.09664v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Large Selective Kernel Network for Remote Sensing Object Detection\n",
      "Paper_id: 2303.09030v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Sparse R-CNN: End-to-End Object Detection with Learnable Proposals\n",
      "Paper_id: 2011.12450v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Oriented R-CNN for Object Detection\n",
      "Paper_id: 2108.05699v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Semi-Supervised Object Detection with Soft Teacher\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TOOD: Task-aligned One-stage Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Open World Object Detection\n",
      "Paper_id: 2103.02603v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 22\n",
      "\n",
      "\n",
      "🎯 Found paper: An End-to-End Transformer Model for 3D Object Detection\n",
      "Paper_id: 2109.08141v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReDet: A Rotation-equivariant Detector for Aerial Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Libra R-CNN: Towards Balanced Learning for Object Detection\n",
      "Paper_id: 1904.02701v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Categorical Depth Distribution Network for Monocular 3D Object Detection\n",
      "Paper_id: 2103.01100v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Group-Free 3D Object Detection via Transformers\n",
      "Paper_id: 2104.00678v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Voxel Transformer for 3D Object Detection\n",
      "Paper_id: 2109.02497v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Dynamic Head: Unifying Object Detection Heads with Attentions\n",
      "Paper_id: 2106.08322v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Soft-NMS — Improving Object Detection with One Line of Code\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Camouflaged Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain Adaptive Faster R-CNN for Object Detection in the Wild\n",
      "Paper_id: 1803.03243v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ByteTrack: Multi-Object Tracking by Associating Every Detection Box\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Scale Interactive Network for Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-Domain Adaptive Teacher for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BASNet: Boundary-Aware Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RBP-Pose: Residual Bounding Box Projection for Category-Level Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Bounding Box Estimation Using Deep Learning and Geometry\n",
      "Paper_id: 1612.00496v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open Vocabulary Object Detection with Pseudo Bounding-Box Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Bounding Box Regression with Uncertainty for Accurate Object Detection\n",
      "Paper_id: 1809.08545v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond Bounding-Box: Convex-hull Feature Adaptation for Oriented and Densely Packed Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bounding-Box Channels for Visual Relationship Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Task Self-Supervised Object Detection via Recycling of Bounding Box Annotations\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Intelligent Dialogs for Bounding Box Annotation\n",
      "Paper_id: 1712.08087v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face Detection, Bounding Box Aggregation and Pose Estimation for Robust Facial Landmark Localisation in the Wild\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploit Bounding Box Annotations for Multi-label Object Recognition\n",
      "Paper_id: 1504.05843v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Visual Tracking with Double Bounding Box Model\n",
      "\n",
      "\n",
      "🎯 Found paper: Siamese Box Adaptive Network for Visual Tracking\n",
      "Paper_id: 2003.06761v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLO-World: Real-Time Open-Vocabulary Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss\n",
      "\n",
      "\n",
      "🎯 Found paper: Is Faster R-CNN Doing Well for Pedestrian Detection?\n",
      "Paper_id: 1607.07032v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking the Faster R-CNN Architecture for Temporal Action Localization\n",
      "\n",
      "\n",
      "🎯 Found paper: Applying Faster R-CNN for Object Detection on Malaria Images\n",
      "Paper_id: 1804.09548v2\n",
      "📦 Source file downloaded\n",
      "❌ 1804.09548v2.tar.gz is not a valid gzip file. Skipping...\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Adaptive Faster R-CNN\n",
      "\n",
      "\n",
      "🎯 Found paper: Faster R-CNN Features for Instance Search\n",
      "Paper_id: 1604.08893v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Fully Convolutional Networks for Semantic Segmentation\n",
      "Paper_id: 1411.4038v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Deconvolution Network for Semantic Segmentation\n",
      "Paper_id: 1505.04366v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Segmenter: Transformer for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CCNet: Criss-Cross Attention for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Adapt Structured Output Space for Semantic Segmentation\n",
      "Paper_id: 1802.10349v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Side Adapter Network for Open-Vocabulary Semantic Segmentation\n",
      "Paper_id: 2302.12242v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision\n",
      "Paper_id: 2106.01226v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GroupViT: Semantic Segmentation Emerges from Text Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Object-Contextual Representations for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels\n",
      "Paper_id: 2203.03884v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FDA: Fourier Domain Adaptation for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-view Transformers for real-time Map-view Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-class Token Transformer for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Delivering Arbitrary-Modal Semantic Segmentation\n",
      "Paper_id: 2303.01480v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semi-Supervised Semantic Segmentation With Cross-Consistency Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking BiSeNet For Real-time Semantic Segmentation\n",
      "Paper_id: 2104.13188v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Cross-Image Pixel Contrast for Semantic Segmentation\n",
      "Paper_id: 2101.11939v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Language-Grounded Indoor 3D Semantic Segmentation in the Wild\n",
      "Paper_id: 2204.07761v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST++: Make Self-trainingWork Better for Semi-supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training\n",
      "\n",
      "\n",
      "🎯 Found paper: ICNet for Real-Time Semantic Segmentation on High-Resolution Images\n",
      "Paper_id: 1704.08545v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PIDNet: A Real-time Semantic Segmentation Network Inspired by PID Controllers\n",
      "\n",
      "\n",
      "🎯 Found paper: Context Encoding for Semantic Segmentation\n",
      "Paper_id: 1803.08904v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks\n",
      "Paper_id: 1711.10275v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseASPP for Semantic Segmentation in Street Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Semantic Segmentation\n",
      "Paper_id: 2303.11316v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Modeling the Background for Incremental Learning in Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Source-Free Domain Adaptation for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals\n",
      "Paper_id: 2102.06191v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PLOP: Learning without Forgetting for Continual Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking Semantic Segmentation: A Prototype View\n",
      "Paper_id: 2203.15102v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "🎯 Found paper: Prototype Mixture Models for Few-shot Semantic Segmentation\n",
      "Paper_id: 2008.03898v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Pixel-Level Semantic Affinity with Image-Level Supervision for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds\n",
      "\n",
      "\n",
      "🎯 Found paper: Representation Compensation Networks for Continual Semantic Segmentation\n",
      "Paper_id: 2203.05402v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 13\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Generate Text-Grounded Mask for Open-World Semantic Segmentation from Only Image-Text Pairs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs\n",
      "Paper_id: 1711.09869v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-supervised Augmentation Consistency for Adapting Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Path Aggregation Network for Instance Segmentation\n",
      "Paper_id: 1803.01534v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LVIS: A Dataset for Large Vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLACT: Real-Time Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Hybrid Task Cascade for Instance Segmentation\n",
      "Paper_id: 1901.07518v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaskClustering: View Consensus Based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-End Video Instance Segmentation with Transformers\n",
      "Paper_id: 2011.14503v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional Convolutions for Instance Segmentation\n",
      "Paper_id: 2003.05664v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DVIS: Decoupled Video Instance Segmentation Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: SoftGroup for 3D Instance Segmentation on Point Clouds\n",
      "Paper_id: 2203.01509v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CTVIS: Consistent Training for Online Video Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask-Attention-Free Transformer for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Open-Vocabulary Video Instance Segmentation\n",
      "Paper_id: 2304.01715v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Query Refinement Transformer for 3D Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Sparse Instance Activation for Real-Time Instance Segmentation\n",
      "Paper_id: 2203.12827v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Video Instance Segmentation\n",
      "Paper_id: 1905.04804v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask-Free Video Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Snake for Real-Time Instance Segmentation\n",
      "Paper_id: 2001.01629v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hi4D: 4D Instance Segmentation of Close Human Interaction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WaterMask: Instance Segmentation for Underwater Imagery\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Weakly Supervised Learning of Instance Segmentation With Inter-Pixel Relations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DoNet: Deep De-Overlapping Network for Cytology Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TopoSeg: Topology-Aware Nuclear Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Camouflaged Instance Segmentation via Explicit De-Camouflaging\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: In Defense of Online Models for Video Instance Segmentation\n",
      "Paper_id: 2207.10661v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxInst: High-Performance Instance Segmentation with Box Annotations\n",
      "\n",
      "\n",
      "🎯 Found paper: Hierarchical Aggregation for 3D Instance Segmentation\n",
      "Paper_id: 2108.02350v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Seesaw Loss for Long-Tailed Instance Segmentation\n",
      "Paper_id: 2008.10032v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxSnake: Polygonal Instance Segmentation with Box Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: SAM-guided Graph Cut for 3D Instance Segmentation\n",
      "Paper_id: 2312.08372v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarMask: Single Shot Instance Segmentation With Polar Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CenterMask: Real-Time Anchor-Free Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Box-supervised Instance Segmentation with Level Set Evolution\n",
      "Paper_id: 2207.09055v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Temporally Efficient Vision Transformer for Video Instance Segmentation\n",
      "Paper_id: 2204.08412v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers\n",
      "Paper_id: 2103.12340v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Generalized Framework for Video Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pointly-Supervised Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Panoptic Segmentation\n",
      "Paper_id: 1801.00868v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: You Only Segment Once: Towards Real-Time Panoptic Segmentation\n",
      "Paper_id: 2303.14651v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-vocabulary Panoptic Segmentation with Embedding Modulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UPSNet: A Unified Panoptic Segmentation Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: 4D Panoptic Segmentation as Invariant and Equivariant Field Prediction\n",
      "Paper_id: 2303.15651v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanopticVis: Integrated Panoptic Segmentation for Visibility Estimation at Twilight and Night\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TubeFormer-DeepLab: Video Mask Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Mask R-CNN\n",
      "Paper_id: 1703.06870v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Boundary-preserving Mask R-CNN\n",
      "Paper_id: 2007.08921v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Building Extraction from Satellite Images Using Mask R-CNN with Building Boundary Regularization\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep High-Resolution Representation Learning for Human Pose Estimation\n",
      "Paper_id: 1902.09212v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Stacked Hourglass Networks for Human Pose Estimation\n",
      "Paper_id: 1603.06937v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\n",
      "Paper_id: 1611.08050v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects\n",
      "\n",
      "\n",
      "🎯 Found paper: Simple Baselines for Human Pose Estimation and Tracking\n",
      "Paper_id: 1804.06208v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Effective Whole-body Pose Estimation with Two-stages Distillation\n",
      "Paper_id: 2307.15880v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation with Spatial and Temporal Transformers\n",
      "Paper_id: 2103.10455v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HuMoR: 3D Human Motion Model for Robust Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2D Human Pose Estimation: New Benchmark and State of the Art Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BOP Challenge 2022 on Detection, Segmentation and Pose Estimation of Specific Rigid Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DensePose: Dense Human Pose Estimation in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CosyPose: Consistent multi-view multi-object 6D pose estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Cascaded Pyramid Network for Multi-Person Pose Estimation\n",
      "Paper_id: 1711.07319v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: A simple yet effective baseline for 3d human pose estimation\n",
      "Paper_id: 1705.03098v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation via Intuitive Physics\n",
      "Paper_id: 2303.18246v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face detection, pose estimation, and landmark localization in the wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TokenPose: Learning Keypoint Tokens for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RMPE: Regional Multi-person Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OnePose: One-Shot Object Pose Estimation without CAD Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Graph Stacked Hourglass Networks for 3D Human Pose Estimation\n",
      "Paper_id: 2103.16385v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepPose: Human Pose Estimation via Deep Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Multi-Person Pose Estimation with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffPose: Toward More Reliable 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression\n",
      "Paper_id: 2104.02300v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gaussian Activated Neural Radiance Fields for High Fidelity Reconstruction and Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OSOP: A Multi-Stage One Shot Object Pose Estimation Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FS6D: Few-Shot 6D Pose Estimation of Novel Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GraFormer: Graph-oriented Transformer for 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation\n",
      "Paper_id: 2205.01271v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HybridPose: 6D Object Pose Estimation Under Hybrid Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-Shot Category-Level Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ego-Body Pose Estimation via Ego-Head Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-Pose: A First-Reconstruct-Then-Regress Approach for Weakly-supervised 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Distribution-Aware Coordinate Representation for Human Pose Estimation\n",
      "Paper_id: 1910.06278v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Hand Shape and Pose Estimation from a Single RGB Image\n",
      "Paper_id: 1903.00812v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PifPaf: Composite Fields for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pose for Everything: Towards Category-Agnostic Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: A Unified Framework for Domain Adaptive Pose Estimation\n",
      "Paper_id: 2204.00172v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimPoE: Simulated Character Control for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimCC: A Simple Coordinate Classification Perspective for Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Localizing Human Keypoints beyond the Bounding Box\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Click Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Virtual Try-On with Pose-Garment Keypoints Guided Inpainting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative Spatial Encoding of Keypoints\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: All Keypoints You Need: Detecting Arbitrary Keypoints on the Body of Triple, High, and Long Jump Athletes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: X-Pose: Detecting Any Keypoints\n",
      "\n",
      "\n",
      "🎯 Found paper: Fine-Grained Head Pose Estimation Without Keypoints\n",
      "Paper_id: 1710.00925v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Recovery of Human Shape and Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Mesh Estimation from Virtual Markers\n",
      "Paper_id: 2303.11726v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReFit: Recurrent Fitting Network for 3D Human Recovery\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Estimate 3D Human Pose and Shape from a Single Color Image\n",
      "Paper_id: 1805.04092v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Poseur: Direct Human Pose Regression with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diff3DHPE: A Diffusion Model for 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 15 Keypoints Is All You Need\n",
      "Paper_id: 1912.02323v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseScript: 3D Human Poses from Natural Language\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Human Pose Estimation = 2D Pose Estimation + Matching\n",
      "Paper_id: 1612.06524v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Online Multi-Person 2D Pose Tracking With Recurrent Spatio-Temporal Affinity Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Animatable Gaussians: Learning Pose-Dependent Gaussian Maps for High-Fidelity Human Avatar Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate 3D Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BOP Challenge 2023 on Detection, Segmentation and Pose Estimation of Seen and Unseen Rigid Objects\n",
      "\n",
      "\n",
      "🎯 Found paper: Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene\n",
      "Paper_id: 1712.01812v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation\n",
      "Paper_id: 1611.05708v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FoundPose: Unseen Object Pose Estimation with Foundation Features\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop\n",
      "\n",
      "\n",
      "🎯 Found paper: On the Benefits of 3D Pose and Tracking for Human Action Recognition\n",
      "Paper_id: 2304.01199v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single-Stage is Enough: Multi-Person Absolute 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation\n",
      "Paper_id: 2108.07181v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TesseTrack: End-to-End Learnable Multi-Person Articulated 3D Pose Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast and Robust Multi-Person 3D Pose Estimation from Multiple Views\n",
      "Paper_id: 1901.04111v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo\n",
      "Paper_id: 2104.02273v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Motion Guided 3D Pose Estimation from Videos\n",
      "Paper_id: 2004.13985v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes: The Importance of Multiple Scene Constraints\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS\n",
      "Paper_id: 2003.03972v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild\n",
      "Paper_id: 1803.11493v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Person 3D Pose Estimation and Tracking in Sports\n",
      "\n",
      "\n",
      "🎯 Found paper: Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation\n",
      "Paper_id: 2004.00329v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Propagating LSTM: 3D Pose Estimation Based on Joint Interdependency\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised 3D Pose Estimation with Geometric Self-Supervision\n",
      "Paper_id: 1904.04812v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Regression using Convolutional Neural Networks\n",
      "Paper_id: 1708.05628v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion\n",
      "\n",
      "\n",
      "🎯 Found paper: Distill Knowledge from NRSfM for Weakly Supervised 3D Pose Learning\n",
      "Paper_id: 1908.06377v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "🎯 Found paper: Recurrent 3D Pose Sequence Machines\n",
      "Paper_id: 1707.09695v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ChatPose: Chatting about 3D Human Pose\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Descriptors for Object Recognition and 3D Pose Estimation\n",
      "Paper_id: 1502.05908v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bayesian Image Based 3D Pose Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: A Dual-Source Approach for 3D Pose Estimation from a Single Image\n",
      "Paper_id: 1509.06720v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Pose Regression using Convolutional Neural Networks\n",
      "Paper_id: 1708.05628v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A coarse-to-fine model for 3D pose estimation and sub-category recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tracking People by Predicting 3D Appearance, Location and Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ray3D: ray-based 3D human pose estimation for monocular absolute 3D localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GFPose: Learning 3D Human Pose Prior with Gradient Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Online Object Tracking: A Benchmark\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TrackFormer: Multi-Object Tracking with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeqTrack: Sequence to Sequence Learning for Visual Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTR: End-to-End Multiple-Object Tracking with TRansformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Referring Multi-Object Tracking\n",
      "Paper_id: 2303.03366v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast Online Object Tracking and Segmentation: A Unifying Approach\n",
      "Paper_id: 1812.05050v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Quasi-Dense Similarity Learning for Multiple Object Tracking\n",
      "Paper_id: 2006.06664v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Real-Time Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Distractor-aware Siamese Networks for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Know Your Surroundings: Exploiting Scene Information for Object Tracking\n",
      "Paper_id: 2003.11014v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Deformable Siamese Attention Networks for Visual Object Tracking\n",
      "Paper_id: 2004.06711v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEHAVE: Dataset and Method for Tracking Human Object Interactions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MeMOT: Multi-Object Tracking with Memory\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Grand Unification of Object Tracking\n",
      "Paper_id: 2207.07078v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spiking Transformers for Event-based Single Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Visual object tracking using adaptive correlation filters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Unified Transformer Tracker for Object Tracking\n",
      "Paper_id: 2203.15175v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Object Tracking Meets Moving UAV\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Seventh Visual Object Tracking VOT2019 Challenge Results\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTS: Multi-Object Tracking and Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning a Neural Solver for Multiple Object Tracking\n",
      "Paper_id: 1912.07515v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiple Object Tracking with Correlation Learning\n",
      "Paper_id: 2104.03541v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SiamMOT: Siamese Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PTTR: Relational 3D Point Cloud Object Tracking with Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking With 2D-3D Multi-Feature Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Object Tracking by Jointly Exploiting Frame and Event Domain\n",
      "Paper_id: 2109.09052v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Ninth Visual Object Tracking VOT2021 Challenge Results\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning a Proposal Classifier for Multiple Object Tracking\n",
      "Paper_id: 2103.07889v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Improving Multiple Object Tracking with Single Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: A Twofold Siamese Network for Real-Time Object Tracking\n",
      "Paper_id: 1802.08817v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Simple 3D Multi-Object Tracking for Autonomous Driving\n",
      "Paper_id: 2108.10312v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Saliency-Associated Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VirtualWorlds as Proxy for Multi-object Tracking Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Triplet Loss in Siamese Network for Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Unified Object Motion and Affinity Model for Online Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ocean: Object-aware Anchor-free Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Dynamic Siamese Network for Visual Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Efficient Adversarial Attacks for Visual Object Tracking\n",
      "Paper_id: 2008.00217v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Track: Online Multi-object Tracking by Decision Making\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Focus On Details: Online Multi-Object Tracking with Diverse Fine-Grained Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Multi-Modality Multi-Object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Online Multi-Object Tracking with Dual Matching Attention Networks\n",
      "Paper_id: 1902.00749v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PolarMOT: How Far Can Geometric Relations Take Us in 3D Multi-Object Tracking?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoseTrack21: A Dataset for Person Search, Multi-Object Tracking and Multi-Person Pose Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Adiabatic Quantum Computing for Multi Object Tracking\n",
      "Paper_id: 2202.08837v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-object Tracking with Neural Gating Using Bilinear LSTM\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning of Global Objective for Network Flow in Multi-Object Tracking\n",
      "Paper_id: 2203.16210v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Multi-Object Tracking by Marginal Inference\n",
      "Paper_id: 2208.03727v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spatial-Temporal Relation Networks for Multi-Object Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOT: Masked Optimal Transport for Partial Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Distractor-Aware Fast Tracking via Dynamic Convolutions and MOT Philosophy\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multiple Hypothesis Tracking Revisited\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tracking the Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Track to Detect and Segment: An Online Multi-Object Tracker\n",
      "\n",
      "\n",
      "🎯 Found paper: Rank & Sort Loss for Object Detection and Instance Segmentation\n",
      "Paper_id: 2107.11669v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SORT: Second-Order Response Transform for Visual Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Enhancing Retail Checkout through Video Inpainting, YOLOv8 Detection, and DeepSort Tracking\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Monocular Depth Estimation with Left-Right Consistency\n",
      "Paper_id: 1609.03677v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniDepth: Universal Monocular Metric Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Digging Into Self-Supervised Monocular Depth Estimation\n",
      "Paper_id: 1806.01260v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Ordinal Regression Network for Monocular Depth Estimation\n",
      "Paper_id: 1806.02446v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaBins: Depth Estimation Using Adaptive Bins\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: iDisc: Internal Discretization for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Zero-Shot Scale-Aware Monocular Depth Estimation\n",
      "Paper_id: 2306.17253v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Monocular Depth Estimation under Challenging Conditions\n",
      "Paper_id: 2308.09711v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Depth Estimation from Thermal Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NDDepth: Normal-Distance Assisted Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural Window Fully-connected CRFs for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Packing for Self-Supervised Monocular Depth Estimation\n",
      "Paper_id: 1905.02693v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LocalBins: Improving Depth Estimation by Learning Local Distributions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PanoFormer: Panorama Transformer for Indoor 360° Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Occlusion-Aware Cost Constructor for Light Field Depth Estimation\n",
      "Paper_id: 2203.01576v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploiting Pseudo Labels in a Self-Supervised Learning Framework for Improved Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spike Transformer: Monocular Depth Estimation for Spiking Camera\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gradient-based Uncertainty for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: On the uncertainty of self-supervised monocular depth estimation\n",
      "Paper_id: 2005.06209v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Monocular Trained Depth Estimation Using Self-Attention and Discrete Disparity Volume\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PAD-Net: Multi-tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n",
      "\n",
      "\n",
      "🎯 Found paper: Depth Map Decomposition for Monocular Depth Estimation\n",
      "Paper_id: 2208.10762v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Consistent Video Depth Estimation\n",
      "Paper_id: 2012.05901v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation\n",
      "\n",
      "\n",
      "🎯 Found paper: Guiding Monocular Depth Estimation Using Depth-Attention Volume\n",
      "Paper_id: 2004.02760v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 360MonoDepth: High-Resolution 360° Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry\n",
      "\n",
      "\n",
      "🎯 Found paper: Toward Practical Monocular Indoor Depth Estimation\n",
      "Paper_id: 2112.02306v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 13\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WorDepth: Variational Language Prior for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth\n",
      "Paper_id: 2104.14540v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GEDepth: Ground Embedding for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-supervised Monocular Depth Estimation: Let’s Talk About The Weather\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NSF: Neural Surface Fields for Human Modeling from Monocular Depth\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GasMono: Geometry-Aided Self-Supervised Monocular Depth Estimation for Indoor Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Second Monocular Depth Estimation Challenge\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RM-Depth: Unsupervised Learning of Recurrent Monocular Depth in Dynamic Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DevNet: Self-supervised Monocular Depth Learning via Density Volume Construction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Supervised Deep Learning for Monocular Depth Map Prediction\n",
      "Paper_id: 1702.02706v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CroMo: Cross-Modal Learning for Monocular Depth Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Iterative Geometry Encoding Volume for Stereo Matching\n",
      "Paper_id: 2303.06615v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Markov Random Field for Stereo Matching\n",
      "Paper_id: 2403.11193v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MoCha-Stereo: Motif Channel Attention Network for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation\n",
      "\n",
      "\n",
      "🎯 Found paper: Pyramid Stereo Matching Network\n",
      "Paper_id: 1803.08669v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: High-Frequency Stereo Matching Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Attention Concatenation Volume for Accurate and Efficient Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AANet: Adaptive Aggregation Network for Efficient Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Masked Representation Learning for Domain Generalized Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ELFNet: Evidential Local-global Fusion for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GA-Net: Guided Aggregation Net for End-To-End Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CroCo v2: Improved Cross-view Completion Pre-training for Stereo Matching and Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Revisiting Domain Generalized Stereo Matching Networks from a Feature Consistency Perspective\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GraftNet: Towards Domain Generalized Stereo Matching with a Broad-Spectrum and Task-Oriented Feature\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Bilateral Grid Learning for Stereo Matching Networks\n",
      "Paper_id: 2101.01601v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: A Decomposition Model for Stereo Matching\n",
      "Paper_id: 2104.07516v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Hierarchical Deep Stereo Matching on High-resolution Images\n",
      "Paper_id: 1912.06704v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PCW-Net: Pyramid Combination and Warping Cost Volume for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaStereo: A Simple and Efficient Approach for Adaptive Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain-invariant Stereo Matching Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch\n",
      "\n",
      "\n",
      "🎯 Found paper: Guided Stereo Matching\n",
      "Paper_id: 1905.10107v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic Stereo Matching With Pyramid Cost Volumes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Deep Learning for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WaveletStereo: Learning Wavelet Coefficients of Disparity Map in Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Level Context Ultra-Aggregation for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Visually Imbalanced Stereo Matching\n",
      "\n",
      "\n",
      "🎯 Found paper: Single View Stereo Matching\n",
      "Paper_id: 1803.02612v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bridging Stereo Matching and Optical Flow via Spatiotemporal Correspondence\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cascade Residual Learning: A Two-Stage Convolutional Neural Network for Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MARMVS: Matching Ambiguity Reduced Multiple View Stereo for Efficient Large Scale Scene Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: P-MVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo\n",
      "\n",
      "\n",
      "🎯 Found paper: Left-Right Comparative Recurrent Model for Stereo Matching\n",
      "Paper_id: 1804.00796v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Material-Aware Cross-Spectral Stereo Matching\n",
      "\n",
      "\n",
      "🎯 Found paper: Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains\n",
      "Paper_id: 1803.06641v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FlowNet: Learning Optical Flow with Convolutional Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RAFT: Recurrent All-Pairs Field Transforms for Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\n",
      "\n",
      "\n",
      "🎯 Found paper: FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\n",
      "Paper_id: 1612.01925v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GMFlow: Learning Optical Flow via Global Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Performance of optical flow techniques\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FlowFormer: A Transformer Architecture for Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Naturalistic Open Source Movie for Optical Flow Evaluation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow\n",
      "\n",
      "\n",
      "🎯 Found paper: Secrets of Event-Based Optical Flow\n",
      "Paper_id: 2207.10022v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Global Matching with Overlapping Attention for Optical Flow Estimation\n",
      "Paper_id: 2203.11335v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Optical Flow Estimation using a Spatial Pyramid Network\n",
      "Paper_id: 1611.00850v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Database and Evaluation Methodology for Optical Flow\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Equilibrium Optical Flow Estimation\n",
      "Paper_id: 2204.08442v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Optical Flow with Kernel Patch Attention\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: High Accuracy Optical Flow Estimation Based on a Theory for Warping\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning by Analogy: Reliable Supervision From Transformations for Unsupervised Optical Flow Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion\n",
      "Paper_id: 1812.08156v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AutoFlow: Learning a Better Training Set for Optical Flow\n",
      "\n",
      "\n",
      "🎯 Found paper: What Matters in Unsupervised Optical Flow\n",
      "Paper_id: 2006.04902v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Separable Flow: Learning Motion Cost Volumes for Optical Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: High-Resolution Optical Flow from 1D Attention and Correlation\n",
      "Paper_id: 2104.13918v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Optical Flow from a Few Matches\n",
      "Paper_id: 2104.02166v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spike-FlowNet: Event-based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SelFlow: Self-Supervised Learning of Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Upgrading Optical Flow to 3D Scene Flow Through Optical Expansion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Optical Flow Estimation for Spiking Camera\n",
      "Paper_id: 2110.03916v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Video Stabilization Using Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deepfake Video Detection through Optical Flow Based CNN\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Single Image Optical Flow Estimation with an Event Camera\n",
      "Paper_id: 2004.00347v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Optical Flow Distillation: Towards Efficient and Stable Video Style Transfer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Optical Flow in the Dark\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EpicFlow: Edge-preserving interpolation of correspondences for optical flow\n",
      "\n",
      "\n",
      "🎯 Found paper: Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow\n",
      "Paper_id: 1802.07095v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 14\n",
      "\n",
      "\n",
      "🎯 Found paper: Attacking Optical Flow\n",
      "Paper_id: 1910.10053v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Learning of Multi-Frame Optical Flow with Occlusions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Occlusions, Motion and Depth Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SegFlow: Joint Learning for Video Object Segmentation and Optical Flow\n",
      "\n",
      "\n",
      "🎯 Found paper: Occlusion Aware Unsupervised Learning of Optical Flow\n",
      "Paper_id: 1711.05890v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast Optical Flow using Dense Inverse Search\n",
      "Paper_id: 1603.03590v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Accurate Optical Flow via Direct Cost Volume Processing\n",
      "Paper_id: 1704.07325v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Coarse-to-Fine Patch Match for Large Displacement Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Simultaneous Optical Flow and Intensity Estimation from an Event Camera\n",
      "\n",
      "\n",
      "🎯 Found paper: Optical Flow in Mostly Rigid Scenes\n",
      "Paper_id: 1705.01352v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Object scene flow for autonomous vehicles\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ICP-Flow: LiDAR Scene Flow Estimation with ICP\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes\n",
      "Paper_id: 2011.13084v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: I Can't Believe It's Not Scene Flow!\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo\n",
      "\n",
      "\n",
      "🎯 Found paper: Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision\n",
      "Paper_id: 2303.00462v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-Labelling\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast Neural Scene Flow\n",
      "Paper_id: 2304.09121v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointPWC-Net: Cost Volume on Point Clouds for (Self-)Supervised Scene Flow Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Self-Supervised 3D Scene Flow Estimation Guided by Superpoints\n",
      "Paper_id: 2305.02528v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FLOT: Scene Flow on Point Clouds Guided by Optimal Transport\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FlowNet3D: Learning Scene Flow in 3D Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud Based Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: IHNet: Iterative Hierarchical Network Guided by High-Resolution Estimated Information for Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RigidFlow: Self-Supervised Scene Flow Learning on Point Clouds by Local Rigidity Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: What Matters for 3D Scene Flow Network\n",
      "Paper_id: 2207.09143v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploiting Rigidity Constraints for LiDAR Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SLIM: Self-Supervised LiDAR Scene Flow and Motion Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Weakly Supervised Learning of Rigid 3D Scene Flow\n",
      "Paper_id: 2102.08945v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Object Detection with a Self-supervised Lidar Scene Flow Backbone\n",
      "Paper_id: 2205.00705v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RAFT-3D: Scene Flow using Rigid-Motion Embeddings\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-Scale Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding\n",
      "\n",
      "\n",
      "🎯 Found paper: Self-Supervised Monocular Scene Flow Estimation\n",
      "Paper_id: 2004.04143v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Point-Flow: Self-Supervised Scene Flow Estimation from Point Clouds with Optimal Transport and Random Walk\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Multi-Frame Monocular Scene Flow\n",
      "\n",
      "\n",
      "🎯 Found paper: Just Go with the Flow: Self-Supervised Scene Flow Estimation\n",
      "Paper_id: 1912.00497v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Occlusion Guided Scene Flow Estimation on 3D Point Clouds\n",
      "Paper_id: 2011.14880v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Rigid Instance Scene Flow\n",
      "Paper_id: 1904.08913v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SENSE: A Shared Encoder Network for Scene-Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes\n",
      "\n",
      "\n",
      "🎯 Found paper: Consistency Guided Scene Flow Estimation\n",
      "Paper_id: 2006.11242v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bounding Boxes, Segmentations and Object Coordinates: How Important is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?\n",
      "\n",
      "\n",
      "🎯 Found paper: Occupancy Networks: Learning 3D Reconstruction in Function Space\n",
      "Paper_id: 1812.03828v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReconFusion: 3D Reconstruction with Diffusion Priors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\n",
      "\n",
      "\n",
      "🎯 Found paper: Splatter Image: Ultra-Fast Single-View 3D Reconstruction\n",
      "Paper_id: 2312.13150v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiview Compressive Coding for 3D Reconstruction\n",
      "Paper_id: 2301.08247v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction\n",
      "\n",
      "\n",
      "🎯 Found paper: Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing\n",
      "Paper_id: 2204.08906v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Point Set Generation Network for 3D Object Reconstruction from a Single Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimpleRecon: 3D Reconstruction Without 3D Convolutions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-view 3D Reconstruction with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors\n",
      "\n",
      "\n",
      "🎯 Found paper: What Do Single-view 3D Reconstruction Networks Learn?\n",
      "Paper_id: 1905.03678v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pix2Vox: Context-Aware 3D Reconstruction From Single and Multi-View Images\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural 3D Scene Reconstruction with the Manhattan-world Assumption\n",
      "Paper_id: 2205.02836v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Atlas: End-to-End 3D Scene Reconstruction from Posed Images\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-Dense 3D Reconstruction with a Stereo Event Camera\n",
      "Paper_id: 1807.07429v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D face reconstruction with dense landmarks\n",
      "Paper_id: 2204.02776v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow\n",
      "Paper_id: 2203.15190v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Texture Mapping for 3D Reconstruction with RGB-D Sensor\n",
      "\n",
      "\n",
      "🎯 Found paper: Few-Shot Generalization for Single-Image 3D Reconstruction via Priors\n",
      "Paper_id: 1909.01205v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-1-to-3: Zero-shot One Image to 3D Object\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Accurate 3D Face Reconstruction With Weakly-Supervised Learning: From Single Image to Image Set\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: KillingFusion: Non-rigid 3D Reconstruction without Correspondences\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud\n",
      "\n",
      "\n",
      "🎯 Found paper: Masked Autoencoders for Point Cloud Self-supervised Learning\n",
      "Paper_id: 2203.06604v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Geometric Transformer for Fast and Robust Point Cloud Registration\n",
      "Paper_id: 2202.06688v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point-SLAM: Dense Neural Point Cloud-based SLAM\n",
      "\n",
      "\n",
      "🎯 Found paper: Stratified Transformer for 3D Point Cloud Segmentation\n",
      "Paper_id: 2203.14508v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers\n",
      "\n",
      "\n",
      "🎯 Found paper: Diffusion Probabilistic Models for 3D Point Cloud Generation\n",
      "Paper_id: 2103.01458v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointCLIP: Point Cloud Understanding by CLIP\n",
      "\n",
      "\n",
      "🎯 Found paper: Self-Supervised Pretraining of 3D Features on any Point-Cloud\n",
      "Paper_id: 2101.02691v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary Point-Cloud Object Detection without 3D Annotation\n",
      "\n",
      "\n",
      "🎯 Found paper: Attention-based Point Cloud Edge Sampling\n",
      "Paper_id: 2302.14673v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structure Aware Single-Stage 3D Object Detection From Point Cloud\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: REGTR: End-to-end Point Cloud Correspondences with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Positioning Point-Based Transformer for Point Cloud Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PF-Net: Point Fractal Network for 3D Point Cloud Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GRNet: Gridding Residual Network for Dense Point Cloud Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data\n",
      "\n",
      "\n",
      "🎯 Found paper: Relation-Shape Convolutional Neural Network for Point Cloud Analysis\n",
      "Paper_id: 1904.07601v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Closest Point: Learning Representations for Point Cloud Registration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Score-Based Point Cloud Denoising\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Point Cloud Pre-Training via Occlusion Completion\n",
      "Paper_id: 2010.01089v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SO-Net: Self-Organizing Network for Point Cloud Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Point Cloud Registration Framework Based on Deep Graph Matching\n",
      "Paper_id: 2103.04256v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Boundary Learning for Point Cloud Segmentation\n",
      "Paper_id: 2203.05272v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 15\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Perturbed Self-Distillation: Weakly Supervised Large-Scale Point Cloud Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TopNet: Structural Point Cloud Decoder\n",
      "\n",
      "\n",
      "🎯 Found paper: Cascaded Refinement Network for Point Cloud Completion\n",
      "Paper_id: 2004.03327v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PU-GAN: A Point Cloud Upsampling Adversarial Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointNetLK: Robust & Efficient Point Cloud Registration Using PointNet\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RPVNet: A Deep and Efficient Range-Point-Voxel Fusion Network for LiDAR Point Cloud Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Offboard 3D Object Detection from Point Cloud Sequences\n",
      "Paper_id: 2103.05073v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointWeb: Enhancing Local Neighborhood Features for Point Cloud Processing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Feature-Metric Registration: A Fast Semi-Supervised Approach for Robust Point Cloud Registration Without Correspondences\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TPCN: Temporal Point Cloud Networks for Motion Forecasting\n",
      "\n",
      "\n",
      "🎯 Found paper: Point Cloud Upsampling via Disentangled Refinement\n",
      "Paper_id: 2106.04779v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SCF-Net: Learning Spatial Contextual Features for Large-Scale Point Cloud Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Adaptive Graph Convolution for Point Cloud Analysis\n",
      "Paper_id: 2108.08035v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PU-Net: Point Cloud Upsampling Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VoxelContext-Net: An Octree based Framework for Point Cloud Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Guided Point Contrastive Learning for Semi-supervised Point Cloud Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lepard: Learning partial point cloud matching in rigid and deformable scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Controllable Mesh Generation Through Sparse Latent Point Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Leveraging 2D Data to Learn Textured 3D Mesh Generation\n",
      "Paper_id: 2004.04180v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion\n",
      "\n",
      "\n",
      "🎯 Found paper: Generating 3D faces using Convolutional Mesh Autoencoders\n",
      "Paper_id: 1807.10267v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-view Convolutional Neural Networks for 3D Shape Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Shape Generation and Completion through Point-Voxel Diffusion\n",
      "Paper_id: 2104.03670v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diffusion-Based Signed Distance Fields for 3D Shape Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Implicit Text-Guided 3D Shape Generation\n",
      "Paper_id: 2203.14622v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: View-GCN: View-Based Graph Convolutional Network for 3D Shape Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Texturify: Generating Textures on 3D Shape Surfaces\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised 3D Shape Completion through GAN Inversion\n",
      "Paper_id: 2104.13366v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MVTN: Multi-View Transformation Network for 3D Shape Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning 3D Shape Feature for Texture-insensitive Person Re-identification\n",
      "\n",
      "\n",
      "🎯 Found paper: Local Deep Implicit Functions for 3D Shape\n",
      "Paper_id: 1912.06126v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Implicit Templates for 3D Shape Representation\n",
      "Paper_id: 2011.14565v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiresolution Deep Implicit Functions for 3D Shape Representation\n",
      "Paper_id: 2109.05591v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators\n",
      "\n",
      "\n",
      "🎯 Found paper: Weakly-supervised 3D Shape Completion in the Wild\n",
      "Paper_id: 2008.09110v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DECOR-GAN: 3D Shape Detailization by Conditional Refinement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointGrid: A Deep Network for 3D Shape Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning 3D Shape Completion from Laser Scan Data with Weak Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation\n",
      "\n",
      "\n",
      "🎯 Found paper: Superquadrics Revisited: Learning 3D Shape Parsing beyond Cuboids\n",
      "Paper_id: 1904.09970v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 14\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Descriptor Networks for 3D Shape Synthesis and Analysis\n",
      "Paper_id: 1804.00586v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Im2Struct: Recovering 3D Shape Structure from a Single RGB Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Render4Completion: Synthesizing Multi-View Depth Maps for 3D Shape Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Escaping Plato’s Cave: 3D Shape From Adversarial Rendering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GIFT: A Real-Time and Scalable 3D Shape Search Engine\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Learning 3D Shape Surfaces Using Geometry Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SurfNet: Generating 3D Shape Surfaces Using Deep Residual Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Menagerie: Modeling the 3D shape and pose of animals\n",
      "Paper_id: 1611.07700v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D ShapeNets: A Deep Representation for Volumetric Shapes\n",
      "Paper_id: 1406.5670v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: D-NeRF: Neural Radiance Fields for Dynamic Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields\n",
      "\n",
      "\n",
      "🎯 Found paper: Depth-supervised NeRF: Fewer Views and Faster Training for Free\n",
      "Paper_id: 2107.02791v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Block-NeRF: Scalable Large Scene Neural View Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point-NeRF: Point-based Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild\n",
      "\n",
      "\n",
      "🎯 Found paper: The NeRFect Match: Exploring NeRF Features for Visual Localization\n",
      "Paper_id: 2403.09577v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields\n",
      "\n",
      "\n",
      "🎯 Found paper: Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis\n",
      "Paper_id: 2104.00677v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly- Throughs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: F2-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MVIP-NeRF: Multi-View 3D Inpainting on NeRF Scenes via Diffusion Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field\n",
      "\n",
      "\n",
      "🎯 Found paper: How Far Can We Compress Instant-NGP-Based NeRF?\n",
      "Paper_id: 2406.04101v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SHERF: Generalizable Human NeRF from a Single Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HeadNeRF: A Realtime NeRF-based Parametric Head Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-Editing: Geometry Editing of Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeaThru-NeRF: Neural Radiance Fields in Scattering Media\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deblur-NeRF: Neural Radiance Fields from Blurry Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-Supervised Deep Stereo\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion\n",
      "Paper_id: 2309.08596v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "🎯 Found paper: NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs\n",
      "Paper_id: 2402.08622v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis\n",
      "\n",
      "\n",
      "🎯 Found paper: Lighting up NeRF via Unsupervised Decomposition and Enhancement\n",
      "Paper_id: 2307.10664v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Date Earth Observation NeRF: The Detail Is in the Shadows\n",
      "\n",
      "\n",
      "🎯 Found paper: Flow supervision for Deformable NeRF\n",
      "Paper_id: 2303.16333v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Few-shot NeRF by Adaptive Rendering Loss Regularization\n",
      "Paper_id: 2410.17839v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeRF-XL: Scaling NeRFs with Multiple GPUs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gaussian Splatting SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LSD-SLAM: Large-Scale Direct Monocular SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NICE-SLAM: Neural Implicit Scalable Encoding for SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Photo-SLAM: Real-Time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: vMAP: Vectorised Object Mapping for Neural Field SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SNI-SLAM: Semantic Neural Implicit SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CNN-SLAM: Real-Time Dense Monocular SLAM with Learned Depth Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BAD SLAM: Bundle Adjusted Direct RGB-D SLAM\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Topological SLAM for Visual Navigation\n",
      "Paper_id: 2005.12256v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CodeSLAM - Learning a Compact, Optimisable Representation for Dense Visual SLAM\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Patch Visual SLAM\n",
      "Paper_id: 2408.01654v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Detecting and Suppressing Marine Snow for Underwater Visual SLAM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Visual SLAM for Automated Driving: Exploring the Applications of Deep Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis\n",
      "Paper_id: 1604.02808v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SUN RGB-D: A RGB-D scene understanding benchmark suite\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Rich Features from RGB-D Images for Object Detection and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images\n",
      "Paper_id: 1511.02300v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural RGB-D Surface Reconstruction\n",
      "Paper_id: 2104.04532v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Calibrated RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Selective Self-Mutual Attention for RGB-D Saliency Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Select, Supplement and Focus for RGB-D Saliency Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scan2Cap: Context-aware Dense Captioning in RGB-D Scans\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Depth Completion of a Single RGB-D Image\n",
      "Paper_id: 1803.09326v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Specificity-preserving RGB-D saliency detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RGB-D Local Implicit Function for Depth Completion of Transparent Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scan2CAD: Learning CAD Model Alignment in RGB-D Scans\n",
      "\n",
      "\n",
      "🎯 Found paper: RGB-D Saliency Detection via Cascaded Mutual Information Minimization\n",
      "Paper_id: 2109.07246v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: From Low-Cost Depth Sensors to CAD: Cross-Domain 3D Shape Retrieval via Regression Tree Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real time hand pose estimation using depth sensors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MegaDepth: Learning Single-View Depth Prediction from Internet Photos\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Is Pseudo-Lidar needed for Monocular 3D Object detection?\n",
      "Paper_id: 2108.06417v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Compact Representations for LiDAR Completion and Generation\n",
      "\n",
      "\n",
      "🎯 Found paper: Point Density-Aware Voxels for LiDAR 3D Object Detection\n",
      "Paper_id: 2203.05662v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: LiDAR Snowfall Simulation for Robust 3D Object Detection\n",
      "Paper_id: 2203.15118v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: LiDAR R-CNN: An Efficient and Universal 3D Object Detector\n",
      "Paper_id: 2103.15297v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scribble-Supervised LiDAR Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: LaserMix for Semi-Supervised LiDAR Semantic Segmentation\n",
      "Paper_id: 2207.00026v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Physically Realizable Adversarial Examples for LiDAR Object Detection\n",
      "Paper_id: 2004.00543v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking Pseudo-LiDAR Representation\n",
      "Paper_id: 2008.04582v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene From Sparse LiDAR Data and Single Color Image\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Reliable Fusion of ToF and Stereo Depth Driven by Confidence Measures\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SRA: Fast Removal of General Multipath for ToF Sensors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image Style Transfer Using Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Large-Scale Video Classification with Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Multi-Domain Convolutional Neural Networks for Visual Tracking\n",
      "Paper_id: 1510.07945v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Deformable Convolutional Networks\n",
      "Paper_id: 1703.06211v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\n",
      "\n",
      "\n",
      "🎯 Found paper: 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks\n",
      "Paper_id: 1904.08755v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Non-local Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CMT: Convolutional Neural Networks Meet Vision Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single Image Dehazing via Multi-scale Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-scene crowd counting via deep convolutional neural networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Age and gender classification using convolutional neural networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Compare Image Patches via Convolutional Neural Networks\n",
      "Paper_id: 1504.03641v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Poincaré ResNet\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RepVGG: Making VGG-style ConvNets Great Again\n",
      "\n",
      "\n",
      "🎯 Found paper: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
      "Paper_id: 2103.14030v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViViT: A Video Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaxViT: Multi-Axis Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Rotary Position Embedding for Vision Transformer\n",
      "Paper_id: 2403.13298v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BiFormer: Vision Transformer with Bi-Level Routing Attention\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LAVT: Language-Aware Vision Transformer for Referring Image Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design\n",
      "\n",
      "\n",
      "🎯 Found paper: Question Aware Vision Transformer for Multimodal Reasoning\n",
      "Paper_id: 2402.05472v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention\n",
      "\n",
      "\n",
      "🎯 Found paper: FLatten Transformer: Vision Transformer using Focused Linear Attention\n",
      "Paper_id: 2308.00442v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Vision Transformer with Deformable Attention\n",
      "Paper_id: 2201.00520v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A-ViT: Adaptive Tokens for Efficient Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DropKey for Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MPViT: Multi-Path Vision Transformer for Dense Prediction\n",
      "\n",
      "\n",
      "🎯 Found paper: Boost Vision Transformer with GPU-Friendly Sparsity and Quantization\n",
      "Paper_id: 2305.10727v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Feature Masking Open-Vocabulary Vision Transformer\n",
      "Paper_id: 2309.00775v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FDViT: Improve the Hierarchical Architecture of Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking and Improving Relative Position Encoding for Vision Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Robust Vision Transformer\n",
      "Paper_id: 2105.07926v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rep ViT: Revisiting Mobile CNN From ViT Perspective\n",
      "\n",
      "\n",
      "🎯 Found paper: DeiT III: Revenge of the ViT\n",
      "Paper_id: 2204.07118v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "🎯 Found paper: All are Worth Words: A ViT Backbone for Diffusion Models\n",
      "Paper_id: 2209.12152v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Splicing ViT Features for Semantic Appearance Transfer\n",
      "Paper_id: 2201.00424v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VIT-LENS: Towards Omni-modal Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViT-YOLO:Transformer-Based YOLO for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-attention for ViT-backed Continual Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference\n",
      "\n",
      "\n",
      "🎯 Found paper: Pyramid Adversarial Training Improves ViT Performance\n",
      "Paper_id: 2111.15121v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SwinIR: Image Restoration Using Swin Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Swin Transformer V2: Scaling Up Capacity and Resolution\n",
      "Paper_id: 2111.09883v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Video Swin Transformer\n",
      "Paper_id: 2106.13230v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DN-DETR: Accelerate DETR Training by Introducing Query DeNoising\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DQ-DETR: DETR with Dynamic Query for Tiny Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional DETR for Fast Training Convergence\n",
      "Paper_id: 2108.06152v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Relation DETR: Exploring Explicit Position Relation Prior for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MS-DETR: Efficient DETR Training with Mixed Supervision\n",
      "\n",
      "\n",
      "🎯 Found paper: Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR\n",
      "Paper_id: 2303.07335v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End 3D Dense Captioning with Vote2Cap-DETR\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Fast Convergence of DETR with Spatially Modulated Co-Attention\n",
      "Paper_id: 2108.02404v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dynamic DETR: End-to-End Object Detection with Dynamic Attention\n",
      "\n",
      "\n",
      "🎯 Found paper: Open-Vocabulary DETR with Conditional Matching\n",
      "Paper_id: 2203.11876v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semi-DETR: Semi-Supervised Object Detection with Detection Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UP-DETR: Unsupervised Pre-training for Object Detection with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cascade-DETR: Delving into High-Quality Universal Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Motorcycle Helmet Detection in Real-World Scenarios: Using Co-DETR and Minority Class Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SEED: A Simple and Effective 3D DETR in Point Clouds\n",
      "\n",
      "\n",
      "🎯 Found paper: Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment\n",
      "Paper_id: 2207.13085v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Q-DETR: An Efficient Low-Bit Quantized Detection Transformer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OW-DETR: Open-world Detection Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Accelerating DETR Convergence via Semantic-Aligned Matching\n",
      "Paper_id: 2203.06883v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CBAM: Convolutional Block Attention Module\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image Super-Resolution Using Very Deep Residual Channel Attention Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\n",
      "\n",
      "\n",
      "🎯 Found paper: Dual Attention Network for Scene Segmentation\n",
      "Paper_id: 1809.02983v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Masked-attention Mask Transformer for Universal Image Segmentation\n",
      "Paper_id: 2112.01527v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Coordinate Attention for Efficient Mobile Network Design\n",
      "Paper_id: 2103.02907v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Image Inpainting with Contextual Attention\n",
      "Paper_id: 1801.07892v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ResNeSt: Split-Attention Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Second-Order Attention Network for Single Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Harmonious Attention Network for Person Re-Identification\n",
      "Paper_id: 1802.08122v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Stacked Cross Attention for Image-Text Matching\n",
      "Paper_id: 1803.08024v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "🎯 Found paper: Neighborhood Attention Transformer\n",
      "Paper_id: 2204.07143v5\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-To-End Multi-Task Learning With Attention\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AiATrack: Attention in Attention for Transformer Visual Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image Captioning with Semantic Attention\n",
      "\n",
      "\n",
      "🎯 Found paper: Transformer Interpretability Beyond Attention Visualization\n",
      "Paper_id: 2012.09838v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Attention on Attention for Image Captioning\n",
      "Paper_id: 1908.06954v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AMC: AutoML for Model Compression and Acceleration on Mobile Devices\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CHEX: CHannel EXploration for CNN Model Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiSparse: Disentangled Sparsification for Multitask Model Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Dimensional Pruning: A Unified Framework for Model Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Adversarial Robustness vs. Model Compression, or Both?\n",
      "\n",
      "\n",
      "🎯 Found paper: Online Ensemble Model Compression using Knowledge Distillation\n",
      "Paper_id: 2011.07449v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Discrete Model Compression With Resource Constraint for Deep Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Efficient Model Compression via Learned Global Ranking\n",
      "Paper_id: 1904.12368v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Knowledge Within: Methods for Data-Free Model Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploration and Estimation for Model Compression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structured Multi-Hashing for Model Compression\n",
      "\n",
      "\n",
      "🎯 Found paper: Importance Estimation for Neural Network Pruning\n",
      "Paper_id: 1906.10771v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structural Alignment for Network Pruning through Partial Regularization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Convolutional Neural Network Pruning with Structural Redundancy Reduction\n",
      "\n",
      "\n",
      "🎯 Found paper: Manifold Regularized Dynamic Network Pruning\n",
      "Paper_id: 2103.05861v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Interpretations Steered Network Pruning via Amortized Inferred Saliency Maps\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Network Pruning via Performance Maximization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangled Differentiable Network Pruning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Variational Convolutional Neural Network Pruning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Post-training deep neural network pruning via layer-wise calibration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural Network Pruning With Residual-Connections and Limited-Data\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Learning with Network Pruning\n",
      "\n",
      "\n",
      "🎯 Found paper: Auto Graph Encoder-Decoder for Neural Network Pruning\n",
      "Paper_id: 2011.12641v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression\n",
      "\n",
      "\n",
      "🎯 Found paper: Revisiting Random Channel Pruning for Neural Network Compression\n",
      "Paper_id: 2205.05676v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\n",
      "\n",
      "\n",
      "🎯 Found paper: Autoregressive Image Generation using Residual Quantization\n",
      "Paper_id: 2203.01941v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HAQ: Hardware-Aware Automated Quantization With Mixed Precision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ZeroQ: A Novel Zero Shot Quantization Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision\n",
      "\n",
      "\n",
      "🎯 Found paper: Data-Free Quantization Through Weight Equalization and Bias Correction\n",
      "Paper_id: 1906.04721v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 13\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Learning with Low Precision by Half-wave Gaussian Quantization\n",
      "Paper_id: 1702.00953v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Low-bit Quantization of Neural Networks for Efficient Inference\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LSQ+: Improving low-bit quantization through learnable offsets and better initialization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lost in quantization: Improving particular object retrieval in large scale image databases\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization\n",
      "\n",
      "\n",
      "🎯 Found paper: Quantization Networks\n",
      "Paper_id: 1911.09464v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss\n",
      "\n",
      "\n",
      "🎯 Found paper: Network Quantization with Element-wise Gradient Scaling\n",
      "Paper_id: 2104.00903v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: APQ: Joint Search for Network Architecture, Pruning and Quantization Policy\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Low-bitwidth Data Free Quantization\n",
      "Paper_id: 2003.03603v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Post-Training Piecewise Linear Quantization for Deep Neural Networks\n",
      "Paper_id: 2002.00104v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-shot Adversarial Quantization\n",
      "\n",
      "\n",
      "🎯 Found paper: Data-Free Network Quantization With Adversarial Knowledge Distillation\n",
      "Paper_id: 2005.04136v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Iterative quantization: A procrustean approach to learning binary codes\n",
      "\n",
      "\n",
      "🎯 Found paper: Adversarial Diffusion Distillation\n",
      "Paper_id: 2311.17042v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: One-Step Diffusion with Distribution Matching Distillation\n",
      "\n",
      "\n",
      "🎯 Found paper: Dataset Distillation by Matching Training Trajectories\n",
      "Paper_id: 2203.11932v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Anomaly Detection via Reverse Distillation from One-Class Embedding\n",
      "Paper_id: 2201.10703v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Decoupled Knowledge Distillation\n",
      "Paper_id: 2203.08679v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Relational Knowledge Distillation\n",
      "Paper_id: 1904.05068v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Revisiting Reverse Distillation for Anomaly Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: On Distillation of Guided Diffusion Models\n",
      "Paper_id: 2210.03142v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TinyViT: Fast Pretraining Distillation for Small Vision Transformers\n",
      "\n",
      "\n",
      "🎯 Found paper: Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection\n",
      "Paper_id: 2303.05892v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Level Logit Distillation\n",
      "\n",
      "\n",
      "🎯 Found paper: Focal and Global Knowledge Distillation for Detectors\n",
      "Paper_id: 2111.11837v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance\n",
      "\n",
      "\n",
      "🎯 Found paper: Revisiting Knowledge Distillation via Label Smoothing Regularization\n",
      "Paper_id: 1909.11723v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Similarity-Preserving Knowledge Distillation\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiresolution Knowledge Distillation for Anomaly Detection\n",
      "Paper_id: 2011.11108v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Knowledge distillation: A good teacher is patient and consistent\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DisWOT: Student Architecture Search for Distillation WithOut Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Masked Generative Distillation\n",
      "Paper_id: 2205.01529v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird's-Eye View\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fast and Accurate Single Image Super-Resolution via Information Distillation Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Lightweight Lane Detection CNNs by Self Attention Distillation\n",
      "Paper_id: 1908.00821v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Variational Information Distillation for Knowledge Transfer\n",
      "Paper_id: 1904.05835v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: On the Efficacy of Knowledge Distillation\n",
      "Paper_id: 1910.01348v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: A Comprehensive Overhaul of Feature Distillation\n",
      "Paper_id: 1904.01866v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structured Knowledge Distillation for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Correlation Congruence for Knowledge Distillation\n",
      "Paper_id: 1904.01802v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross-Image Relational Knowledge Distillation for Semantic Segmentation\n",
      "Paper_id: 2204.06986v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Knowledge Distillation with the Reused Teacher Classifier\n",
      "Paper_id: 2203.14001v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining\n",
      "\n",
      "\n",
      "🎯 Found paper: Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation\n",
      "Paper_id: 2206.02099v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation\n",
      "\n",
      "\n",
      "🎯 Found paper: Knowledge Distillation via the Target-aware Transformer\n",
      "Paper_id: 2205.10793v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Knowledge Distillation Meets Self-Supervision\n",
      "Paper_id: 2006.07114v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Channel-wise Knowledge Distillation for Dense Prediction*\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CutPaste: Self-Supervised Learning for Anomaly Detection and Localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Learning of Pretext-Invariant Representations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Emerging Properties in Self-Supervised Vision Transformers\n",
      "Paper_id: 2104.14294v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond Appearance: A Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: S4L: Self-Supervised Semi-Supervised Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Masked Discrimination for Self-Supervised Learning on Point Clouds\n",
      "Paper_id: 2203.11183v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Benchmarking Self-Supervised Learning on Diverse Pathology Datasets\n",
      "Paper_id: 2212.04690v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Learning of Object Parts for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Learning of Audio-Visual Objects from Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Backdoor Attacks on Self-Supervised Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: On Feature Decorrelation in Self-Supervised Learning\n",
      "Paper_id: 2105.00470v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mean Shift for Self-Supervised Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Back to Event Basics: Self-Supervised Learning of Image Reconstruction for Event Cameras via Photometric Constancy\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Geography-Aware Self-Supervised Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Learning for Unpaired Image-to-Image Translation\n",
      "Paper_id: 2007.15651v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dense Contrastive Learning for Self-Supervised Visual Pre-Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Learning for Compact Single Image Dehazing\n",
      "Paper_id: 2104.09367v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations\n",
      "\n",
      "\n",
      "🎯 Found paper: Parametric Contrastive Learning\n",
      "Paper_id: 2107.12028v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dynamic Graph Enhanced Contrastive Learning for Chest X-Ray Report Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vision-Language Pre-Training with Triple Contrastive Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Twin Contrastive Learning with Noisy Labels\n",
      "Paper_id: 2303.06930v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dynamic Conceptional Contrastive Learning for Generalized Category Discovery\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Change-Aware Sampling and Contrastive Learning for Satellite Images\n",
      "\n",
      "\n",
      "🎯 Found paper: Balanced Contrastive Learning for Long-Tailed Visual Recognition\n",
      "Paper_id: 2207.09052v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-Modal Contrastive Learning for Text-to-Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DetCo: Unsupervised Contrastive Learning for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Unified Contrastive Learning in Image-Text-Label Space\n",
      "Paper_id: 2204.03610v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Targeted Supervised Contrastive Learning for Long-Tailed Recognition\n",
      "Paper_id: 2111.13998v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Dual Contrastive Learning for Unsupervised Image-to-Image Translation\n",
      "Paper_id: 2104.07689v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Selective-Supervised Contrastive Learning with Noisy Labels\n",
      "Paper_id: 2203.04181v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Best of Both Worlds: Multimodal Contrastive Learning with Tabular and Imaging Data\n",
      "\n",
      "\n",
      "🎯 Found paper: A Self-Supervised Descriptor for Image Copy Detection\n",
      "Paper_id: 2202.10261v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Representations of Satellite Images From Metadata Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dual Temperature Helps Contrastive Learning Without Many Negative Samples: Towards Understanding and Simplifying MoCo\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches\n",
      "\n",
      "\n",
      "🎯 Found paper: Momentum Contrast for Unsupervised Visual Representation Learning\n",
      "Paper_id: 1911.05722v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: An Empirical Study of Training Self-Supervised Vision Transformers\n",
      "Paper_id: 2104.02057v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Test-Time Adaptation\n",
      "Paper_id: 2204.10377v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Feature Generating Networks for Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-Shot Learning — The Good, the Bad and the Ugly\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic Autoencoder for Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Synthesized Classifiers for Zero-Shot Learning\n",
      "Paper_id: 1603.00550v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning a Deep Embedding Model for Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Graph Embeddings for Compositional Zero-shot Learning\n",
      "Paper_id: 2102.01987v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-Shot Learning via Semantic Similarity Embedding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Compare: Relation Network for Few-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Open World Compositional Zero-Shot Learning\n",
      "Paper_id: 2101.12609v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Zero-Shot Learning via Synthesized Examples\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Contrastive Embedding for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Leveraging the Invariant Side of Generative Zero-Shot Learning\n",
      "Paper_id: 1904.04092v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Conditional Attributes for Compositional Zero-Shot Learning\n",
      "Paper_id: 2305.17940v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Attention as Disentangler for Compositional Zero-shot Learning\n",
      "Paper_id: 2303.15111v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking Knowledge Graph Propagation for Zero-Shot Learning\n",
      "Paper_id: 1805.11724v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Goal-Oriented Gaze Estimation for Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-modal Cycle-consistent Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: An Empirical Study and Analysis of Generalized Zero-Shot Learning for Object Recognition in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FREE: Feature Refinement for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantics Disentangling for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Attentive Region Embedding Network for Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain-aware Visual Bias Eliminating for Generalized Zero-Shot Learning\n",
      "Paper_id: 2003.13261v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Audiovisual Generalised Zero-shot Learning with Cross-modal Attention and Language\n",
      "\n",
      "\n",
      "🎯 Found paper: Preserving Semantic Relations for Zero-Shot Learning\n",
      "Paper_id: 1803.03049v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generative Dual Adversarial Network for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Zero-Shot Learning via Over-Complete Distribution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Invariant Visual Representations for Compositional Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transferable Contrastive Network for Generalized Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Transductive Unbiased Embedding for Zero-Shot Learning\n",
      "Paper_id: 1803.11320v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Generative Model for Zero Shot Learning Using Conditional Variational Autoencoders\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Attribute Attention for Semantic Disambiguation in Zero-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Temporal and cross-modal attention for audio-visual zero-shot learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gradient Matching Generative Networks for Zero-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions\n",
      "Paper_id: 1812.03664v6\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CANet: Class-Agnostic Segmentation Networks With Iterative Refinement and Attentive Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Transfer Learning for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Revisiting Local Descriptor Based Image-To-Class Measure for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pushing the Limits of Simple Pipelines for Few-Shot Learning: External Data and Fine-Tuning Make a Difference\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Adaptive Subspaces for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Broader Study of Cross-Domain Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Edge-Labeling Graph Neural Network for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-domain Few-shot Learning with Task-specific Adapters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transductive Few-Shot Learning with Prototype-Based Label Propagation by Iterative Graph Refinement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DPGN: Distribution Propagation Graph Network for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Frequency Guidance Matters in Few-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Adversarial Feature Hallucination Networks for Few-Shot Learning\n",
      "Paper_id: 2003.13193v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Instance Credibility Inference for Few-Shot Learning\n",
      "Paper_id: 2003.11853v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Dynamic Alignment via Meta-filter for Few-shot Learning\n",
      "Paper_id: 2103.13582v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Few-Shot Learning With Adaptive Margin Loss\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Finding Task-Relevant Features for Few-Shot Learning by Category Traversal\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pareto Self-Supervised Training for Few-Shot Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Prototype Rectification for Few-Shot Learning\n",
      "Paper_id: 1911.10713v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Task Agnostic Meta-Learning for Few-Shot Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Integrative Few-Shot Learning for Classification and Segmentation\n",
      "Paper_id: 2203.15712v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-shot Learning with Noisy Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Learning With Differentiable Convex Optimization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MetaFSCIL: A Meta-Learning Approach for Few-Shot Class Incremental Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open Domain Generalization with Domain-Augmented Meta-Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Open-Set Recognition Using Meta-Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: iTAML: An Incremental Task-Agnostic Meta-learning Approach\n",
      "\n",
      "\n",
      "🎯 Found paper: Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition\n",
      "Paper_id: 2101.02833v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tracking by Instance Detection: A Meta-Learning Approach\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Balanced Multimodal Learning via On-the-fly Gradient Modulation\n",
      "Paper_id: 2203.15332v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Distribution-Consistent Modal Recovering for Incomplete Multimodal Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: nuScenes: A Multimodal Dataset for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PMR: Prototypical Modal Rebalance for Multimodal Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hypergraph Attention Networks for Multimodal Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Multimodal Representation Learning by Alternating Unimodal Adaptation\n",
      "Paper_id: 2311.10707v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n",
      "Paper_id: 2401.06209v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional Prompt Learning for Vision-Language Models\n",
      "Paper_id: 2203.05557v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VinVL: Revisiting Visual Representations in Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FairCLIP: Harnessing Fairness in Vision-Language Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: The Neglected Tails in Vision-Language Models\n",
      "Paper_id: 2401.12425v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 15\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Test-Time Adaptation of Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PromptKD: Unsupervised Prompt Distillation for Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RegionGPT: Towards Region Understanding Vision Language Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ViTamin: Designing Scalable Vision Models in the Vision-Language Era\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MMA: Multi-Modal Adapter for Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VLP: Vision Language Planning for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BRAVE: Broadening the visual encoding of vision-language models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Volumetric Environment Representation for Vision-Language Navigation\n",
      "Paper_id: 2403.14158v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GeoChat:Grounded Large Vision-Language Model for Remote Sensing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Long-CLIP: Unlocking the Long-Text Capability of CLIP\n",
      "\n",
      "\n",
      "🎯 Found paper: CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection\n",
      "Paper_id: 2301.00785v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: What does CLIP know about a red circle? Visual prompt engineering for VLMs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP\n",
      "\n",
      "\n",
      "🎯 Found paper: Extract Free Dense Labels from CLIP\n",
      "Paper_id: 2112.01071v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MotionCLIP: Exposing Human Motion Generation to CLIP Space\n",
      "\n",
      "\n",
      "🎯 Found paper: Teaching CLIP to Count to Ten\n",
      "Paper_id: 2302.12066v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No\n",
      "Paper_id: 2308.12213v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CRIS: CLIP-Driven Referring Image Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Alpha-CLIP: A CLIP Model Focusing on Wherever you Want\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement\n",
      "\n",
      "\n",
      "🎯 Found paper: Raising the Bar of AI-generated Image Detection with CLIP\n",
      "Paper_id: 2312.00195v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP\n",
      "\n",
      "\n",
      "🎯 Found paper: Turning a CLIP Model into a Scene Text Detector\n",
      "Paper_id: 2302.14338v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Frozen CLIP Models are Efficient Video Learners\n",
      "Paper_id: 2208.03550v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AD-CLIP: Adapting Domains in Prompt Space Using CLIP\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PADCLIP: Pseudo-labeling with Adaptive Debiasing in CLIP for Unsupervised Domain Adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Fine-tuned CLIP Models are Efficient Video Learners\n",
      "Paper_id: 2212.03640v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning\n",
      "Paper_id: 2211.11682v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-KD: An Empirical Study of CLIP Model Distillation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Supervised Spatiotemporal Learning via Video Clip Order Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Conditioned and composed image retrieval combining and partially fine-tuning CLIP-based features\n",
      "\n",
      "\n",
      "🎯 Found paper: Simple but Effective: CLIP Embeddings for Embodied AI\n",
      "Paper_id: 2111.09888v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-Event: Connecting Text and Images with Event Structures\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Unreasonable Effectiveness of CLIP Features for Image Captioning: An Experimental Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ProposalCLIP: Unsupervised Open-Category Object Proposal Generation via Exploiting CLIP Cues\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Per-Clip Video Object Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Disentangling visual and written concepts in CLIP\n",
      "Paper_id: 2206.07835v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GLIGEN: Open-Set Grounded Text-to-Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InstanceDiffusion: Instance-Level Control for Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Instruct-Imagen: Image Generation with Multi-modal Instruction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation\n",
      "\n",
      "\n",
      "🎯 Found paper: Style Aligned Image Generation via Shared Attention\n",
      "Paper_id: 2312.02133v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TediGAN: Text-Guided Diverse Face Image Generation and Manipulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dense Text-to-Image Generation with Attention Modulation\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking FID: Towards a Better Evaluation Metric for Image Generation\n",
      "Paper_id: 2401.09603v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors\n",
      "\n",
      "\n",
      "🎯 Found paper: Rich Human Feedback for Text-to-Image Generation\n",
      "Paper_id: 2312.10240v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D-aware Image Generation using 2D Diffusion Models\n",
      "Paper_id: 2303.17905v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ITI-Gen: Inclusive Text-to-Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DemoFusion: Democratising High-Resolution Image Generation With No $$$\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional Text Image Generation with Diffusion Models\n",
      "Paper_id: 2306.10804v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-shot Image Generation via Cross-domain Correspondence\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation\n",
      "\n",
      "\n",
      "🎯 Found paper: Expressive Text-to-Image Generation with Rich Text\n",
      "Paper_id: 2304.06720v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation\n",
      "Paper_id: 2011.12799v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Evaluating Text-to-Visual Generation with Image-to-Text Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaskSketch: Unpaired Structure-guided Masked Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SpaText: Spatio-Textual Representation for Controllable Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro\n",
      "\n",
      "\n",
      "🎯 Found paper: GAN Prior Embedded Network for Blind Face Restoration in the Wild\n",
      "Paper_id: 2105.06070v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: In-Domain GAN Inversion for Real Image Editing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-To-Image Synthesis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangled Representation Learning GAN for Pose-Invariant Face Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: High-Fidelity GAN Inversion for Image Attribute Editing\n",
      "Paper_id: 2109.06590v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints\n",
      "Paper_id: 1811.08180v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PD-GAN: Probabilistic Diverse GAN for Image Inpainting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GAN Compression: Efficient Architectures for Interactive Conditional GANs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GAN-Control: Explicitly Controllable GANs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing\n",
      "\n",
      "\n",
      "🎯 Found paper: Image Processing Using Multi-Code GAN Prior\n",
      "Paper_id: 1912.07116v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Seeing What a GAN Cannot Generate\n",
      "Paper_id: 1910.11626v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: How good is my GAN?\n",
      "Paper_id: 1807.09499v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StoryGAN: A Sequential Conditional GAN for Story Visualization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HP-GAN: Probabilistic 3D Human Motion Prediction via GAN\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCO-GAN: Generation by Parts via Conditional Coordinating\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis\n",
      "\n",
      "\n",
      "🎯 Found paper: Analyzing and Improving the Image Quality of StyleGAN\n",
      "Paper_id: 1912.04958v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery\n",
      "\n",
      "\n",
      "🎯 Found paper: Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\n",
      "Paper_id: 2008.00951v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleGAN-Human: A Data-Centric Odyssey of Human Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using Pixel-aligned Reconstruction Priors\n",
      "\n",
      "\n",
      "🎯 Found paper: Robust Unsupervised StyleGAN Image Restoration\n",
      "Paper_id: 2302.06733v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: When StyleGAN Meets Stable Diffusion: a $\\mathcal{W}_{+}$ Adapter for Personalized Image Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping\n",
      "\n",
      "\n",
      "🎯 Found paper: Diffusion Model Alignment Using Direct Preference Optimization\n",
      "Paper_id: 2311.12908v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model\n",
      "\n",
      "\n",
      "🎯 Found paper: Your Diffusion Model is Secretly a Zero-Shot Classifier\n",
      "Paper_id: 2303.16203v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffIR: Efficient Diffusion Model for Image Restoration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model\n",
      "\n",
      "\n",
      "🎯 Found paper: Vector Quantized Diffusion Model for Text-to-Image Synthesis\n",
      "Paper_id: 2111.14822v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D\n",
      "\n",
      "\n",
      "🎯 Found paper: Leapfrog Diffusion Model for Stochastic Trajectory Prediction\n",
      "Paper_id: 2303.10895v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PhysDiff: Physics-Guided Human Motion Diffusion Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: High-Resolution Image Synthesis with Latent Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Person Image Synthesis via Denoising Diffusion Model\n",
      "Paper_id: 2211.12500v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Versatile Diffusion: Text, Images and Variations All in One Diffusion Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion\n",
      "\n",
      "\n",
      "🎯 Found paper: Erasing Concepts from Diffusion Models\n",
      "Paper_id: 2303.07345v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structure and Content-Guided Video Synthesis with Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: DIRE for Diffusion-Generated Image Detection\n",
      "Paper_id: 2303.09295v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RePaint: Inpainting using Denoising Diffusion Probabilistic Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Ablating Concepts in Text-to-Image Diffusion Models\n",
      "Paper_id: 2303.13516v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: The Stable Signature: Rooting Watermarks in Latent Diffusion Models\n",
      "Paper_id: 2303.15435v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models\n",
      "Paper_id: 2305.10474v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Unleashing Text-to-Image Diffusion Models for Visual Perception\n",
      "Paper_id: 2303.02153v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image-to-Image Translation with Conditional Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Multimodal Unsupervised Image-to-Image Translation\n",
      "Paper_id: 1804.04732v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DualGAN: Unsupervised Dual Learning for Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Diverse Image-to-Image Translation via Disentangled Representations\n",
      "Paper_id: 1808.00948v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Unsupervised Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Image-to-image Translation via Hierarchical Style Disentanglement\n",
      "Paper_id: 2103.01456v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "🎯 Found paper: General Image-to-Image Translation with One-Shot Image Guidance\n",
      "Paper_id: 2307.14352v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unpaired Image-to-Image Translation with Shortest Path Regularization\n",
      "\n",
      "\n",
      "🎯 Found paper: Unpaired Image-to-Image Translation using Adversarial Consistency Loss\n",
      "Paper_id: 2003.04858v7\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TSIT: A Simple and Versatile Framework for Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Image to Image Translation for Domain Adaptation\n",
      "Paper_id: 1712.00479v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking the Truly Unsupervised Image-to-Image Translation\n",
      "Paper_id: 2006.06500v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InstaFormer: Instance-Aware Image-to-Image Translation with Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Image-to-Image Translation with Generative Prior\n",
      "Paper_id: 2204.03641v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image-To-Image Translation via Group-Wise Deep Whitening-And-Coloring Transformation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Instance-Level Image-To-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VecGAN: Image-to-Image Translation with Interpretable Latent Directions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DUNIT: Detection-Based Unsupervised Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Memory-guided Unsupervised Image-to-image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Guided Image-to-Image Translation With Bi-Directional Feature Transformation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Homomorphic Latent Space Interpolation for Unpaired Image-To-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TransGaGa: Geometry-Aware Unsupervised Image-To-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CoMoGAN: continuous model-guided image-to-image translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TraVeLGAN: Image-To-Image Translation by Transformation Vector Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semi-Supervised Learning for Few-Shot Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Unaligned Image-to-Image Translation by Learning to Reweight\n",
      "Paper_id: 2109.11736v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Improving Shape Deformation in Unsupervised Image-to-Image Translation\n",
      "Paper_id: 1808.04325v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Conditional Image-to-Image Translation\n",
      "Paper_id: 1805.00251v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transformation Consistency Regularization- A Semi-Supervised Paradigm for Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Unpaired Image-to-Image Translation via Latent Energy Transport\n",
      "Paper_id: 2012.00649v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "🎯 Found paper: Reversible GANs for Memory-efficient Image-to-Image Translation\n",
      "Paper_id: 1902.02729v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GANHopper: Multi-Hop GAN for Unsupervised Image-to-Image Translation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Adaptive Image-to-Image Translation\n",
      "\n",
      "\n",
      "🎯 Found paper: Perceptual Losses for Real-Time Style Transfer and Super-Resolution\n",
      "Paper_id: 1603.08155v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyTr2: Image Style Transfer with Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIPstyler: Image Style Transfer with a Single Text Condition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Style Injection in Diffusion: A Training-Free Approach for Adapting Large-Scale Diffusion Models for Style Transfer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Preset for Color Style Transfer\n",
      "Paper_id: 2303.13511v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Style-Based Generator Architecture for Generative Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Inversion-based Style Transfer with Diffusion Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Diffusion-Enhanced PatchMatch: A Framework for Arbitrary Style Transfer with Diffusion Models\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Dynamic Style Kernels for Artistic Style Transfer\n",
      "Paper_id: 2304.00414v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model\n",
      "\n",
      "\n",
      "🎯 Found paper: Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer\n",
      "Paper_id: 2203.13248v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer\n",
      "\n",
      "\n",
      "🎯 Found paper: Photorealistic Style Transfer via Wavelet Transforms\n",
      "Paper_id: 1903.09760v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Graph Neural Networks for Image Style Transfer\n",
      "Paper_id: 2207.11681v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Photo Style Transfer\n",
      "Paper_id: 1703.07511v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity\n",
      "\n",
      "\n",
      "🎯 Found paper: All-to-key Attention for Arbitrary Style Transfer\n",
      "Paper_id: 2212.04105v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: StyleFormer: Real-time Arbitrary Style Transfer via Parametric Style Composition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PCA-Based Knowledge Distillation Towards Lightweight and Content-Style Balanced Photorealistic Style Transfer Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Style-ERD: Responsive and Coherent Online Motion Style Transfer\n",
      "\n",
      "\n",
      "🎯 Found paper: Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement\n",
      "Paper_id: 2001.06826v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: URetinex-Net: Retinex-based Deep Unfolding Network for Low-light Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SNR-Aware Low-light Image Enhancement\n",
      "\n",
      "\n",
      "🎯 Found paper: Iterative Prompt Learning for Unsupervised Backlit Image Enhancement\n",
      "Paper_id: 2303.17569v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement\n",
      "\n",
      "\n",
      "🎯 Found paper: Uncertainty Inspired Underwater Image Enhancement\n",
      "Paper_id: 2207.09689v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Diffusion Prior for Unified Image Restoration and Enhancement\n",
      "Paper_id: 2304.01247v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Local Color Distributions Prior for Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeepLPF: Deep Local Parametric Filters for Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: STAR: A Structure-aware Lightweight Transformer for Real-time Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Representative Color Transform for Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Global and Local Enhancement Networks for Paired and Unpaired Image Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PieNet: Personalized Image Enhancement Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Enhanced Deep Residual Networks for Single Image Super-Resolution\n",
      "Paper_id: 1707.02921v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Residual Dense Network for Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning a Deep Convolutional Network for Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Accurate Image Super-Resolution Using Very Deep Convolutional Networks\n",
      "Paper_id: 1511.04587v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution\n",
      "Paper_id: 1704.03915v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Accelerating the Super-Resolution Convolutional Neural Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SeeSR: Towards Semantics-Aware Real-World Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Activating More Pixels in Image Super-Resolution Transformer\n",
      "Paper_id: 2205.04437v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single image super-resolution from transformed self-exemplars\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deeply-Recursive Convolutional Network for Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Designing a Practical Degradation Model for Deep Blind Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Implicit Diffusion Models for Continuous Super-Resolution\n",
      "Paper_id: 2303.16491v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Dual Aggregation Transformer for Image Super-Resolution\n",
      "Paper_id: 2308.03364v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image Super-Resolution via Deep Recursive Residual Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Degradation Representation Learning for Blind Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NAFSSR: Stereo Image Super-Resolution Using NAFNet\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Long-Range Attention Network for Image Super-resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NTIRE 2023 Challenge on Efficient Super-Resolution: Methods and Results\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Texture Transformer Network for Image Super-Resolution\n",
      "Paper_id: 2006.04139v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Back-Projection Networks for Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Correction to: Single Image Super-Resolution via a Holistic Attention Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Unfolding Network for Image Super-Resolution\n",
      "Paper_id: 2003.10428v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fast, Accurate, and, Lightweight Super-Resolution with Cascading Residual Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Residual Feature Aggregation Network for Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SRFlow: Learning the Super-Resolution Space with Normalizing Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results\n",
      "\n",
      "\n",
      "🎯 Found paper: Residual Local Feature Network for Efficient Super-Resolution\n",
      "Paper_id: 2205.07514v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Structure-Preserving Super Resolution with Gradient Guidance\n",
      "Paper_id: 2003.13081v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real-World Super-Resolution via Kernel Estimation and Noise Injection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transformer for Single Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Feedback Network for Image Super-Resolution\n",
      "Paper_id: 1903.09814v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Burst Super-Resolution\n",
      "Paper_id: 2101.10997v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Face Super-Resolution With Iterative Collaboration Between Attentive Recovery and Landmark Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Hybrid Network of CNN and Transformer for Lightweight Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning a Single Convolutional Super-Resolution Network for Multiple Degradations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scene Text Image Super-Resolution in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Trajectory-Aware Transformer for Video Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-Transfer Learning for Zero-Shot Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Blind Super-Resolution With Iterative Kernel Correction\n",
      "Paper_id: 1904.03377v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Closed-Loop Matters: Dual Regression Networks for Single Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NTIRE 2019 Challenge on Video Deblurring and Super-Resolution: Dataset and Study\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Toward Real-World Single Image Super-Resolution: A New Benchmark and a New Model\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Component Divide-and-Conquer for Real-World Image Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Video Super-Resolution with Recurrent Structure-Detail Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-scale Residual Network for Image Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Sparsity in Image Super-Resolution for Efficient Inference\n",
      "Paper_id: 2006.09603v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recurrent Back-Projection Network for Video Super-Resolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meta-SR: A Magnification-Arbitrary Network for Super-Resolution\n",
      "\n",
      "\n",
      "🎯 Found paper: Explore the Power of Synthetic Data on Few-shot Object Detection\n",
      "Paper_id: 2303.13221v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Going Beyond Nouns With Vision & Language Models Using Synthetic Data\n",
      "Paper_id: 2303.17590v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fake it till you make it: face analysis in the wild using synthetic data alone\n",
      "\n",
      "\n",
      "🎯 Found paper: Synthetic Data for Text Localisation in Natural Images\n",
      "Paper_id: 1604.06646v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning from Synthetic Data for Crowd Counting in the Wild\n",
      "Paper_id: 1903.03303v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SynFace: Face Recognition with Synthetic Data\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?\n",
      "\n",
      "\n",
      "🎯 Found paper: Adversarial Discriminative Domain Adaptation\n",
      "Paper_id: 1702.05464v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Continual Test-Time Domain Adaptation\n",
      "Paper_id: 2203.13591v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Moment Matching for Multi-Source Domain Adaptation\n",
      "Paper_id: 1812.01754v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Hashing Network for Unsupervised Domain Adaptation\n",
      "Paper_id: 1706.07522v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Maximum Classifier Discrepancy for Unsupervised Domain Adaptation\n",
      "Paper_id: 1712.02560v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "🎯 Found paper: Model Adaptation: Unsupervised Domain Adaptation without Source Data\n",
      "Paper_id: 2502.19316v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OVANet: One-vs-All Network for Universal Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Geodesic flow kernel for unsupervised domain adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Generalized Source-free Domain Adaptation\n",
      "Paper_id: 2108.01614v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain Adaptation for Image Dehazing\n",
      "Paper_id: 2005.04668v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Instance Adaptive Self-Training for Unsupervised Domain Adaptation\n",
      "Paper_id: 2008.12197v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "🎯 Found paper: Universal Source-Free Domain Adaptation\n",
      "Paper_id: 2004.04393v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Gradually Vanishing Bridge for Adversarial Domain Adaptation\n",
      "Paper_id: 2003.13183v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Semi-supervised Domain Adaptation via Minimax Entropy\n",
      "Paper_id: 1904.06487v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Multi-source Domain Adaptation Without Access to Source Data\n",
      "\n",
      "\n",
      "🎯 Found paper: Contrastive Adaptation Network for Unsupervised Domain Adaptation\n",
      "Paper_id: 1901.00976v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Bidirectional Learning for Domain Adaptation of Semantic Segmentation\n",
      "Paper_id: 1904.10620v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Source-Free Domain Adaptation via Distribution Estimation\n",
      "Paper_id: 2204.11257v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Consensus Clustering for Universal Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Safe Self-Refinement for Transformer-based Domain Adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Open Set Domain Adaptation by Backpropagation\n",
      "Paper_id: 1804.10427v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Universal Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Domain Adaptation for Nighttime Aerial Tracking\n",
      "Paper_id: 2203.10541v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Domain-Invariant Parameters for Source Free Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Modal Domain Adaptation for Fine-Grained Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Uncertainty-guided Source-free Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Adaptive Adversarial Network for Source-free Domain Adaptation\n",
      "\n",
      "\n",
      "🎯 Found paper: Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation\n",
      "Paper_id: 1903.04064v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Contrastive Regression for Domain Adaptation on Gaze Estimation\n",
      "\n",
      "\n",
      "🎯 Found paper: ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer\n",
      "Paper_id: 2204.02389v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Cycle-Dehaze: Enhanced CycleGAN for Single Image Dehazing\n",
      "\n",
      "\n",
      "🎯 Found paper: Deeper, Broader and Artier Domain Generalization\n",
      "Paper_id: 1710.03077v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Generalization with Adversarial Feature Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Test-Time Domain Generalization for Face Anti-Spoofing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Fourier-based Framework for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Challenging Improves Cross-Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain Generalization by Solving Jigsaw Puzzles\n",
      "Paper_id: 1903.06864v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Domain Generalization via Conditional Invariant Adversarial Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP the Gap: A Single Domain Generalization Approach for Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sharpness-Aware Gradient Matching for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Federated Domain Generalization with Generalization Adjustment\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Learn Single Domain Generalization\n",
      "Paper_id: 2003.13216v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Instance-Aware Domain Generalization for Face Anti-Spoofing\n",
      "\n",
      "\n",
      "🎯 Found paper: Improved Test-Time Adaptation for Domain Generalization\n",
      "Paper_id: 2304.04494v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 15\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment\n",
      "\n",
      "\n",
      "🎯 Found paper: Progressive Random Convolutions for Single Domain Generalization\n",
      "Paper_id: 2304.00424v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "🎯 Found paper: Meta-causal Learning for Single Domain Generalization\n",
      "Paper_id: 2304.03709v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single Domain Generalization for LiDAR Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SelfReg: Self-supervised Contrastive Regularization for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Unsupervised Domain Generalization for Face Anti-Spoofing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bi-Level Meta-Learning for Few-Shot Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning to Diversify for Single Domain Generalization\n",
      "Paper_id: 2108.11726v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning to Generate Novel Domains for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Generalization by Mutual-Information Regularization with Pre-trained Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Single-Side Domain Generalization for Face Anti-Spoofing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SimDE: A Simple Domain Expansion Approach for Single-source Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DomainDrop: Suppressing Domain-Sensitive Channels for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Flatness-Aware Minimization for Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: Modality-Agnostic Debiasing for Single Domain Generalization\n",
      "Paper_id: 2303.07123v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Generalization for Object Recognition with Multi-task Autoencoders\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Crossing the Gap: Domain Generalization for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Episodic Training for Domain Generalization\n",
      "Paper_id: 1902.00113v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Causality Inspired Representation Learning for Domain Generalization\n",
      "Paper_id: 2203.14237v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 2\n",
      "\n",
      "\n",
      "🎯 Found paper: Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing\n",
      "Paper_id: 2203.05340v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PCL: Proxy-based Contrastive Learning for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning from Extrinsic and Intrinsic Supervisions for Domain Generalization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Style Neophile: Constantly Seeking Novel Styles for Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: A Broad Study of Pre-training for Domain Generalization and Adaptation\n",
      "Paper_id: 2203.11819v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Progressive Domain Expansion Network for Single Domain Generalization\n",
      "Paper_id: 2103.16050v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FSDR: Frequency Space Domain Randomization for Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: Compound Domain Generalization via Meta-Knowledge Encoding\n",
      "Paper_id: 2203.13006v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Simple Feature Augmentation for Domain Generalization\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross-Domain Ensemble Distillation for Domain Generalization\n",
      "Paper_id: 2211.14058v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ImageNet: A large-scale hierarchical image database\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Training With Noisy Student Improves ImageNet Classification\n",
      "\n",
      "\n",
      "🎯 Found paper: Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\n",
      "Paper_id: 1711.09577v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Do Better ImageNet Models Transfer Better?\n",
      "Paper_id: 1805.08974v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VQ3D: Learning a 3D-Aware Generative Model on ImageNet\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking ImageNet Pre-training\n",
      "Paper_id: 1811.08883v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fake it Till You Make it: Learning Transferable Representations from Synthetic ImageNet Clones\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet\n",
      "\n",
      "\n",
      "🎯 Found paper: Does Robustness on ImageNet Transfer to Downstream Tasks?\n",
      "Paper_id: 2204.03934v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: N-ImageNet: Towards Robust, Fine-Grained Object Recognition with Event Cameras\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: In Defense of Pre-Trained ImageNet Architectures for Real-Time Semantic Segmentation of Road-Driving Images\n",
      "\n",
      "\n",
      "🎯 Found paper: How Well Do Sparse Imagenet Models Transfer?\n",
      "Paper_id: 2111.13445v5\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-training on Indoor Segmentation?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings\n",
      "\n",
      "\n",
      "🎯 Found paper: Microsoft COCO: Common Objects in Context\n",
      "Paper_id: 1405.0312v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCONut: Modernizing COCO Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCO-Stuff: Thing and Stuff Classes in Context\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: COCO Attributes: Attributes for People, Animals, and Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Boosting Object Proposals: From Pascal to COCO\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MMBench: Is Your Multi-modal Model an All-around Player?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YOLO9000: Better, Faster, Stronger\n",
      "\n",
      "\n",
      "🎯 Found paper: Aggregated Residual Transformations for Deep Neural Networks\n",
      "Paper_id: 1611.05431v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Reconstructing PASCAL VOC\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SSD: Single Shot MultiBox Detector\n",
      "\n",
      "\n",
      "🎯 Found paper: Pyramid Scene Parsing Network\n",
      "Paper_id: 1612.01105v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n",
      "\n",
      "\n",
      "🎯 Found paper: Unsupervised Visual Representation Learning by Context Prediction\n",
      "Paper_id: 1505.05192v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Role of Context for Object Detection and Semantic Segmentation in the Wild\n",
      "\n",
      "\n",
      "🎯 Found paper: Training Region-based Object Detectors with Online Hard Example Mining\n",
      "Paper_id: 1604.03540v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Instance-aware Semantic Segmentation via Multi-task Network Cascades\n",
      "Paper_id: 1512.04412v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unbiased look at dataset bias\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Is object localization for free? - Weakly-supervised learning with convolutional neural networks\n",
      "\n",
      "\n",
      "🎯 Found paper: What's the Point: Semantic Segmentation with Point Supervision\n",
      "Paper_id: 1506.02106v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PSANet: Point-wise Spatial Attention Network for Scene Parsing\n",
      "\n",
      "\n",
      "🎯 Found paper: Weakly Supervised Deep Detection Networks\n",
      "Paper_id: 1511.02853v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning a Discriminative Feature Network for Semantic Segmentation\n",
      "Paper_id: 1804.09337v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: What is an object?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: How to Evaluate Foreground Maps\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Expectation-Maximization Attention Networks for Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-task Self-Supervised Visual Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Tell Me Where to Look: Guided Attention Inference Network\n",
      "Paper_id: 1802.10171v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Asymmetric Loss For Multi-Label Classification\n",
      "Paper_id: 2009.14119v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-view Supervision for Single-View Reconstruction via Differentiable Ray Consistency\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Seeing 3D Chairs: Exemplar Part-Based 2D-3D Alignment Using a Large Dataset of CAD Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ScaleDet: A Scalable Multi-Dataset Object Detector\n",
      "\n",
      "\n",
      "🎯 Found paper: Large-scale interactive object segmentation with human annotators\n",
      "Paper_id: 1903.10830v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Webly Supervised Concept Expansion for General Purpose Vision Models\n",
      "Paper_id: 2202.02317v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Large Loss Matters in Weakly Supervised Multi-Label Classification\n",
      "Paper_id: 2206.03740v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hyperbolic Contrastive Learning for Visual Representations beyond Objects\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BigDetection: A Large-scale Benchmark for Improved Object Detector Pre-training\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAFE: Sensitivity-Aware Features for Out-of-Distribution Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Gender Artifacts in Visual Datasets\n",
      "Paper_id: 2206.09191v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-world Instance Segmentation: Top-down Learning with Bottom-up Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-label Classification with Partial Annotations using Class-aware Selective Loss\n",
      "\n",
      "\n",
      "🎯 Found paper: Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels\n",
      "Paper_id: 2005.08455v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "🎯 Found paper: Normalization Matters in Weakly Supervised Object Localization\n",
      "Paper_id: 2107.13221v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Object Recognition from Machine-Labeled Web Images\n",
      "\n",
      "\n",
      "🎯 Found paper: Improving Text-guided Object Inpainting with Semantic Pre-inpainting\n",
      "Paper_id: 2409.08260v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask-Free OVIS: Open-Vocabulary Instance Segmentation without Manual Mask Annotations\n",
      "\n",
      "\n",
      "🎯 Found paper: Revisiting the Sibling Head in Object Detector\n",
      "Paper_id: 2003.07540v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Graphical Contrastive Losses for Scene Graph Parsing\n",
      "Paper_id: 1903.02728v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Set Semi-Supervised Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: YouTube-VOS: Sequence-to-Sequence Video Object Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Video Object Segmentation Using Space-Time Memory Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Language as Queries for Referring Video Object Segmentation\n",
      "Paper_id: 2201.00487v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Collaborative Video Object Segmentation by Foreground-Background Integration\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RVOS: End-To-End Recurrent Network for Video Object Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning What to Learn for Video Object Segmentation\n",
      "Paper_id: 2003.11540v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Video Object Segmentation Using Global and Instance Embedding Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised Video Object Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Accelerating Video Object Segmentation with Compressed Video\n",
      "Paper_id: 2107.12192v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VORNet: Spatio-Temporally Consistent Video Inpainting for Object Removal\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dual Embedding Learning for Video Instance Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Enhanced Memory Network for Video Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Motion-Guided Spatial Time Attention for Video Object Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
      "Paper_id: 1705.07750v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AVA: A large-scale database for aesthetic visual analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Supplementary Material: AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: SlowFast Networks for Video Recognition\n",
      "Paper_id: 1812.03982v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Masked Feature Prediction for Self-Supervised Visual Pre-Training\n",
      "\n",
      "\n",
      "🎯 Found paper: Video Action Transformer Network\n",
      "Paper_id: 1812.02707v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Long-Term Feature Banks for Detailed Video Understanding\n",
      "Paper_id: 1812.05038v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 16\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Kernel Surface Reconstruction\n",
      "Paper_id: 2305.19590v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Recurrent Slice Networks for 3D Segmentation of Point Clouds\n",
      "Paper_id: 1802.04402v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-Vocabulary 3D Semantic Segmentation with Foundation Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OneFormer3D: One Transformer for Unified Point Cloud Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Omni-Crack30k: A Benchmark for Crack Segmentation and the Reasonable Effectiveness of Transfer Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ODIN: A Single Model for 2D and 3D Segmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Monocular Occupancy Prediction for Scalable Indoor Scenes\n",
      "Paper_id: 2407.11730v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A-CNN: Annularly Convolutional Neural Networks on Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Multiview Point Cloud Registration with Reliable Pose Graph Initialization and History Reweighting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: One Thing One Click: A Self-Training Approach for Weakly Supervised 3D Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HybridCR: Weakly-Supervised 3D Point Cloud Semantic Segmentation via Hybrid Contrastive Regularization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Mask3D: Pretraining 2D Vision Transformers by Learning Masked 3D Priors\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Non-Lambertian Object Intrinsics across ShapeNet Categories\n",
      "Paper_id: 1612.08510v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: pixelNeRF: Neural Radiance Fields from One or Few Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Papier-Mache Approach to Learning 3D Surface Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Geodesic Convolutional Neural Networks on Riemannian Manifolds\n",
      "\n",
      "\n",
      "🎯 Found paper: Zero-Shot Text-Guided Object Generation with Dream Fields\n",
      "Paper_id: 2112.01455v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 14\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ULIP-2: Towards Scalable Multimodal Pre-Training for 3D Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Neural Field Generation using Triplane Diffusion\n",
      "Paper_id: 2211.16677v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SfmCAD: Unsupervised CAD Reconstruction by Learning Sketch-based Feature Modeling Operations\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Shape Segmentation with Projective Convolutional Networks\n",
      "Paper_id: 1612.02808v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hyperbolic Chamfer Distance for Point Cloud Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Multiresolution Tree Networks for 3D Point Cloud Processing\n",
      "Paper_id: 1807.03520v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Symmetry-Aware Geometry Correspondences for 6D Object Pose Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GIFS: Neural Implicit Function for General Shape Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Are we ready for autonomous driving? The KITTI vision benchmark suite\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Learning of Depth and Ego-Motion from Video\n",
      "\n",
      "\n",
      "🎯 Found paper: Tracking Objects as Points\n",
      "Paper_id: 2004.01177v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 14\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-End Learning of Geometry and Context for Deep Stereo Regression\n",
      "Paper_id: 1703.04309v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3DSSD: Point-Based 3D Single Stage Object Detector\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PIXOR: Real-time 3D Object Detection from Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Monocular 3D Object Detection for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointPainting: Sequential Fusion for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Deep Continuous Fusion for Multi-Sensor 3D Object Detection\n",
      "Paper_id: 2012.10992v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: STD: Sparse-to-Dense 3D Object Detector for Point Cloud\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-Task Multi-Sensor Fusion for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Computing the Stereo Matching Cost with a Convolutional Neural Network\n",
      "Paper_id: 1409.4326v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: 3D Registration with Maximal Cliques\n",
      "Paper_id: 2305.10854v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving\n",
      "\n",
      "\n",
      "🎯 Found paper: Stereo R-CNN based 3D Object Detection for Autonomous Driving\n",
      "Paper_id: 1902.09738v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Scalability in Perception for Autonomous Driving: Waymo Open Dataset\n",
      "Paper_id: 1912.04838v7\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Waymo Open Dataset: Panoramic Video Panoptic Segmentation\n",
      "Paper_id: 2206.07704v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Multi-Space Alignments Towards Universal LiDAR Segmentation\n",
      "Paper_id: 2405.01538v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RangeDet: In Defense of Range View for LiDAR-based 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RetinaTrack: Online Single Stage Joint Detection and Tracking\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VAD: Vectorized Scene Representation for Efficient Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DriveLM: Driving with Graph Visual Question Answering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DDP: Diffusion Model for Dense Visual Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spherical Transformer for LiDAR-Based 3D Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FB-BEV: BEV Representation from Forward-Backward View Transformations\n",
      "\n",
      "\n",
      "🎯 Found paper: Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?\n",
      "Paper_id: 2312.03031v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FUTR3D: A Unified Sensor Fusion Framework for 3D Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Panacea: Panoramic and Controllable Video Generation for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FIERY: Future Instance Prediction in Bird’s-Eye View from Surround Monocular Cameras\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction\n",
      "\n",
      "\n",
      "🎯 Found paper: Focal Sparse Convolutional Networks for 3D Object Detection\n",
      "Paper_id: 2204.12463v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LiDAR4D: Dynamic Neural Fields for Novel Space-Time View LiDAR Synthesis\n",
      "\n",
      "\n",
      "🎯 Found paper: Producing and Leveraging Online Map Uncertainty in Trajectory Prediction\n",
      "Paper_id: 2403.16439v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross Modal Transformer: Towards Fast and Robust 3D Object Detection\n",
      "Paper_id: 2301.01283v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UnO: Unsupervised Occupancy Fields for Perception and Forecasting\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GraphBEV: Towards Robust BEV Feature Alignment for Multi-Modal 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RangeLDM: Fast Realistic LiDAR Point Cloud Generation\n",
      "\n",
      "\n",
      "🎯 Found paper: Occupancy as Set of Points\n",
      "Paper_id: 2407.04049v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FocalFormer3D : Focusing on Hard Instance for 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generalized Predictive Model for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GenAD: Generative End-to-End Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Planning-oriented Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes\n",
      "\n",
      "\n",
      "🎯 Found paper: Multi-Modal Fusion Transformer for End-to-End Autonomous Driving\n",
      "Paper_id: 2104.09224v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Driving Into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: NeuRAD: Neural Rendering for Autonomous Driving\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Map Prior for Autonomous Driving\n",
      "Paper_id: 2304.08481v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rope3D: The Roadside Perception Dataset for Autonomous Driving and Monocular 3D Object Detection Task\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ADAS: A Direct Adaptation Strategy for Multi-Target Domain Adaptive Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CLRNet: Cross Layer Refinement Network for Lane Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark\n",
      "\n",
      "\n",
      "🎯 Found paper: Ultra Fast Structure-aware Deep Lane Detection\n",
      "Paper_id: 2004.11757v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Anchor3DLane: Learning to Regress 3D Anchors for Monocular 3D Lane Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual Camera via Key-Points\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CondLaneNet: a Top-to-down Lane Detection Framework Based on Conditional Convolution\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LATR: 3D Lane Detection from Monocular Images with Transformer\n",
      "\n",
      "\n",
      "🎯 Found paper: Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection\n",
      "Paper_id: 2010.12035v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Generating Dynamic Kernels via Transformers for Lane Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Recursive Video Lane Detection\n",
      "Paper_id: 2308.11106v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Rethinking Efficient Lane Detection via Curve Modeling\n",
      "Paper_id: 2203.02431v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 13\n",
      "\n",
      "\n",
      "🎯 Found paper: A Keypoint-based Global Association Network for Lane Detection\n",
      "Paper_id: 2204.07335v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ONCE-3DLanes: Building Monocular 3D Lane Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Reconstruct from Top View: A 3D Lane Detection Approach based on Geometry Structure Prior\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RCLane: Relay Chain Prediction for Lane Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VIL-100: A New Dataset and A Baseline Model for Video Instance Lane Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Driving-Oriented Metric for Lane Detection Models\n",
      "Paper_id: 2203.16851v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "🎯 Found paper: Multi-level Domain Adaptation for Lane Detection\n",
      "Paper_id: 2206.10692v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FastDraw: Addressing the Long Tail of Lane Detection by Adapting a Sequential Prediction Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Active Learning for Lane Detection: A Knowledge Distillation Approach\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-LaneNet: End-to-End 3D Multiple Lane Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-end Lane Detection through Differentiable Least-Squares Fitting\n",
      "Paper_id: 1902.00293v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEV-Guided Multi-Modality Fusion for Driving Perception\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEV@DC: Bird's-Eye View Assisted Training for Depth Completion\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SA-BEV: Generating Semantic-Aware Bird’s-Eye-View Feature for Multi-view 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PointBeV: A Sparse Approach to BeV Predictions\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RCBEVDet: Radar-Camera Fusion in Bird's Eye View for 3D Object Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird’s Eye View Maps\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniTR: A Unified and Efficient Multi-Modal Transformer for Bird’s-Eye-View Representation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEVPlace: Learning LiDAR-based Place Recognition using Bird’s Eye View Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lifting Multi-View Detection and Tracking to the Bird’s Eye View\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BAEFormer: Bi-Directional and Early Interaction Transformers for Bird's Eye View Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Bird’s-Eye-View Scene Graph for Vision-Language Navigation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: BEV-DG: Cross-Modal Learning under Bird’s-Eye View for Domain Generalization of 3D Semantic Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Viewpoint Robustness in Bird’s Eye View Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniFusion: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird’s-Eye-View\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Domain Adaptation for Vehicle Detection from Bird's Eye View LiDAR Point Cloud Data\n",
      "\n",
      "\n",
      "🎯 Found paper: A Geometric Approach to Obtain a Bird's Eye View from an Image\n",
      "Paper_id: 1905.02231v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Implicit Neural Representation in Medical Imaging: A Comparative Survey\n",
      "Paper_id: 2307.16142v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Randomized Ensemble Approach to Industrial CT Segmentation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Is Deep Learning Safe for Robot Vision? Adversarial Examples Against the iCub Humanoid\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Language-driven Grasp Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: An Economic Framework for 6-DoF Grasp Detection\n",
      "Paper_id: 2407.08366v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "🎯 Found paper: Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance\n",
      "Paper_id: 2407.13842v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Graspness Discovery in Clutters for Fast and Accurate Grasp Detection\n",
      "Paper_id: 2406.11142v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MaskGAN: Towards Diverse and Interactive Facial Image Manipulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ManTra-Net: Manipulation Tracing Network for Detection and Localization of Image Forgeries With Anomalous Features\n",
      "\n",
      "\n",
      "🎯 Found paper: On the Detection of Digital Face Manipulation\n",
      "Paper_id: 1910.01717v5\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Generative Visual Manipulation on the Natural Image Manifold\n",
      "Paper_id: 1609.03552v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: 6-DOF GraspNet: Variational Grasp Generation for Object Manipulation\n",
      "Paper_id: 1905.10520v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Rich Features for Image Manipulation Detection\n",
      "Paper_id: 1805.04953v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ManipulaTHOR: A Framework for Visual Object Manipulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ManiGAN: Text-Guided Image Manipulation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition\n",
      "Paper_id: 1711.11248v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Convolutional Two-Stream Network Fusion for Video Action Recognition\n",
      "Paper_id: 1604.06573v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Action Recognition with Improved Trajectories\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Revisiting Skeleton-based Action Recognition\n",
      "Paper_id: 2104.13586v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InfoGCN: Representation Learning for Human Skeleton-based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Skeleton-Based Action Recognition With Shift Graph Convolutional Network\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Learning Discriminative Representations for Skeleton Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Temporal-Relational CrossTransformers for Few-Shot Action Recognition\n",
      "Paper_id: 2101.06184v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TEA: Temporal Excitation and Aggregation for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hierarchical recurrent neural network for skeleton based action recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TDN: Temporal Difference Networks for Efficient Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Temporal Pyramid Network for Action Recognition\n",
      "Paper_id: 2004.03548v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Skeleton-Based Action Recognition With Directed Graph Neural Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition\n",
      "Paper_id: 1607.07043v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Few-Shot Action Recognition with Permutation-Invariant Attention\n",
      "\n",
      "\n",
      "🎯 Found paper: Hybrid Relation Guided Set Matching for Few-shot Action Recognition\n",
      "Paper_id: 2204.13423v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Action recognition by dense trajectories\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SVFormer: Semi-supervised Video Transformer for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recurring the Transformer for Video Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ACTION-Net: Multipath Excitation for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SPAct: Self-supervised Privacy Preservation for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: STM: SpatioTemporal and Motion Encoding for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AR-Net: Adaptive Frame Resolution for Efficient Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Evidential Deep Learning for Open Set Action Recognition\n",
      "Paper_id: 2107.10161v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group\n",
      "\n",
      "\n",
      "🎯 Found paper: Elaborative Rehearsal for Zero-shot Action Recognition\n",
      "Paper_id: 2108.02833v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: A New Representation of Skeleton Sequences for 3D Action Recognition\n",
      "Paper_id: 1703.03492v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spatio-temporal Relation Modeling for Few-shot Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MGSampler: An Explainable Sampling Strategy for Video Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Context Aware Graph Convolution for Skeleton-Based Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semi-Supervised Action Recognition with Temporal Contrastive Learning\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning from Temporal Gradient for Semi-supervised Action Recognition\n",
      "Paper_id: 2111.13241v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 11\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3DV: 3D Dynamic Voxel for Action Recognition in Depth Video\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Graph-based High-order Relation Modeling for Long-term Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: E2(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MARS: Motion-Augmented RGB Stream for Action Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: One-shot action recognition in challenging therapy scenarios\n",
      "Paper_id: 2102.08997v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Low Power, Fully Event-Based Gesture Recognition System\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: An Efficient PointLSTM for Point Clouds Based Gesture Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Decoupled Representation Learning for Skeleton-Based Gesture Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MVBench: A Comprehensive Multi-modal Video Understanding Benchmark\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TSM: Temporal Shift Module for Efficient Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MovieChat: From Dense Token to Sparse Memory for Long Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoMamba: State Space Model for Efficient Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VideoAgent: Long-form Video Understanding with Large Language Model as Agent\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: InternVideo2: Scaling Foundation Models for Multimodal Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LongVLM: Efficient Long Video Understanding via Large Language Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Abductive Ego-View Accident Video Understanding for Safe Driving Perception\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OmniViD: A Generative Framework for Universal Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Prompting Visual-Language Models for Efficient Video Understanding\n",
      "Paper_id: 2112.04478v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Selective Structured State-Spaces for Long-Form Video Understanding\n",
      "Paper_id: 2303.14526v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Procedure-Aware Pretraining for Instructional Video Understanding\n",
      "Paper_id: 2303.18230v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vamos: Versatile Action Models for Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Long-Form Video Understanding\n",
      "Paper_id: 2106.11310v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 10\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ECO: Efficient Convolutional Network for Online Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: System-Status-Aware Adaptive Network for Online Streaming Video Understanding\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Visual Semantic Role Labeling for Video Understanding\n",
      "Paper_id: 2104.00990v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MotionSqueeze: Neural Motion Feature Learning for Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Unified Graph Structured Models for Video Understanding\n",
      "Paper_id: 2103.15662v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Temporal Query Networks for Fine-grained Video Understanding\n",
      "Paper_id: 2104.09496v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Temporal Aggregate Representations for Long-Range Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Large Scale Holistic Video Understanding\n",
      "Paper_id: 1904.11451v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Learning of Motion Representation for Video Understanding\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards VQA Models That Can Read\n",
      "Paper_id: 1904.08920v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge\n",
      "\n",
      "\n",
      "🎯 Found paper: Counterfactual VQA: A Cause-Effect Look at Language Bias\n",
      "Paper_id: 2006.04315v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GRAM: Global Reasoning for Multi-Page VQA\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WSI-VQA: Interpreting Whole Slide Images by Generative Visual Question Answering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VDPVE: VQA Dataset for Perceptual Video Enhancement\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!\n",
      "\n",
      "\n",
      "🎯 Found paper: VQA Therapy: Exploring Answer Differences by Visually Grounding Answers\n",
      "Paper_id: 2308.11662v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LaTr: Layout-Aware Transformer for Scene-Text VQA\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Roses are Red, Violets are Blue… But Should VQA expect Them To?\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VQA-LOL: Visual Question Answering under the Lens of Logic\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sim VQA: Exploring Simulated Environments for Visual Question Answering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval\n",
      "\n",
      "\n",
      "🎯 Found paper: Streaming Dense Video Captioning\n",
      "Paper_id: 2404.01297v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "🎯 Found paper: Retrieval-Augmented Egocentric Video Captioning\n",
      "Paper_id: 2401.00789v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Enhancing Traffic Safety with Parallel Dense Video Captioning for End-to-End Event Analysis\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-End Dense Video Captioning with Parallel Decoding\n",
      "Paper_id: 2108.07781v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Video ReCap: Recursive Captioning of Hour-Long Videos\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SoccerNet-Caption: Dense Video Captioning for Soccer Broadcasts Commentaries\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation\n",
      "\n",
      "\n",
      "🎯 Found paper: Text with Knowledge Graph Augmented Transformer for Video Captioning\n",
      "Paper_id: 2303.12423v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Accurate and Fast Compressed Video Captioning\n",
      "Paper_id: 2309.12867v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-End Dense Video Captioning with Masked Transformer\n",
      "Paper_id: 1804.00819v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Object Relational Graph With Teacher-Recommended Learning for Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Group Video Captioning with Efficient Relational Approximation\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SEM-POS: Grammatically and Semantically Correct Video Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: End-to-end Generative Pretraining for Multimodal Video Captioning\n",
      "Paper_id: 2201.08264v2\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "🎯 Found paper: Spatio-Temporal Graph for Video Captioning with Knowledge Distillation\n",
      "Paper_id: 2003.13942v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Open-book Video Captioning with Retrieve-Copy-Generate Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Hierarchical Modular Network for Video Captioning\n",
      "Paper_id: 2111.12476v3\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Syntax-Aware Action Targeting for Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-modal Dense Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Hierarchical Video-Moment Retrieval and Step-Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Motion Guided Region Message Passing for Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sketch, Ground, and Refine: Top-Down Dense Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\n",
      "\n",
      "\n",
      "🎯 Found paper: Reconstruction Network for Video Captioning\n",
      "Paper_id: 1803.11438v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Memory-Attended Recurrent Network for Video Captioning\n",
      "Paper_id: 1905.03966v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SODA: Story Oriented Dense Video Captioning Evaluation Framework\n",
      "\n",
      "\n",
      "🎯 Found paper: Streamlined Dense Video Captioning\n",
      "Paper_id: 1904.03870v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Self-Critical Sequence Training for Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Meshed-Memory Transformer for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: X-Linear Attention Networks for Image Captioning\n",
      "Paper_id: 2003.14080v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: TextCaps: a Dataset for Image Captioning with Reading Comprehension\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transferable Decoding with Visual Entities for Zero-Shot Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Exploring Visual Relationship for Image Captioning\n",
      "Paper_id: 1809.07041v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Auto-Encoding Scene Graphs for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Guiding Image Captioning Models Toward More Specific Captions\n",
      "Paper_id: 2307.16686v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 9\n",
      "\n",
      "\n",
      "🎯 Found paper: Cross-Domain Image Captioning with Discriminative Finetuning\n",
      "Paper_id: 2304.01662v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 12\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scaling Up Vision-Language Pretraining for Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Smallcap: Lightweight Image Captioning Prompted with Retrieval Augmentation\n",
      "\n",
      "\n",
      "🎯 Found paper: Comprehending and Ordering Semantics for Image Captioning\n",
      "Paper_id: 2206.06930v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Semantic-Conditional Diffusion Networks for Image Captioning*\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Show, Deconfound and Tell: Image Captioning with Causal Inference\n",
      "\n",
      "\n",
      "🎯 Found paper: Quantifying Societal Bias Amplification in Image Captioning\n",
      "Paper_id: 2203.15395v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeeCap: Dynamic Early Exiting for Efficient Image Captioning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DIFNet: Boosting Visual Information Flow for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Understanding and Evaluating Racial Biases in Image Captioning\n",
      "Paper_id: 2106.08503v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Boosting Image Captioning with Attributes\n",
      "Paper_id: 1611.01646v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Normalized and Geometry-Aware Self-Attention Network for Image Captioning\n",
      "\n",
      "\n",
      "🎯 Found paper: Injecting Semantic Concepts into End-to-End Image Captioning\n",
      "Paper_id: 2112.05230v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Robust Real-Time Face Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Neural network-based face detection\n",
      "\n",
      "\n",
      "🎯 Found paper: WIDER FACE: A Face Detection Benchmark\n",
      "Paper_id: 1511.06523v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: Representative Forgery Mining for Fake Face Detection\n",
      "Paper_id: 2104.06609v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A convolutional neural network cascade for face detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Global Texture Enhancement for Fake Face Detection in the Wild\n",
      "Paper_id: 2002.00133v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 8\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: HLA-Face: Joint High-Low Adaptation for Low Light Face Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: End-to-End Reconstruction-Classification Learning for Face Forgery Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain\n",
      "\n",
      "\n",
      "🎯 Found paper: Generalizing Face Forgery Detection with High-frequency Features\n",
      "Paper_id: 2103.12376v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Implicit Identity Driven Deepfake Face Swapping Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark A\n",
      "\n",
      "\n",
      "🎯 Found paper: AltFreezing for More General Video Face Forgery Detection\n",
      "Paper_id: 2307.08317v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "🎯 Found paper: Two-Stream Neural Networks for Tampered Face Detection\n",
      "Paper_id: 1803.11276v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Joint Face Detection and Facial Motion Retargeting for Multiple Faces\n",
      "Paper_id: 1902.10744v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lips Don't Lie: A Generalisable and Robust Approach to Face Forgery Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Exploring Temporal Coherence for More General Video Face Forgery Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Design and Interpretation of Universal Adversarial Patches in Face Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: From Facial Parts Responses to Face Detection: A Deep Learning Approach\n",
      "Paper_id: 1509.06451v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Training support vector machines: an application to face detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Face X-ray for More General Face Forgery Detection\n",
      "Paper_id: 1912.13458v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "🎯 Found paper: Supervised Transformer Network for Efficient Face Detection\n",
      "Paper_id: 1607.05477v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Joint Training of Cascaded CNN for Face Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Face Detection with End-to-End Integration of a ConvNet and a 3D Model\n",
      "Paper_id: 1606.00850v3\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Scale-Aware Face Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: FaceNet: A unified embedding for face recognition and clustering\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: A Discriminative Feature Learning Approach for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CosFace: Large Margin Cosine Loss for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: SphereFace: Deep Hypersphere Embedding for Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaFace: Quality Adaptive Margin for Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: The FERET evaluation methodology for face-recognition algorithms\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MagFace: A Universal Representation for Face Recognition and Quality Assessment\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: ElasticFace: Elastic Margin Loss for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Overview of the face recognition grand challenge\n",
      "\n",
      "\n",
      "🎯 Found paper: Data Uncertainty Learning in Face Recognition\n",
      "Paper_id: 2003.11339v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Improving Transferability of Adversarial Patches on Face Recognition with Generative Models\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: When Age-Invariant Face Recognition Meets Face Age Synthesis: A Multi-Task Learning Framework\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition\n",
      "\n",
      "\n",
      "🎯 Found paper: Learning Meta Face Recognition in Unseen Domains\n",
      "Paper_id: 2003.07733v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Spherical Confidence Learning for Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face recognition using eigenfaces\n",
      "\n",
      "\n",
      "🎯 Found paper: Face Recognition: Too Bias, or Not Too Bias?\n",
      "Paper_id: 2002.06483v4\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Feature Transfer Learning for Face Recognition With Under-Represented Data\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Universal Representation Learning for Deep Face Recognition\n",
      "Paper_id: 2002.11841v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AdaptiveFace: Adaptive Margin and Sampling for Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Lightweight Face Recognition Challenge\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UniformFace: Learning Deep Equidistributed Representation for Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Towards Pose Invariant Face Recognition in the Wild\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: RegularFace: Deep Face Recognition via Exclusive Regularization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Analyzing and Reducing the Damage of Dataset Bias to Face Recognition With Synthetic Data\n",
      "\n",
      "\n",
      "🎯 Found paper: The Devil of Face Recognition is in the Noise\n",
      "Paper_id: 1807.11649v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "🎯 Found paper: Decorrelated Adversarial Learning for Age-Invariant Face Recognition\n",
      "Paper_id: 1904.04972v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Characterizing the Variability in Face Recognition Accuracy Relative to Race\n",
      "\n",
      "\n",
      "🎯 Found paper: Ring loss: Convex Feature Normalization for Face Recognition\n",
      "Paper_id: 1803.00130v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 4\n",
      "\n",
      "\n",
      "🎯 Found paper: Pose-Robust Face Recognition via Deep Residual Equivariant Mapping\n",
      "Paper_id: 1803.00839v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 5\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Face recognition in unconstrained videos with matched background similarity\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Recognizing Multi-Modal Face Spoofing With Face Recognition Networks\n",
      "\n",
      "\n",
      "🎯 Found paper: Neural Aggregation Network for Video Face Recognition\n",
      "Paper_id: 1603.05474v4\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 0\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Range Loss for Deep Face Recognition with Long-Tailed Training Data\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Interpretable Face Recognition\n",
      "Paper_id: 1805.00611v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 6\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Marginal Loss for Deep Face Recognition\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: How Far are We from Solving the 2D & 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)\n",
      "\n",
      "\n",
      "🎯 Found paper: Exemplar Guided Face Image Super-Resolution without Facial Landmarks\n",
      "Paper_id: 1906.07078v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "🎯 Found paper: I Know How You Feel: Emotion Recognition with Facial Landmarks\n",
      "Paper_id: 1805.00326v2\n",
      "📦 Source file downloaded\n",
      "Number of sections: 3\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Annotated Facial Landmarks in the Wild: A large-scale, real-world database for facial landmark localization\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Discrimination Between Genuine Versus Fake Emotion Using Long-Short Term Memory with Parametric Bias and Facial Landmarks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Shape-based automatic detection of a large number of 3D facial landmarks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Facial Landmarks Localization Estimation by Cascaded Boosted Regression\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Detector of Facial Landmarks Learned by the Structured Output SVM\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: 3D-aware Facial Landmark Detection via Multi-view Consistent Training on Synthetic Data\n",
      "\n",
      "\n",
      "🎯 Found paper: Towards Accurate Facial Landmark Detection via Cascaded Transformers\n",
      "Paper_id: 2208.10808v1\n",
      "📦 Source file downloaded\n",
      "Number of sections: 1\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multi-attentional Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Rethinking the Up-Sampling Operations in CNN-Based Generative Network for Generalizable Deepfake Detection\n",
      "\n",
      "\n",
      "🎯 Found paper: Preserving Fairness Generalization in Deepfake Detection\n",
      "Paper_id: 2402.17229v1\n",
      "📦 Source file downloaded\n",
      "No abstract found\n",
      "Number of sections: 7\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Faster Than Lies: Real-time Deepfake Detection using Binary Neural Networks\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: UCF: Uncovering Common Features for Generalizable Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Multimodaltrace: Deepfake Detection using Audiovisual Representation Learning\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: PUDD: Towards Robust Multi-modal Prototype-based Deepfake Detection\n",
      "\n",
      "\n",
      "⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\n",
      "query: Dynamic Graph Learning with Content-guided Spatial-Frequency Relation Reasoning for Deepfake Detection\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "\n",
    "for paper in papers:\n",
    "    query = paper[\"title\"].strip()\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "\n",
    "    # 정확히 제목 기반으로 arXiv 검색\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=ti:{encoded_query}&max_results=5\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    found_match = False\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        arxiv_title = entry.title.strip()\n",
    "\n",
    "        if query.lower() == arxiv_title.lower():\n",
    "            found_match = True\n",
    "            paper_id = entry.id.split(\"/\")[-1]\n",
    "            print(f\"🎯 Found paper: {arxiv_title}\")\n",
    "            print(f\"Paper_id: {paper_id}\")\n",
    "\n",
    "            # 논문 소스 다운로드\n",
    "            url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "            max_try = 0\n",
    "            while max_try < 5:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(f\"{paper_id}.tar.gz\", \"wb\") as f:\n",
    "                        f.write(response.content)\n",
    "                    print(\"📦 Source file downloaded\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Retrying download...\")\n",
    "                    max_try += 1\n",
    "            else:\n",
    "                print(\"❌ Failed to download after 5 tries\")\n",
    "                continue\n",
    "\n",
    "            # 압축 해제\n",
    "            tar_path = f\"{paper_id}.tar.gz\"\n",
    "            try:\n",
    "                with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                    tar.extractall(\"SourceFiles\")\n",
    "                os.remove(tar_path)\n",
    "            except tarfile.ReadError:\n",
    "                print(f\"❌ {tar_path} is not a valid gzip file. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # .tex 파일 추출\n",
    "            tex_file = None\n",
    "            for fname in os.listdir(\"SourceFiles\"):\n",
    "                if fname.endswith(\".tex\"):\n",
    "                    tex_file = fname\n",
    "                    break\n",
    "\n",
    "            if not tex_file:\n",
    "                print(\"❌ No .tex file found.\")\n",
    "                shutil.rmtree(\"SourceFiles\")\n",
    "                continue\n",
    "\n",
    "            with open(os.path.join(\"SourceFiles\", tex_file), \"r\", encoding=\"utf-8\") as f:\n",
    "                latex_text = f.read()\n",
    "\n",
    "            splitBy_section = split_section(latex_text)\n",
    "\n",
    "            paper_entry = {\n",
    "                \"title\": arxiv_title,\n",
    "                \"paper_id\": paper_id,\n",
    "                \"sections\": []\n",
    "            }\n",
    "\n",
    "            for section_name, content in splitBy_section:\n",
    "                cleaned_content = re.sub(r\"\\\\begin{([a-zA-Z*]+)}(?:\\[[^\\]]*\\])?.*?\\\\end{\\1}\", \"\", content, flags=re.DOTALL)\n",
    "                lines = cleaned_content.strip().splitlines()\n",
    "                cleaned_content = \"\\n\".join([line for line in lines if line.strip() and not line.strip().startswith(\"%\")])\n",
    "                paper_entry[\"sections\"].append({section_name: cleaned_content})\n",
    "\n",
    "            json_data.append(paper_entry)\n",
    "            shutil.rmtree(\"SourceFiles\")\n",
    "            break  # 일치하는 제목 찾으면 종료\n",
    "\n",
    "    if not found_match:\n",
    "        print(\"⚠ 제목과 정확히 일치하는 논문을 찾을 수 없습니다.\")\n",
    "        print(f\"query: {query}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35d37d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Residual Attention Network for Image Classification',\n",
       "  'paper_id': '1704.06904v1',\n",
       "  'sections': [{'abstract': '%Mixed nature of human attention has been proposed in the literature of biology and been applied to sequential learning task using RNN and LSTM.\\nIn this work, we propose ``Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion.\\n%\\nOur Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers.\\n\\nExtensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.45\\\\% error) and ImageNet (4.8\\\\% single model and single crop, top-5 error). Note that, our method achieves \\\\textbf{0.6\\\\%} top-1 accuracy improvement with \\\\textbf{46\\\\%} trunk depth and \\\\textbf{69\\\\%} forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.'},\n",
       "   {'Introduction': '\\\\begin{figure*}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{.9\\\\linewidth}{0pt}}\\n\\\\includegraphics[width=1\\\\linewidth]{motivation.pdf}\\n\\\\end{center}\\n   \\\\caption{\\\\textbf{Left:} an example shows the interaction between features and attention masks. \\\\textbf{Right:} example images illustrating that different features have different corresponding attention masks in our network. The sky mask diminishes low-level background blue color features. The balloon instance mask highlights high-level balloon bottom part features.}\\n\\\\label{fig:motivation}\\n\\\\end{figure*}\\n\\nNot only a friendly face but also red color will draw our attention. The mixed nature of attention has been studied extensively in the previous literatures~\\\\cite{walther2002attentional, itti2001computational,mnih2014recurrent,zhao2016diversified}. Attention not only serves to select a focused location but also enhances different representations of objects at that location. Previous works formulate attention drift as a sequential process to capture different attended aspects. However, as far as we know, no attention mechanism has been applied to feedforward network structure to achieve state-of-art results in image classification task. Recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}.\\n\\nInspired by the attention mechanism and recent advances in the deep neural network, we propose Residual Attention Network, a convolutional network that adopts mixed attention mechanism in ``very deep\" structure. The Residual Attention Network is composed of multiple Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper.\\n\\nApart from more discriminative feature representation brought by the attention mechanism, our model also exhibits following appealing properties:\\n\\n\\\\noindent\\n(1) Increasing Attention Modules lead to consistent performance improvement, as different types of attention are captured extensively. Fig.\\\\ref{fig:motivation} shows an example of different types of attentions for a hot air balloon image. The sky attention mask diminishes background responses while the balloon instance mask highlighting the bottom part of the balloon.\\n\\n\\\\noindent\\n(2) It is able to incorporate with state-of-the-art deep network structures in an end-to-end training fashion. Specifically, the depth of our network can be easily extended to hundreds of layers. Our Residual Attention Network outperforms state-of-the-art residual networks on CIFAR-10, CIFAR-100 and challenging ImageNet~\\\\cite{deng2009imagenet} image classification dataset with significant reduction of computation (\\\\textbf{69\\\\%} forward FLOPs).\\n\\nAll of the aforementioned properties, which are challenging to achieve with previous approaches, are made possible with following contributions:\\n\\n\\\\noindent\\n(1) \\\\textit{Stacked network structure}: Our Residual Attention Network is constructed by stacking multiple Attention Modules. The stacked structure is the basic application of mixed attention mechanism. Thus, different types of attention are able to be captured in different Attention Modules.\\n%\\n\\n\\\\noindent\\n(2) \\\\textit{Attention Residual Learning}: Stacking Attention Modules directly would lead to the obvious performance drop. Therefore, we propose attention residual learning mechanism to optimize very deep Residual Attention Network with hundreds of layers. %Details\\n\\n\\\\noindent\\n(3) \\\\textit{Bottom-up top-down feedforward attention}: Bottom-up top-down feedforward structure has been successfully applied to human pose estimation~\\\\cite{newell2016stacked} and image segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet}. We use such structure as part of Attention Module to add soft weights on features. This structure can mimic bottom-up fast feedforward process and top-down attention feedback in a single feedforward process which allows us to develop an end-to-end trainable network with top-down attention. The bottom-up top-down structure in our work differs from stacked hourglass network~\\\\cite{newell2016stacked} in its intention of guiding feature learning.'},\n",
       "   {'Related Work': 'Evidence from human perception process~\\\\cite{mnih2014recurrent} shows the importance of attention mechanism, which uses top information to guide bottom-up feedforward process. Recently, tentative efforts have been made towards applying attention into deep neural network. Deep Boltzmann Machine (DBM)~\\\\cite{larochelle2010learning} contains top-down attention by its reconstruction process in the training stage. Attention mechanism has also been widely applied to recurrent neural networks (RNN) and long short term memory (LSTM) ~\\\\cite{hochreiter1997long} to tackle sequential decision tasks~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal}. Top information is gathered sequentially and decides where to attend for the next feature learning steps.\\n\\nResidual learning~\\\\cite{resnet2016} is proposed to learn residual of identity mapping. This technique greatly increases the depth of feedforward neuron network. Similar to our work, ~\\\\cite{noh2015learning, srivastava2015training, larochelle2010learning, kim2016multimodal} use residual learning with attention mechanism to benefit from residual learning. Two information sources (query and query context) are captured using attention mechanism to assist each other in their work. While in our work, a single information source (image) is split into two different ones and combined repeatedly. And residual learning is applied to alleviate the problem brought by repeated splitting and combining.\\n\\nIn image classification, top-down attention mechanism has been applied using different methods: sequential process, region proposal and control gates. Sequential process ~\\\\cite{mnih2014recurrent,hendricks2015deep,xu2015show,gregor2015draw} models image classification as a sequential decision. Thus attention can be applied similarly with above. This formulation allows end-to-end optimization using RNN and LSTM and can capture different kinds of attention in a goal-driven way.\\n\\nRegion proposal~\\\\cite{shrivastava2016contextual,dai2015convolutional,hariharan2014simultaneous,yang2015faceness} has been successfully adopted in image detection task. In image classification, an additional region proposal stage is added before feedforward classification. The proposed regions contain top information and are used for feature learning in the second stage. Unlike image detection whose region proposals rely on large amount of supervision, e.g. the ground truth bounding boxes or detailed segmentation masks~\\\\cite{erhan2014scalable}, unsupervised learning~\\\\cite{xiao2015application} is usually used to generate region proposals for image classification.\\n\\nControl gates have been extensively used in LSTM.  In image classification with attention, control gates for neurones are updated with top information and have influence on the feedforward process during training~\\\\cite{cao2015look,stollenga2014deep}. However, a new process, reinforcement learning~\\\\cite{stollenga2014deep} or optimization~\\\\cite{cao2015look} is involved during the training step. Highway Network~\\\\cite{srivastava2015training} extends control gate to solve gradient degradation problem for deep convolutional neural network.\\n\\nHowever, recent advances of image classification focus on training feedforward convolutional neural networks using ``very deep\" structure~\\\\cite{simonyan2014very,szegedy2015going,resnet2016}. The feedforward convolutional network mimics the bottom-up paths of human cortex. Various approaches have been proposed to further improve the discriminative ability of deep convolutional neural network. VGG~\\\\cite{simonyan2014very}, Inception~\\\\cite{szegedy2015going} and residual learning~\\\\cite{resnet2016} are proposed to train very deep neural networks. Stochastic depth~\\\\cite{huang2016deep}, Batch Normalization~\\\\cite{BN2015} and Dropout~\\\\cite{dropout2014} exploit regularization for convergence and avoiding overfitting and degradation.\\n\\nSoft attention developed in recent work~\\\\cite{chen2015attention, jaderberg2015spatial} can be trained end-to-end for convolutional network. Our Residual Attention Network incorporates the soft attention in fast developing feedforward network structure in an innovative way. Recent proposed spatial transformer module~\\\\cite{jaderberg2015spatial} achieves state-of-the-art results on house number recognition task. A deep network module capturing top information is used to generate affine transformation. The affine transformation is applied to the input image to get attended region and then feed to another deep network module. The whole process can be trained end-to-end by using differentiable network layer which performs spatial transformation. Attention to scale~\\\\cite{chen2015attention} uses soft attention as a scale selection mechanism and gets state-of-the-art results in image segmentation task.\\n\\n\\nThe design of soft attention structure in our Residual Attention Network is inspired by recent development of localization oriented task, \\\\ie segmentation~\\\\cite{long2015fully,noh2015learning,badrinarayanan2015segnet} and human pose estimation~\\\\cite{newell2016stacked}. These tasks motivate researchers to explore structure with fined-grained feature maps. The frameworks tend to cascade a bottom-up and a top-down structure. The bottom-up feedforward structure produces low resolution feature maps with strong semantic information. After that, a top-down network produces dense features to inference on each pixel. Skip connection~\\\\cite{long2015fully} is employed between bottom and top feature maps and achieved state-of-the-art result on image segmentation. The recent stacked hourglass network~\\\\cite{newell2016stacked} fuses information from multiple scales to predict human pose, and benefits from encoding both global and local information.\\n\\n%-------------------------------------------------------------------------'},\n",
       "   {'Residual Attention Network': \"\\\\begin{figure*}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n  \\\\includegraphics[width=1.0\\\\linewidth]{whole_net.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{Example architecture of the proposed network for ImageNet. We use three hyper-parameters for the design of Attention Module: $p,t$ and $r$. The hyper-parameter $p$ denotes the number of pre-processing Residual Units before splitting into trunk branch and mask branch. $t$ denotes the number of Residual Units in trunk branch. $r$ denotes the number of Residual Units between adjacent pooling layer in the mask branch. In our experiments, we use the following hyper-parameters setting: $\\\\{p=1$, $t=2$, $r=1\\\\}$. The number of channels in the soft mask Residual Unit and corresponding trunk branches is the same.}\\n\\n\\\\label{fig:Attention}\\n\\\\end{figure*}\\n\\nOur Residual Attention Network is constructed by stacking multiple Attention Modules. Each Attention Module is divided into two branches: mask branch and trunk branch. The trunk branch performs feature processing and can be adapted to any state-of-the-art network structures.\\n%\\nIn this work, we use pre-activation Residual Unit~\\\\cite{he2016identity}, ResNeXt~\\\\cite{resnext} and Inception~\\\\cite{inception} as our Residual Attention Networks basic unit to construct Attention Module. Given trunk branch output $T(x)$ with input $x$, the mask branch uses bottom-up top-down structure~\\\\cite{long2015fully, noh2015learning, badrinarayanan2015segnet, newell2016stacked} to learn same size mask $M(x)$ that softly weight output features $T(x)$. The bottom-up top-down structure mimics the fast feedforward and feedback attention process. The output mask is used as control gates for neurons of trunk branch similar to Highway Network~\\\\cite{srivastava2015training}. The output of Attention Module $H$ is:\\n\\\\begin{equation}\\nH_{i,c}(x)=M_{i,c}(x)*T_{i,c}(x)\\n\\\\end{equation}\\nwhere i ranges over all spatial positions and $c\\\\in \\\\{1,...,C\\\\}$ is the index of the channel. The whole structure can be trained end-to-end.\\n\\nIn Attention Modules, the attention mask can not only serve as a feature selector during forward inference, but also as a gradient update filter during back propagation. In the soft mask branch, the gradient of mask for input feature is:\\n\\\\begin{equation}\\n\\\\frac{\\\\partial M(x, \\\\theta)T(x,\\\\phi)}{\\\\partial \\\\phi} = M(x, \\\\theta)\\\\frac{\\\\partial T(x,\\\\phi)}{\\\\partial \\\\phi}\\n\\\\end{equation}\\n\\\\noindent\\nwhere the $\\\\theta$ are the mask branch parameters and the $\\\\phi$ are the trunk branch parameters. This property makes Attention Modules robust to noisy labels. Mask branches can prevent wrong gradients (from noisy labels) to update trunk parameters. Experiment in Sec.\\\\ref{para:noise} shows the robustness of our Residual Attention Network against noisy labels.\\n\\nInstead of stacking Attention Modules in our design, a simple approach would be using a single network branch to generate soft weight mask, similar to spatial transformer layer~\\\\cite{jaderberg2015spatial}. However, these methods have several drawbacks on challenging datasets such as ImageNet. First, images with clutter background, complex scenes, and large appearance variations need to be modeled by different types of attentions. In this case, features from different layers need to be modeled by different attention masks. Using a single mask branch would require exponential number of channels to capture all combinations of different factors. Second, a single Attention Module only modify the features once. If the modification fails on some parts of the image, the following network modules do not get a second chance.\\n\\nThe Residual Attention Network alleviates above problems. In Attention Module, each trunk branch has its own mask branch to learn attention that is specialized for its features. As shown in Fig.\\\\ref{fig:motivation}, in hot air balloon images, blue color features from bottom layer have corresponding sky mask to eliminate background, while part features from top layer are refined by balloon instance mask. Besides, the incremental nature of stacked network structure can gradually refine attention for complex images.\\n\\n\\n\\\\subsection{Attention Residual Learning}\\nHowever, naive stacking Attention Modules leads to the obvious performance drop. First, dot production with mask range from zero to one repeatedly will degrade the value of features in deep layers. Second, soft mask can potentially break good property of trunk branch, for example, the identical mapping of Residual Unit.\\n\\nWe propose attention residual learning to ease the above problems. Similar to ideas in residual learning, if soft mask unit can be constructed as identical mapping, the performances should be no worse than its counterpart without attention. Thus we modify output $H$ of Attention Module as\\n\\\\begin{equation}\\nH_{i,c}(x)=(1+M_{i,c}(x))*F_{i,c}(x)\\n\\\\end{equation}\\n$M(x)$ ranges from $[0,1]$, with $M(x)$ approximating 0, $H(x)$ will approximate original features $F(x)$. We call this method attention residual learning.\\n\\\\\\\\\\n\\\\indent\\nOur stacked attention residual learning is different from residual learning. In the origin ResNet, residual learning is formulated as $H_{i,c}(x)= x + F_{i,c}(x)$, where $F_{i,c}(x)$ approximates the residual function. In our formulation, $F_{i,c}(x)$ indicates the features generated by deep convolutional networks. The key lies on our mask branches $M(x)$. They work as feature selectors which enhance good features and suppress noises from trunk features.\\n\\\\\\\\\\n\\\\indent\\nIn addition, stacking Attention Modules backs up attention residual learning by its incremental nature. Attention residual learning can keep good properties of original features, but also gives them the ability to bypass soft mask branch and forward to top layers to weaken mask branch's feature selection ability. Stacked Attention Modules can gradually refine the feature maps. As show in Fig.\\\\ref{fig:motivation}, features become much clearer as depth going deeper. By using attention residual learning, increasing depth of the proposed Residual Attention Network can improve performance consistently. As shown in the experiment section, the depth of Residual Attention Network is increased up to 452 whose performance surpasses ResNet-1001 by a large margin on CIFAR dataset.\\n\\n\\\\subsection{Soft Mask Branch}\\nFollowing previous attention mechanism idea in DBN~\\\\cite{larochelle2010learning}, our mask branch contains fast feed-forward sweep and top-down feedback steps. The former operation quickly collects global information of the whole image, the latter operation combines global information with original feature maps. In convolutional neural network, the two steps unfold into bottom-up top-down fully convolutional structure.\\n\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}\\n   \\\\includegraphics[width=1\\\\linewidth]{attention.pdf}\\n\\\\end{center}\\n   \\\\caption{The receptive field comparison between mask branch and trunk branch.}\\n\\\\label{fig:attentionunit}\\n\\\\end{figure}\\n\\nFrom input, max pooling are performed several times to increase the receptive field rapidly after a small number of Residual Units. After reaching the lowest resolution, the global information is then expanded by a symmetrical top-down architecture to guide input features in each position. Linear interpolation up sample the output after some Residual Units. The number of bilinear interpolation is the same as max pooling to keep the output size the same as the input feature map. Then a sigmoid layer normalizes the output range to $[0,1]$ after two consecutive $1\\\\times 1$ convolution layers. We also added skip connections between bottom-up and top-down parts to capture information from different scales. The full module is illustrated in Fig.\\\\ref{fig:Attention}.\\n\\nThe bottom-up top-down structure has been applied to image segmentation and human pose estimation. However, the difference between our structure and the previous one lies in its intention. Our mask branch aims at improving trunk branch features rather than solving a complex problem directly. Experiment in Sec.\\\\ref{para:Comparison} is conducted to verify above arguments.\\n%Using additional classification supervision on mask branch directly leads to 0.5\\\\% performance drop on CIFAR-10.\\n\\n\\\\subsection{Spatial Attention and Channel Attention}\\nIn our work, attention provided by mask branch changes adaptably with trunk branch features. However, constrains to attention can still be added to mask branch by changing normalization step in activation function before soft mask output. We use three types of activation functions corresponding to mixed attention, channel attention and spatial attention. Mixed attention $f_{1}$ without additional restriction use simple sigmoid for each channel and spatial position. Channel attention $f_{2}$ performs $L2$ normalization within all channels for each spatial position to remove spatial information. Spatial attention $f_{3}$ performs normalization within feature map from each channel and then sigmoid to get soft mask related to spatial information only.\\n\\\\begin{eqnarray}\\n&&f_{1}(x_{i,c}) = \\\\frac{1}{1+ exp(-x_{i,c})}\\\\\\\\\\n&&f_{2}(x_{i,c}) = \\\\frac{x_{i,c}}{\\\\|x_{i}\\\\|}\\\\\\\\\\n&&f_{3}(x_{i,c}) = \\\\frac{1}{1+ exp(-(x_{i,c} - \\\\text{mean}_c) / \\\\text{std}_c)}\\n\\\\end{eqnarray}\\nWhere $i$ ranges over all spatial positions and $c$ ranges over all channels. $\\\\text{mean}_c$ and $\\\\text{std}_c$ denotes the mean value and standard deviation of feature map from $c$-th channel. $x_{i}$ denotes the feature vector at the $i$th spatial position.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nActivation Function & Attention Type & Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n$f_{1}(x)$ & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n$f_{2}(x)$  & Channel Attention &6.24\\\\\\\\\\n\\\\hline\\n$f_{3}(x)$ & Spatial Attention &6.33\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{The test error (\\\\%) on CIFAR-10 of Attention-56 network with different activation functions.}\\n\\\\label{tab:activation_exp}\\n\\\\end{table}\\n\\nThe experiment results are shown in Table~\\\\ref{tab:activation_exp}, the mixed attention has the best performance. Previous works normally focus on only one type of attention, for example scale attention~\\\\cite{chen2015attention} or spatial attention~\\\\cite{jaderberg2015spatial}, which puts additional constrain on soft mask by weight sharing or normalization. However, as supported by our experiments, making attention change adaptively with features without additional constraint leads to the best performance.\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\footnotesize\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c|c} \\\\hline\\n\\nLayer &Output Size &Attention-56&Attention-92 \\\\\\\\\\n\\\\hline\\nConv1 & 112$\\\\times$112 & \\\\multicolumn{2}{|c}{$7\\\\times 7$, 64, stride 2}  \\\\\\\\\\n\\\\hline\\nMax pooling & 56$\\\\times$56& \\\\multicolumn{2}{|c}{$3\\\\times 3$ stride 2}  \\\\\\\\\\n\\\\hline\\nResidual Unit& 56$\\\\times$56 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 64 \\\\\\\\\\n\\t3\\\\times 3, 64 \\\\\\\\\\n\\t1\\\\times\\t1, 256\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 56$\\\\times$56 & Attention $\\\\times$1 & Attention $\\\\times$1  \\\\\\\\\\n\\\\hline\\nResidual Unit& 28$\\\\times$28 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 128 \\\\\\\\\\n\\t3\\\\times 3, 128 \\\\\\\\\\n\\t1\\\\times\\t1, 512\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 28$\\\\times$28 & Attention $\\\\times$1 & Attention $\\\\times$2  \\\\\\\\\\n\\\\hline\\nResidual Unit& 14$\\\\times$14 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 256 \\\\\\\\\\n\\t3\\\\times 3, 256 \\\\\\\\\\n\\t1\\\\times\\t1, 1024\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 1$\\n}  \\\\\\\\\\n\\\\hline\\nAttention Module& 14$\\\\times$14 & Attention $\\\\times$1 & Attention $\\\\times$3  \\\\\\\\\\n\\\\hline\\nResidual Unit& 7$\\\\times$7 & \\\\multicolumn{2}{|c}{\\n$\\\\left(\\n\\t\\\\begin{matrix}\\n\\t1\\\\times 1, 512 \\\\\\\\\\n\\t3\\\\times 3, 512 \\\\\\\\\\n\\t1\\\\times\\t1, 2048\\n\\t\\\\end{matrix}\\n\\\\right)\\\\times 3$\\n}  \\\\\\\\\\n\\\\hline\\nAverage pooling & 1$\\\\times$1& \\\\multicolumn{2}{|c}{$7\\\\times 7$ stride 1}  \\\\\\\\\\n\\\\hline\\nFC,Softmax & \\\\multicolumn{3}{|c}{1000}  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{params$\\\\times 10^6$} & $31.9$ & $51.3$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{FLOPs$\\\\times 10^9$} & $6.2$ &$10.4$  \\\\\\\\\\n\\\\hline\\n\\\\multicolumn{2}{c|}{Trunk depth} & $56 $ & $92$  \\\\\\\\\\n\\\\hline\\n\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Residual Attention Network architecture details for ImageNet. Attention structure is described in Fig.~\\\\ref{fig:Attention}.  We make the size of the smallest output map in each mask branch 7$\\\\times$7 to be consistent with the smallest trunk output map size. Thus 3,2,1 max-pooling layers are used in mask branch with input size 56$\\\\times$56, 28$\\\\times$28, 14$\\\\times$14 respectively.\\n%\\nThe Attention Module is built by pre-activation Residual Unit~\\\\cite{he2016identity} with the number of channels in each stage is the same as ResNet~\\\\cite{resnet2016}.\\n%\\n}\\n\\\\label{tab:attention_structure}\\n\\\\end{table}\\n\\n\\n%-------------------------------------------------------------------------\"},\n",
       "   {'Experiments': \"In this section, we evaluate the performance of proposed Residual Attention Network on a series of benchmark datasets including CIFAR-10, CIFAR-100~\\\\cite{krizhevsky2009learning}, and ImageNet~\\\\cite{deng2009imagenet}.\\n%\\nOur experiments contain two parts. In the first part, we analyze the effectiveness of each component in the Residual Attention Network including attention residual learning mechanism and different architectures of soft mask branch in the Attention Module.\\n%\\nAfter that, we explore the noise resistance property. Given limited computation resources, we choose CIFAR-10 and CIFAR-100 dataset to conduct these experiments. Finally, we compare our network with state-of-the-art results in CIFAR dataset.\\n%\\nIn the second part, we replace the Residual Unit with Inception Module and ResNeXt to demonstrate our Residual Attention Network surpasses origin networks both in parameter efficiency and final performance.\\n%\\nWe also compare image classification performance with state-of-the-art ResNet and Inception on ImageNet dataset.\\n%\\n\\n\\n\\\\subsection{CIFAR and Analysis}\\n\\n\\n\\\\paragraph{Implementation.}\\n\\\\phantomsection\\n\\\\label{para:imple}\\nThe CIFAR-10 and CIFAR-100 datasets consist of $60,000$ $32\\\\times32$ color images of $10$ and $100$ classes respectively, with $50,000$ training images and $10,000$ test images.\\n%\\nThe broadly applied state-of-the-art network structure ResNet is used as baseline method.\\n%\\nTo conduct fair comparison, we keep most of the settings same as ResNet paper~\\\\cite{resnet2016}.\\n%\\nThe image is padded by 4 pixels on each side, filled with $0$ value resulting in $40\\\\times40$ image. A $32\\\\times32$ crop is randomly sampled from an image or its horizontal flip, with the per-pixel RGB mean value subtracted.\\n%\\nWe adopt the same weight initialization method following previous study~\\\\cite{prelu2015} and train Residual Attention Network using nesterov SGD with a mini-batch size of 64.\\n%\\nWe use a weight decay of $0.0001$ with a momentum of $0.9$ and set the initial learning rate to 0.1. The learning rate is divided by 10 at $64$k and $96$k iterations. We terminate training at $160$k iterations.\\n\\nThe overall network architecture and the hyper parameters setting are described in Fig.\\\\ref{fig:Attention}.\\n%\\nThe network consists of 3 stages and similar to ResNet~\\\\cite{resnet2016}, equal number of Attention Modules are stacked in each stage.\\n%\\nAdditionally, we add two Residual Units at each stage. The number of weighted layers in trunk branch is 36$m$+20 where $m$ is the number of Attention Module in one stage.\\n%\\nWe use original $32\\\\times32$ image for testing.\\n\\n\\n\\\\paragraph{Attention Residual Learning.}\\n\\n\\nIn this experiment, we evaluate the effectiveness of attention residual learning mechanism.\\n%\\nSince the notion of attention residual learning (ARL) is new, no suitable previous methods are comparable therefore we use ``naive attention learning'' (NAL) as baseline.\\n%\\nSpecifically, ``naive attention learning'' uses Attention Module where features are directly dot product by soft mask without attention residual learning.\\n%\\n% Add a small figure here.\\nWe set the number of Attention Module in each stage $m$ = \\\\{1, 2, 3, 4\\\\}. For Attention Module, this leads to Attention-56 (named by trunk layer depth), Attention-92, Attention-128 and Attention-164 respectively.\\n%\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\n Network & ARL (Top-1 err. \\\\%) & NAL (Top-1 err.\\\\%)\\\\\\\\\\n\\\\hline\\nAttention-56 &\\\\textbf{5.52} & 5.89\\\\\\\\\\n\\\\hline\\nAttention-92 &\\\\textbf{4.99} & 5.35\\\\\\\\\\n\\\\hline\\nAttention-128 &\\\\textbf{4.44} & 5.57\\\\\\\\\\n\\\\hline\\nAttention-164 &\\\\textbf{4.31} & 7.18\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{\\nClassification error (\\\\%) on CIAFR-10.}\\n\\\\label{tab:learning}\\n\\\\end{table}\\n\\nWe train these networks using different mechanisms and summarize the results in the Table~\\\\ref{tab:learning}.\\n%\\nAs shown in Table~\\\\ref{tab:learning}, the networks trained using attention residual learning technique consistently outperform the networks trained with baseline method which proves the effectiveness of our method. \\n%\\nThe performance increases with the number of Attention Module when applying attention residual learning. In contrast, the performance of networks trained with ``naive attention learning'' method suffers obvious degradation with increased number of Attention Module.\\n\\n%\\n\\\\begin{figure}[t]\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-15pt}\\n\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n  \\\\includegraphics[width=1\\\\linewidth]{mean_value.pdf}\\n  %\\\\includegraphics{images/whole_net.eps}\\n\\\\end{center}\\n   \\\\caption{The mean absolute response of output features in each stage. }\\n\\\\label{fig:mean_response}\\n\\\\end{figure}\\nTo understand the benefit of attention residual learning, we calculate mean absolute response value of output layers for each stage. We use Attention-164 to conduct this experiment.\\n%\\nAs shown in the Fig.~\\\\ref{fig:mean_response}, the response generated by the network trained using naive attention learning quickly vanishes in the stage 2 after four Attention Modules compared with network trained using attention residual learning.\\n%\\nThe Attention Module is designed to suppress noise while keeping useful information by applying dot product between feature and soft mask. However, repeated dot product will lead to severe degradation of both useful and useless information in this process.\\n%\\nThe attention residual learning can relieve signal attenuation using identical mapping, which enhances the feature contrast.\\n%\\nTherefore, it gains benefits from noise reduction without significant information loss, which makes optimization much easier while improving the discrimination of represented features.\\n%\\nIn the rest of the experiments, we apply this technique to train our networks.\\n\\n%----------------------------------------------------------------------------------\\n\\\\paragraph{Comparison of different mask structures.}\\n\\\\label{para:Comparison}\\nWe conduct experiments to validate the effectiveness of encoder-decoder structure by comparing with local convolutions without any down sampling or up sampling. The local convolutions soft mask consists of three Residual Units using the same number of FLOPs.\\n%\\nThe Attention-56 is used to construct Attention-Encoder-Decoder-56 and Attention-Local-Conv-56 respectively.\\n%\\nResults are shown in Table~\\\\ref{tab:local_global_attention}.\\n%\\nThe Attention-Encoder-Decoder-56 network achieves lower test error $5.52\\\\%$ compared with Attention-Local-Conv-56 network $6.48\\\\%$ with a considerable margin $0.94\\\\%$. The result suggests that the soft attention optimization process will benefit from multi-scale information.\\n\\n\\\\begin{table}[h]\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nMask Type  & Attention Type &Top-1 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nLocal Convolutions & Local Attention &6.48 \\\\\\\\\\n\\\\hline\\nEncoder and Decoder  & Mixed Attention &\\\\textbf{5.52}\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 using different mask structures.}\\n\\\\label{tab:local_global_attention}\\n\\\\end{table}\\n\\n\\n\\\\paragraph{Noisy Label Robustness.}\\n\\\\label{para:noise}\\n\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n\\\\begin{tabular}{c|c|c} \\\\hline\\nNoise Level &ResNet-164 err. (\\\\%) & Attention-92 err. (\\\\%) \\\\\\\\\\n\\\\hline\\n10\\\\% &5.93 &5.15\\\\\\\\\\n\\\\hline\\n30\\\\% &6.61 &5.79\\\\\\\\\\n\\\\hline\\n50\\\\% &8.35 &7.27\\\\\\\\\\n\\\\hline\\n70\\\\% &17.21 &15.75\\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\\\end{center}\\n\\\\caption{Test error (\\\\%) on CIFAR-10 with label noises.}\\n\\\\label{tab:noise_label}\\n\\\\end{table}\\nIn this experiment, we show our Residual Attention Network enjoys noise resistant property on CIFAR-10 dataset following the setting of paper~\\\\cite{sukhbaatar2014training}.\\n%\\nThe confusion matrix $Q$ in our experiment is set as follows:\\n\\\\begin{equation}\\nQ =\\n\\\\left(\\n\\\\begin{matrix}\\nr & \\\\frac{1-r}{9} &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\frac{1-r}{9} &r  &\\\\cdots &\\\\frac{1-r}{9} \\\\\\\\\\n\\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\\\n\\\\frac{1-r}{9} & \\\\frac{1-r}{9} &\\\\cdots &r \\\\\\\\\\n\\\\end{matrix}\\n\\\\right)_{10\\\\times 10}\\n\\\\end{equation}\\n\\n%\\n\\\\noindent\\nwhere $r$ denotes the clean label ratio for the whole dataset.\\n\\nWe compare ResNet-164 network with Attention-92 network under different noise levels.\\n%\\nThe Table~\\\\ref{tab:noise_label} shows the results.\\n%\\nThe test error of Attention-92 network is significantly lower than ResNet-164 network with the same noise level.\\n%\\nIn addition, when we increase the ratio of noise, test error of Attenion-92 declines slowly compared with ResNet-164 network.\\n%\\nThese results suggest that our Residual Attention Network can perform well even trained with high level noise data.\\n%\\n%The encode-decode structure can fast feedforward the whole image and obtain the global and local information of image.\\n%\\nWhen the label is noisy, the corresponding mask can prevent gradient caused by label error to update trunk branch parameters in the network.\\n%\\nIn this way, only the trunk branch is learning the wrong supervision information and soft mask branch masks the wrong label.\\n\\n\\n\\\\paragraph{Comparisons with state-of-the-art methods.}\\n\\\\begin{table}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-5pt}\\n\\\\begin{center}\\n\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c}\\n\\t\\\\hline\\n   \\tNetwork& params$\\\\times 10^6$ & CIFAR-10  &  CIFAR-100 \\\\\\\\\\n\\t\\\\hline\\n\\tResNet-164~\\\\cite{he2016identity}  & 1.7   & 5.46  & 24.33 \\\\\\\\\\n\\tResNet-1001~\\\\cite{he2016identity} & 10.3   & 4.64  & 22.71 \\\\\\\\\\n\\t\\\\hline\\n\\tWRN-16-8~\\\\cite{zagoruyko2016wide} & 11.0   & 4.81  & 22.07 \\\\\\\\\\n\\tWRN-28-10~\\\\cite{zagoruyko2016wide} & 36.5   & 4.17  & 20.50 \\\\\\\\\\n\\t\\\\hline\\n\\tAttention-92 & 1.9 & 4.99 & 21.71 \\\\\\\\\\n\\tAttention-236 & 5.1 & 4.14 & 21.16 \\\\\\\\\\n\\tAttention-452$\\\\dag$ & 8.6 & \\\\textbf{3.90}  & \\\\textbf{20.45}\\\\\\\\\\n\\t\\\\hline\\n\\\\end{tabular}\\n}\\n\\\\end{center}\\n\\\\caption{Comparisons with state-of-the-art methods on CIFAR-10/100. $\\\\dag$: the Attention-452 consists of Attention Module with hyper-parameters setting: $\\\\{p=2$, $t=4$, $r=3\\\\}$ and 6 Attention Modules per stage. }\\n\\\\label{tab:cifar_results}\\n\\\\end{table}\\n\\nWe compare our Residual Attention Network with state-of-the-art methods including ResNet~\\\\cite{he2016identity} and Wide ResNet~\\\\cite{zagoruyko2016wide} on CIFAR-10 and CIFAR-100 datasets.\\n%\\nThe results are shown in Table~\\\\ref{tab:cifar_results}.\\n%\\nOur Attention-452 outperforms all the baseline methods on CIFAR-10 and CIFAR-100 datasets.\\n%\\nNote that Attention-92 network achieves $4.99\\\\%$ test error on CIFAR-10 and $21.71\\\\%$ test error on CIFAR-100 compared with $5.46\\\\%$ and $24.33\\\\%$ test error on CIFAR-10 and CIFAR-100 for ResNet-164 network under similar parameter size.\\n%\\nIn addition, Attention-236 outperforms ResNet-1001 using only half of the parameters. It suggests that our Attention Module and attention residual learning scheme can effectively reduce the number of parameters in the network while improving the classification performance.\\n%\\n%It worth to mention that, our method is complementary with other state-of-the-art methods which focus on regularization and can achieve better results by applying these advanced techniques.\\n\\n\\n\\\\subsection{ImageNet Classification}\\n\\nIn this section, we conduct experiments using ImageNet LSVRC $2012$ dataset~\\\\cite{deng2009imagenet}, which contains $1,000$ classes with $1.2$ million training images, $50,000$ validation images, and $100,000$ test images.\\n%\\nThe evaluation is measured on the non-blacklist images of the ImageNet LSVRC $2012$ validation set.\\n%\\nWe use Attention-56 and Attention-92 to conduct the experiments. The network structures and hyper parameters can be found in the Table~\\\\ref{tab:attention_structure}.\\n%\\n\\n\\\\paragraph{Implementation.}\\nOur implementation generally follows the practice in the previous study~\\\\cite{krizhevsky2012imagenet}.\\n%\\nWe apply scale and aspect ratio augmentation~\\\\cite{szegedy2015going} to the original image.\\n%\\nA $224\\\\times 224$ crop is randomly sampled from an augment image or its horizontal flip, with the per-pixel RGB scale to $[0,1]$ and mean value subtracted and standard variance divided. We adopt standard color augmentation~\\\\cite{krizhevsky2012imagenet}.\\n%\\nThe network is trained using SGD with a momentum of $0.9$.\\n%\\nWe set initial learning rate to 0.1. The learning rate is divided by 10 at $200$k, $400$k, $500$k iterations. We terminate training at $530$k iterations.\\n\\n\\n\\\\paragraph{Mask Influence.}\\n\\n\\\\begin{table*}\\\\small\\n\\\\setlength{\\\\abovecaptionskip}{0pt}\\n\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n\\\\begin{tabular}{c|c|c|c|c|c} \\\\hline\\nNetwork & params$\\\\times 10^6$ &FLOPs$\\\\times 10^9$ & Test Size &Top-1 err. (\\\\%) &Top-5 err. (\\\\%) \\\\\\\\\\n\\\\hline\\nResNet-152~\\\\cite{resnet2016}  &60.2 &11.3 &$224\\\\times224$&22.16 &6.16\\\\\\\\\\n\\\\hline\\nAttention-56 &31.9 &6.3 &$224\\\\times224$&\\\\textbf{21.76} &\\\\textbf{5.9} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNeXt-101 ~\\\\cite{resnext}&44.5 & 7.8&$224\\\\times224$    &21.2 &5.6 \\\\\\\\\\n\\\\hline\\nAttentionNeXt-56 &31.9 & 6.3&$224\\\\times224$  &\\\\textbf{21.2} &\\\\textbf{5.6} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nInception-ResNet-v1~\\\\cite{inception} &- &-&$299\\\\times299$&21.3 &5.5 \\\\\\\\\\n\\\\hline\\nAttentionInception-56 &31.9 & 6.3 &$299\\\\times299$ &\\\\textbf{20.36} &\\\\textbf{5.29} \\\\\\\\\\n\\\\hline\\n\\\\hline\\nResNet-200~\\\\cite{he2016identity} &64.7 &15.0 &$320\\\\times320$ &20.1  &4.8 \\\\\\\\\\n\\\\hline\\n{Inception-ResNet-v2} &- &- &$299\\\\times299$ &19.9  &4.9 \\\\\\\\\\n\\\\hline\\nAttention-92 &51.3  & 10.4&$320\\\\times320$ &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n%}\\n\\\\end{center}\\n\\t\\\\caption{Single crop validation error on ImageNet.\\n}\\n\\\\label{tab:single_crop_validation_error}\\n\\\\end{table*}\\n\\nIn this experiment, we explore the efficiency of proposed Residual Attention Network.\\n%\\nWe compare Attention-56 with ResNet-152~\\\\cite{resnet2016}.\\n%\\nThe ResNet-152 has 50 trunk Residual Units and 60.2$\\\\times 10^6$ parameters compared with 18 trunk Residual Units and 31.9$\\\\times 10^6$ parameters in Attention-56.\\n%\\nWe evaluate our model using single crop scheme on the ImageNet validation set and show results in Table~\\\\ref{tab:single_crop_validation_error}.\\n%\\nThe Attention-56 network outperforms ResNet-152 by a large margin with a $0.4\\\\%$ reduction on top-1 error and a $0.26\\\\%$ reduction on top-5 error.\\n%\\nMore importantly, Attention-56 network achieves better performance with only 52\\\\% parameters and 56\\\\% FLOPs compared with ResNet-152, which suggests that the proposed attention mechanism can significantly improve network performance while reducing the model complexity.\\n\\n\\n\\\\paragraph{Different Basic Units.}\\n%\\nIn this experiment, we show Residual Attention Network can generalize well using different basic unit. We apply three popular basic units: Residual Unit, ResNeXt~\\\\cite{resnext}, and Inception~\\\\cite{inception} to construct our Residual Attention Networks. To keep the number of parameters and FLOPs in the same scale, we simplify the Inception. Results are shown in Table~\\\\ref{tab:single_crop_validation_error}.\\n\\n%\\n%\\\\begin{figure}[t]\\n%\\\\setlength{\\\\abovecaptionskip}{0pt}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\fbox{\\\\rule{0pt}{2in} \\\\rule{0.9\\\\linewidth}{0pt}}%\\n%  \\\\includegraphics[width=1\\\\linewidth]{images/inception.png}\\n  %\\\\includegraphics{images/whole_net.eps}\\n%\\\\end{center}\\n%   \\\\caption{The simple inception module stucture. The hyper-parameter $c$ denotes the number of channel in one stage. In this experiment, we choose $\\\\{256, 512, 1024, 2048\\\\}$ at feature map $\\\\{56\\\\times56, 28\\\\times28, 14\\\\times14, 7\\\\times7\\\\}$.}\\n%\\\\label{fig:inception}\\n%\\\\end{figure}\\n%\\nWhen the basic unit is ResNeXt, the AttentionNeXt-56 network performance is the same as ResNeXt-101 while the parameters and FLOPs are significantly fewer than ResNeXt-101.\\n%\\nFor Inception, The AttentionIncepiton-56 outperforms Inception-ResNet-v1~\\\\cite{inception} by a margin with a 0.94\\\\% reduction on top-1 error and a 0.21\\\\% reduction on top-5 error.\\n%\\nThe results show that our method can be applied on different network structures.\\n\\n\\\\paragraph{Comparisons with State-of-the-art Methods.}\\n\\n%\\\\begin{table}\\n%\\\\setlength{\\\\belowcaptionskip}{-10pt}\\n%\\\\begin{center}\\n%\\\\resizebox{\\\\linewidth}{!}{%\\n%\\\\begin{tabular}{c|c|c|c|c} \\\\hline\\n%Network &param/M & FLOPs$\\\\times 10^9$ &top-1 err. &top-5 err.\\\\\\\\\\n%\\\\hline\\n%{ResNet-200}~\\\\cite{he2016identity} &64.7 &15.0  &20.1  &4.8 \\\\\\\\\\n%\\\\hline\\n%{Inception-ResNet-v2} &- &-  &19.9  &4.9 \\\\\\\\\\n%\\\\hline\\n%Attention-92 &51.3  & 10.4 &\\\\textbf{19.5 }  &\\\\textbf{4.8} \\\\\\\\\\n%\\\\hline\\n%\\\\end{tabular}\\n%}\\n%\\\\end{center}\\n %\\\\caption{\\n%Comparisons of single crop error on the ILSVRC 2012 validation set. In order to compare fairly, we also test our Attention Network on a %single 320$\\\\times$320 crop.}\\n%\\\\label{tab:imagenet_result}\\n%\\\\end{table}\\n\\n\\nWe compare our Attention-92 evaluated using single crop on the ILSVRC 2012 validation set with state-of-the-art algorithms.\\n%\\nTable~\\\\ref{tab:single_crop_validation_error} shows the results.\\n%\\nOur Attention-92 outperforms ResNet-200 with a large margin. The reduction on top-1 error is $0.6\\\\%$.\\n%\\nNote that the ResNet-200 network contains $32\\\\%$ more parameters than Attention-92.\\n%\\nThe computational complexity of Attention-92 shown in the Table~\\\\ref{tab:single_crop_validation_error} suggests that our network reduces nearly half training time comparing with ResNet-200 by adding attention mechanism and reducing trunk depth.\\n%\\nAbove results suggest that our model enjoys high efficiency and good performance.\\n\\n%Our architecture is parallel to major structure of original network, which is friendly to parallel computation. (3) Stacked Attention Module on $14\\\\times14$ feature map gains $1.3\\\\%$ improvement, contrast to the one of single unit, benefits from more Attention Module.\\n\\n%Note that we test a single 320$\\\\times$320 crop from short side of 320, which is consistent with ResNet-200[].\\n%Although our Attention-80 has significantly computation complexity than pre-activation ResNet-200 [](15.0$\\\\times 10^9$), Our Attention-80 has achieved top-1 error rate of 20.3\\\\%, which is 0.4\\\\% lower than the baseline ResNet-200.\"},\n",
       "   {'Discussion': 'We propose a Residual Attention Network which stacks multiple Attention Modules. The benefits of our network are in two folds: it can capture mixed attention and is an extensible convolutional neural network. The first benefit lies in that different Attention Modules capture different types of attention to guide feature learning. Our experiments on the forms of activation function also validate this point: free form mixed attention will have better performance than constrained (including single) attention. The second benefit comes from encoding top-down attention mechanism into bottom-up top-down feedforward convolutional structure in each Attention Module. Thus, the basic Attention Modules can be combined to form larger network structure. Moreover, residual attention learning allows training very deep Residual Attention Network. The performance of our model surpasses state-of-the-art image classification methods, \\\\ie ResNet on CIFAR-10 (3.90\\\\% error), CIFAR-100 (20.67\\\\% error), and challenging ImageNet dataset (0.6\\\\% top-1 accuracy improvement) with only $46\\\\%$ trunk depth and $69\\\\%$ forward FLOPs (comparing with ResNet-200). In the future, we will exploit different applications of deep Residual Attention Network such as detection and segmentation to better explore mixed attention mechanism for specific tasks.\\n\\n\\n{\\\\small\\n\\\\bibliographystyle{ieee}\\n\\\\bibliography{attention-net_camera_ready_wf}\\n}\\n\\n\\\\end{document}'}]}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57f7933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
